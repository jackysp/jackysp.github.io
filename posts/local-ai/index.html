<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Exploring Local LLMs with Ollama: My Journey and Practices | Mini Fish</title><meta name=keywords content><meta name=description content="Local Large Language Models (LLMs) have been gaining traction as developers and enthusiasts seek more control over their AI tools without relying solely on cloud-based solutions. In this blog post, I&rsquo;ll share my experiences with Ollama, a remarkable tool for running local LLMs, along with other tools like llamaindex and Candle. I&rsquo;ll also discuss various user interfaces (UI) that enhance the local LLM experience.

Table of Contents

Introduction to Ollama

A Popular Choice
Ease of Use
Built with Golang


My Practices with Ollama

Preferred Models

Llama 3.1
Mistral
Phi-3
Qwen-2


Hardware Constraints


Exploring UIs for Ollama

OpenWebUI
Page Assist
Enchanted
AnythingLLM
Dify


Diving into llamaindex
Experimenting with Candle
Conclusion


Introduction to Ollama
A Popular Choice
Ollama has rapidly become a favorite among developers interested in local LLMs. Within a year, it has garnered significant attention on GitHub, reflecting its growing user base and community support."><meta name=author content="Jack Yu"><link rel=canonical href=https://blog.minifish.org/posts/local-ai/><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.minifish.org/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.minifish.org/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.minifish.org/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.minifish.org/apple-touch-icon.png><link rel=mask-icon href=https://blog.minifish.org/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.minifish.org/posts/local-ai/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-G0H8JF722Y"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-G0H8JF722Y")}</script><meta property="og:url" content="https://blog.minifish.org/posts/local-ai/"><meta property="og:site_name" content="Mini Fish"><meta property="og:title" content="Exploring Local LLMs with Ollama: My Journey and Practices"><meta property="og:description" content="Local Large Language Models (LLMs) have been gaining traction as developers and enthusiasts seek more control over their AI tools without relying solely on cloud-based solutions. In this blog post, I’ll share my experiences with Ollama, a remarkable tool for running local LLMs, along with other tools like llamaindex and Candle. I’ll also discuss various user interfaces (UI) that enhance the local LLM experience.
Table of Contents Introduction to Ollama A Popular Choice Ease of Use Built with Golang My Practices with Ollama Preferred Models Llama 3.1 Mistral Phi-3 Qwen-2 Hardware Constraints Exploring UIs for Ollama OpenWebUI Page Assist Enchanted AnythingLLM Dify Diving into llamaindex Experimenting with Candle Conclusion Introduction to Ollama A Popular Choice Ollama has rapidly become a favorite among developers interested in local LLMs. Within a year, it has garnered significant attention on GitHub, reflecting its growing user base and community support."><meta property="og:locale" content="en-US"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-27T18:26:14+08:00"><meta property="article:modified_time" content="2024-11-27T18:26:14+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Exploring Local LLMs with Ollama: My Journey and Practices"><meta name=twitter:description content="Local Large Language Models (LLMs) have been gaining traction as developers and enthusiasts seek more control over their AI tools without relying solely on cloud-based solutions. In this blog post, I&rsquo;ll share my experiences with Ollama, a remarkable tool for running local LLMs, along with other tools like llamaindex and Candle. I&rsquo;ll also discuss various user interfaces (UI) that enhance the local LLM experience.

Table of Contents

Introduction to Ollama

A Popular Choice
Ease of Use
Built with Golang


My Practices with Ollama

Preferred Models

Llama 3.1
Mistral
Phi-3
Qwen-2


Hardware Constraints


Exploring UIs for Ollama

OpenWebUI
Page Assist
Enchanted
AnythingLLM
Dify


Diving into llamaindex
Experimenting with Candle
Conclusion


Introduction to Ollama
A Popular Choice
Ollama has rapidly become a favorite among developers interested in local LLMs. Within a year, it has garnered significant attention on GitHub, reflecting its growing user base and community support."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.minifish.org/posts/"},{"@type":"ListItem","position":2,"name":"Exploring Local LLMs with Ollama: My Journey and Practices","item":"https://blog.minifish.org/posts/local-ai/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Exploring Local LLMs with Ollama: My Journey and Practices","name":"Exploring Local LLMs with Ollama: My Journey and Practices","description":"Local Large Language Models (LLMs) have been gaining traction as developers and enthusiasts seek more control over their AI tools without relying solely on cloud-based solutions. In this blog post, I\u0026rsquo;ll share my experiences with Ollama, a remarkable tool for running local LLMs, along with other tools like llamaindex and Candle. I\u0026rsquo;ll also discuss various user interfaces (UI) that enhance the local LLM experience.\nTable of Contents Introduction to Ollama A Popular Choice Ease of Use Built with Golang My Practices with Ollama Preferred Models Llama 3.1 Mistral Phi-3 Qwen-2 Hardware Constraints Exploring UIs for Ollama OpenWebUI Page Assist Enchanted AnythingLLM Dify Diving into llamaindex Experimenting with Candle Conclusion Introduction to Ollama A Popular Choice Ollama has rapidly become a favorite among developers interested in local LLMs. Within a year, it has garnered significant attention on GitHub, reflecting its growing user base and community support.\n","keywords":[],"articleBody":"Local Large Language Models (LLMs) have been gaining traction as developers and enthusiasts seek more control over their AI tools without relying solely on cloud-based solutions. In this blog post, I’ll share my experiences with Ollama, a remarkable tool for running local LLMs, along with other tools like llamaindex and Candle. I’ll also discuss various user interfaces (UI) that enhance the local LLM experience.\nTable of Contents Introduction to Ollama A Popular Choice Ease of Use Built with Golang My Practices with Ollama Preferred Models Llama 3.1 Mistral Phi-3 Qwen-2 Hardware Constraints Exploring UIs for Ollama OpenWebUI Page Assist Enchanted AnythingLLM Dify Diving into llamaindex Experimenting with Candle Conclusion Introduction to Ollama A Popular Choice Ollama has rapidly become a favorite among developers interested in local LLMs. Within a year, it has garnered significant attention on GitHub, reflecting its growing user base and community support.\nEase of Use One of Ollama’s standout features is its simplicity. It’s as easy to use as Docker, making it accessible even to those who may not be deeply familiar with machine learning frameworks. The straightforward command-line interface allows users to download and run models with minimal setup.\nBuilt with Golang Ollama is written in Golang, ensuring performance and efficiency. Golang’s concurrency features contribute to Ollama’s ability to handle tasks effectively, which is crucial when working with resource-intensive LLMs.\nMy Practices with Ollama Preferred Models Llama 3.1 I’ve found that Llama 3.1 works exceptionally well with Ollama. It’s my go-to choice due to its performance and compatibility.\nMistral While Mistral also performs well, it hasn’t gained as much popularity as Llama. Nevertheless, it’s a solid option worth exploring.\nPhi-3 Developed by Microsoft, Phi-3 is both fast and efficient. The 2B parameter model strikes a balance between size and performance, making it one of the best small-sized LLMs available.\nQwen-2 Despite its impressive benchmarks, Qwen-2 didn’t meet my expectations in practice. It might work well in certain contexts, but it didn’t suit my specific needs.\nHardware Constraints Running large models on hardware with limited resources can be challenging. On my 16GB MacBook, models around 7B to 8B parameters are the upper limit. Attempting to run larger models results in performance issues.\nExploring UIs for Ollama Enhancing the user experience with UIs can make interacting with local LLMs more intuitive. Here’s a look at some UIs I’ve tried:\nOpenWebUI OpenWebUI offers a smooth and user-friendly interface similar to Ollama’s default UI. It requires Docker to run efficiently, which might be a barrier for some users.\nFeatures: Basic Retrieval-Augmented Generation (RAG) capabilities. Connection to OpenAI APIs. Page Assist Page Assist is a Chrome extension that I’ve chosen for its simplicity and convenience.\nAdvantages: No requirement for Docker. Accesses the current browser page as input, enabling context-aware interactions. Enchanted Enchanted is unique as it provides an iOS UI for local LLMs with support for Ollama.\nUsage: By using Tailscale, I can connect it to Ollama running on my MacBook. Serves as an alternative to Apple’s native intelligence features. AnythingLLM AnythingLLM offers enhanced RAG capabilities. However, in my experience, it hasn’t performed consistently well enough for regular use.\nDify Dify is a powerful and feature-rich option.\nPros: Easy to set up with an extensive feature set. Cons: Resource-intensive, requiring Docker and running multiple containers like Redis and PostgreSQL. Diving into llamaindex llamaindex is geared towards developers who are comfortable writing code. While it offers robust functionalities, it does have a learning curve.\nObservations: Documentation is somewhat limited, often necessitating diving into the source code. The llamaindex-cli tool aims to simplify getting started but isn’t entirely stable. Works seamlessly with OpenAI. Requires code modifications to function with Ollama. Experimenting with Candle Candle is an intriguing project written in Rust.\nFeatures:\nUses Hugging Face to download models. Simple to run but exhibits slower performance compared to Ollama. Additional Tools:\nCake: A distributed solution based on Candle, Cake opens up possibilities for scaling and extending use cases. Conclusion Exploring local LLMs has been an exciting journey filled with learning and experimentation. Tools like Ollama, llamaindex, and Candle offer various pathways to harnessing the power of LLMs on personal hardware. While there are challenges, especially with hardware limitations and setup complexities, the control and privacy afforded by local models make the effort worthwhile.\nFeel free to share your experiences or ask questions in the comments below!\n","wordCount":"722","inLanguage":"en","datePublished":"2024-11-27T18:26:14+08:00","dateModified":"2024-11-27T18:26:14+08:00","author":{"@type":"Person","name":"Jack Yu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.minifish.org/posts/local-ai/"},"publisher":{"@type":"Organization","name":"Mini Fish","logo":{"@type":"ImageObject","url":"https://blog.minifish.org/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.minifish.org/ accesskey=h title="Mini Fish (Alt + H)">Mini Fish</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.minifish.org/>Home</a>&nbsp;»&nbsp;<a href=https://blog.minifish.org/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Exploring Local LLMs with Ollama: My Journey and Practices</h1><div class=post-meta><span title='2024-11-27 18:26:14 +0800 +0800'>November 27, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;722 words&nbsp;·&nbsp;Jack Yu&nbsp;|&nbsp;<a href=https://github.com/jackysp/blog/tree/master/content/posts/local-ai.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#introduction-to-ollama>Introduction to Ollama</a><ul><li><a href=#a-popular-choice>A Popular Choice</a></li><li><a href=#ease-of-use>Ease of Use</a></li><li><a href=#built-with-golang>Built with Golang</a></li></ul></li><li><a href=#my-practices-with-ollama>My Practices with Ollama</a><ul><li><a href=#preferred-models>Preferred Models</a></li><li><a href=#hardware-constraints>Hardware Constraints</a></li></ul></li><li><a href=#exploring-uis-for-ollama>Exploring UIs for Ollama</a><ul><li><a href=#openwebui>OpenWebUI</a></li><li><a href=#page-assist>Page Assist</a></li><li><a href=#enchanted>Enchanted</a></li><li><a href=#anythingllm>AnythingLLM</a></li><li><a href=#dify>Dify</a></li></ul></li><li><a href=#diving-into-llamaindex>Diving into llamaindex</a></li><li><a href=#experimenting-with-candle>Experimenting with Candle</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><p>Local Large Language Models (LLMs) have been gaining traction as developers and enthusiasts seek more control over their AI tools without relying solely on cloud-based solutions. In this blog post, I&rsquo;ll share my experiences with <strong>Ollama</strong>, a remarkable tool for running local LLMs, along with other tools like <strong>llamaindex</strong> and <strong>Candle</strong>. I&rsquo;ll also discuss various user interfaces (UI) that enhance the local LLM experience.</p><hr><h2 id=table-of-contents>Table of Contents<a hidden class=anchor aria-hidden=true href=#table-of-contents>#</a></h2><ul><li><a href=#introduction-to-ollama>Introduction to Ollama</a><ul><li><a href=#a-popular-choice>A Popular Choice</a></li><li><a href=#ease-of-use>Ease of Use</a></li><li><a href=#built-with-golang>Built with Golang</a></li></ul></li><li><a href=#my-practices-with-ollama>My Practices with Ollama</a><ul><li><a href=#preferred-models>Preferred Models</a><ul><li><a href=#llama-31>Llama 3.1</a></li><li><a href=#mistral>Mistral</a></li><li><a href=#phi-3>Phi-3</a></li><li><a href=#qwen-2>Qwen-2</a></li></ul></li><li><a href=#hardware-constraints>Hardware Constraints</a></li></ul></li><li><a href=#exploring-uis-for-ollama>Exploring UIs for Ollama</a><ul><li><a href=#openwebui>OpenWebUI</a></li><li><a href=#page-assist>Page Assist</a></li><li><a href=#enchanted>Enchanted</a></li><li><a href=#anythingllm>AnythingLLM</a></li><li><a href=#dify>Dify</a></li></ul></li><li><a href=#diving-into-llamaindex>Diving into llamaindex</a></li><li><a href=#experimenting-with-candle>Experimenting with Candle</a></li><li><a href=#conclusion>Conclusion</a></li></ul><hr><h2 id=introduction-to-ollama>Introduction to Ollama<a hidden class=anchor aria-hidden=true href=#introduction-to-ollama>#</a></h2><h3 id=a-popular-choice>A Popular Choice<a hidden class=anchor aria-hidden=true href=#a-popular-choice>#</a></h3><p><a href=https://github.com/jmorganca/ollama>Ollama</a> has rapidly become a favorite among developers interested in local LLMs. Within a year, it has garnered significant attention on GitHub, reflecting its growing user base and community support.</p><h3 id=ease-of-use>Ease of Use<a hidden class=anchor aria-hidden=true href=#ease-of-use>#</a></h3><p>One of Ollama&rsquo;s standout features is its simplicity. It&rsquo;s as easy to use as Docker, making it accessible even to those who may not be deeply familiar with machine learning frameworks. The straightforward command-line interface allows users to download and run models with minimal setup.</p><h3 id=built-with-golang>Built with Golang<a hidden class=anchor aria-hidden=true href=#built-with-golang>#</a></h3><p>Ollama is written in <strong>Golang</strong>, ensuring performance and efficiency. Golang&rsquo;s concurrency features contribute to Ollama&rsquo;s ability to handle tasks effectively, which is crucial when working with resource-intensive LLMs.</p><h2 id=my-practices-with-ollama>My Practices with Ollama<a hidden class=anchor aria-hidden=true href=#my-practices-with-ollama>#</a></h2><h3 id=preferred-models>Preferred Models<a hidden class=anchor aria-hidden=true href=#preferred-models>#</a></h3><h4 id=llama-31>Llama 3.1<a hidden class=anchor aria-hidden=true href=#llama-31>#</a></h4><p>I&rsquo;ve found that <strong>Llama 3.1</strong> works exceptionally well with Ollama. It&rsquo;s my go-to choice due to its performance and compatibility.</p><h4 id=mistral>Mistral<a hidden class=anchor aria-hidden=true href=#mistral>#</a></h4><p>While <strong>Mistral</strong> also performs well, it hasn&rsquo;t gained as much popularity as Llama. Nevertheless, it&rsquo;s a solid option worth exploring.</p><h4 id=phi-3>Phi-3<a hidden class=anchor aria-hidden=true href=#phi-3>#</a></h4><p>Developed by Microsoft, <strong>Phi-3</strong> is both fast and efficient. The 2B parameter model strikes a balance between size and performance, making it one of the best small-sized LLMs available.</p><h4 id=qwen-2>Qwen-2<a hidden class=anchor aria-hidden=true href=#qwen-2>#</a></h4><p>Despite its impressive benchmarks, <strong>Qwen-2</strong> didn&rsquo;t meet my expectations in practice. It might work well in certain contexts, but it didn&rsquo;t suit my specific needs.</p><h3 id=hardware-constraints>Hardware Constraints<a hidden class=anchor aria-hidden=true href=#hardware-constraints>#</a></h3><p>Running large models on hardware with limited resources can be challenging. On my 16GB MacBook, models around <strong>7B to 8B parameters</strong> are the upper limit. Attempting to run larger models results in performance issues.</p><h2 id=exploring-uis-for-ollama>Exploring UIs for Ollama<a hidden class=anchor aria-hidden=true href=#exploring-uis-for-ollama>#</a></h2><p>Enhancing the user experience with UIs can make interacting with local LLMs more intuitive. Here&rsquo;s a look at some UIs I&rsquo;ve tried:</p><h3 id=openwebui>OpenWebUI<a hidden class=anchor aria-hidden=true href=#openwebui>#</a></h3><p><a href=https://github.com/OpenWebUI/OpenWebUI>OpenWebUI</a> offers a smooth and user-friendly interface similar to Ollama&rsquo;s default UI. It requires Docker to run efficiently, which might be a barrier for some users.</p><ul><li><strong>Features</strong>:<ul><li>Basic Retrieval-Augmented Generation (RAG) capabilities.</li><li>Connection to OpenAI APIs.</li></ul></li></ul><h3 id=page-assist>Page Assist<a hidden class=anchor aria-hidden=true href=#page-assist>#</a></h3><p><a href=https://chrome.google.com/webstore/detail/page-assist/>Page Assist</a> is a Chrome extension that I&rsquo;ve chosen for its simplicity and convenience.</p><ul><li><strong>Advantages</strong>:<ul><li>No requirement for Docker.</li><li>Accesses the current browser page as input, enabling context-aware interactions.</li></ul></li></ul><h3 id=enchanted>Enchanted<a hidden class=anchor aria-hidden=true href=#enchanted>#</a></h3><p><a href=https://apps.apple.com/app/enchanted-ai-assistant/id>Enchanted</a> is unique as it provides an iOS UI for local LLMs with support for Ollama.</p><ul><li><strong>Usage</strong>:<ul><li>By using <strong>Tailscale</strong>, I can connect it to Ollama running on my MacBook.</li><li>Serves as an alternative to Apple’s native intelligence features.</li></ul></li></ul><h3 id=anythingllm>AnythingLLM<a hidden class=anchor aria-hidden=true href=#anythingllm>#</a></h3><p><a href=https://github.com/Mintplex-Labs/anything-llm>AnythingLLM</a> offers enhanced RAG capabilities. However, in my experience, it hasn&rsquo;t performed consistently well enough for regular use.</p><h3 id=dify>Dify<a hidden class=anchor aria-hidden=true href=#dify>#</a></h3><p><a href=https://github.com/langgenius/dify>Dify</a> is a powerful and feature-rich option.</p><ul><li><strong>Pros</strong>:<ul><li>Easy to set up with an extensive feature set.</li></ul></li><li><strong>Cons</strong>:<ul><li>Resource-intensive, requiring Docker and running multiple containers like Redis and PostgreSQL.</li></ul></li></ul><h2 id=diving-into-llamaindex>Diving into llamaindex<a hidden class=anchor aria-hidden=true href=#diving-into-llamaindex>#</a></h2><p><a href=https://github.com/jerryjliu/llama_index>llamaindex</a> is geared towards developers who are comfortable writing code. While it offers robust functionalities, it does have a learning curve.</p><ul><li><strong>Observations</strong>:<ul><li>Documentation is somewhat limited, often necessitating diving into the source code.</li><li>The <code>llamaindex-cli</code> tool aims to simplify getting started but isn&rsquo;t entirely stable.<ul><li>Works seamlessly with OpenAI.</li><li>Requires code modifications to function with Ollama.</li></ul></li></ul></li></ul><h2 id=experimenting-with-candle>Experimenting with Candle<a hidden class=anchor aria-hidden=true href=#experimenting-with-candle>#</a></h2><p><strong>Candle</strong> is an intriguing project written in <strong>Rust</strong>.</p><ul><li><p><strong>Features</strong>:</p><ul><li>Uses <a href=https://huggingface.co/>Hugging Face</a> to download models.</li><li>Simple to run but exhibits slower performance compared to Ollama.</li></ul></li><li><p><strong>Additional Tools</strong>:</p><ul><li><strong>Cake</strong>: A distributed solution based on Candle, <strong>Cake</strong> opens up possibilities for scaling and extending use cases.</li></ul></li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Exploring local LLMs has been an exciting journey filled with learning and experimentation. Tools like Ollama, llamaindex, and Candle offer various pathways to harnessing the power of LLMs on personal hardware. While there are challenges, especially with hardware limitations and setup complexities, the control and privacy afforded by local models make the effort worthwhile.</p><hr><p><em>Feel free to share your experiences or ask questions in the comments below!</em></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://blog.minifish.org/posts/go-build/><span class=title>« Prev</span><br><span>The Correct Way to Use `go build`</span>
</a><a class=next href=https://blog.minifish.org/posts/tailscale/><span class=title>Next »</span><br><span>Exploring Tailscale: Building Your Own Network Easily</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Exploring Local LLMs with Ollama: My Journey and Practices on x" href="https://x.com/intent/tweet/?text=Exploring%20Local%20LLMs%20with%20Ollama%3a%20My%20Journey%20and%20Practices&amp;url=https%3a%2f%2fblog.minifish.org%2fposts%2flocal-ai%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Exploring Local LLMs with Ollama: My Journey and Practices on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fblog.minifish.org%2fposts%2flocal-ai%2f&amp;title=Exploring%20Local%20LLMs%20with%20Ollama%3a%20My%20Journey%20and%20Practices&amp;summary=Exploring%20Local%20LLMs%20with%20Ollama%3a%20My%20Journey%20and%20Practices&amp;source=https%3a%2f%2fblog.minifish.org%2fposts%2flocal-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Exploring Local LLMs with Ollama: My Journey and Practices on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fblog.minifish.org%2fposts%2flocal-ai%2f&title=Exploring%20Local%20LLMs%20with%20Ollama%3a%20My%20Journey%20and%20Practices"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Exploring Local LLMs with Ollama: My Journey and Practices on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.minifish.org%2fposts%2flocal-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Exploring Local LLMs with Ollama: My Journey and Practices on whatsapp" href="https://api.whatsapp.com/send?text=Exploring%20Local%20LLMs%20with%20Ollama%3a%20My%20Journey%20and%20Practices%20-%20https%3a%2f%2fblog.minifish.org%2fposts%2flocal-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Exploring Local LLMs with Ollama: My Journey and Practices on telegram" href="https://telegram.me/share/url?text=Exploring%20Local%20LLMs%20with%20Ollama%3a%20My%20Journey%20and%20Practices&amp;url=https%3a%2f%2fblog.minifish.org%2fposts%2flocal-ai%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Exploring Local LLMs with Ollama: My Journey and Practices on ycombinator" href="https://news.ycombinator.com/submitlink?t=Exploring%20Local%20LLMs%20with%20Ollama%3a%20My%20Journey%20and%20Practices&u=https%3a%2f%2fblog.minifish.org%2fposts%2flocal-ai%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://blog.minifish.org/>Mini Fish</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>