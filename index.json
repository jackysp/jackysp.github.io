[{"content":"I\u0026rsquo;m excited to share the final version of my Aceoffix 01, a Brompton clone bike I\u0026rsquo;ve been customizing:\nWhat Modifications Did I Make? Handlebar Grips\nThe original grips were glued on and became loose over time, so I replaced them with new, more secure ones.\nBell\nSafety first! I added a bell to alert pedestrians and other cyclists.\nCNC Mount Base for Action Cameras\nInstalled at the center of the handlebar, it\u0026rsquo;s perfect for mounting my DJI OSMO Pocket 1 or a phone holder.\nFront Bottle Bag from Decathlon Riverside\nA convenient spot for a water bottle and small items.\nFront Light\nMounted just above the front wheel to illuminate the path without blinding anyone.\nSoft Saddle from Decathlon\nFor a more comfortable ride.\nFour Universal Easy Wheels\nThese make the bike easier to push when folded. The set of four cost me 100 SGD.\nTaillight\nFor visibility and safety during night rides.\nFront Bag\nProvides additional storage space for essentials.\nAnti-Loose Accessory\nHelps keep components tight and secure.\nTool Set\nEssential for on-the-go repairs and adjustments.\nWhy Did I Choose These Modifications? I experimented with mounting accessories on the handlebar, like the bottle holder and front light, but it made the handlebar too crowded and unstable. For instance, the bottle holder wasn\u0026rsquo;t secure when attached to the handlebar. I decided to utilize existing mounts, such as the taillight bracket, to streamline the setup.\nThe additional accessories are intended to make the bike even easier to push when folded. It\u0026rsquo;s already convenient, but I thought, why not enhance it further?\nAreas for Improvement Anti-Loose Accessory: It\u0026rsquo;s not working as well as I\u0026rsquo;d hoped and might need to be replaced.\nFront Bag Straps: They\u0026rsquo;re a bit long, and I\u0026rsquo;m concerned they could get caught in the front wheel. I might need to shorten them.\nCost Breakdown Most accessories cost between 5–50 SGD, except for the universal wheels, which were 100 SGD for all four. Initially, I wasn\u0026rsquo;t sure if I needed all four wheels—maybe two would have sufficed—but I ultimately decided to get all four for better stability.\nAnything Else? I can\u0026rsquo;t think of anything else at the moment. If you have any suggestions or feedback, I\u0026rsquo;d love to hear them!\nTips for Buying Accessories in Singapore Decathlon: A great place to find affordable accessories, and they have a 365-day return policy!\nShopee: Good for finding unique or hard-to-find items.\nTaobao: Offers a wide range of accessories, but shipping can be expensive. You can ask a friend in China to help you buy and ship the items to Singapore.\nI don\u0026rsquo;t recommend buying from bike shops in Singapore, as they tend to be more expensive—about twice the price.\nLet me know if there\u0026rsquo;s anything else you\u0026rsquo;d like to know or discuss!\n","permalink":"https://blog.minifish.org/posts/brompton-modification/","summary":"\u003cp\u003eI\u0026rsquo;m excited to share the final version of my \u003cstrong\u003eAceoffix 01\u003c/strong\u003e, a Brompton clone bike I\u0026rsquo;ve been customizing:\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"20250110_181217_IMG_6530\" loading=\"lazy\" src=\"/posts/images/20250110_181217_IMG_6530.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"what-modifications-did-i-make\"\u003eWhat Modifications Did I Make?\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eHandlebar Grips\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe original grips were glued on and became loose over time, so I replaced them with new, more secure ones.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBell\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSafety first! I added a bell to alert pedestrians and other cyclists.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCNC Mount Base for Action Cameras\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eInstalled at the center of the handlebar, it\u0026rsquo;s perfect for mounting my DJI OSMO Pocket 1 or a phone holder.\u003c/p\u003e","title":"Brompton Clone Bike Modification"},{"content":"Introduction As a long-time podcaster, I\u0026rsquo;ve always enjoyed sharing my thoughts and ideas through audio. While the world of video content—and the role of a YouTuber—has its allure, the complexities of video editing have kept me anchored in the realm of podcasting. My journey has involved leveraging platforms like Spotify Creator (formerly Anchor) for hosting and distributing my recordings. This platform offers a wide array of features for free, including audio recording, editing capabilities, and automatic promotion to Spotify.\nHowever, I sought a more comprehensive solution, one that would allow me to listen to my own podcast while driving, using the Podcast app on my CarPlay device. To achieve this, I ventured into publishing on Apple Podcasts (podcastsconnect.apple.com), which also offers free hosting. With a self-designed cover and episodes uploaded, I was set—or so I thought.\nThe Challenges of Traditional Podcasting Despite having the technical setup, I faced significant challenges:\nConsistency: Maintaining a regular publishing schedule proved difficult. Voice Quality: My voice quality was inconsistent, affecting listener engagement. Content Preparation: Crafting well-structured episodes without improvisation was challenging. Enhancements: Incorporating background music and other audio elements to enrich the listening experience required additional effort. These hurdles led to my podcast being suspended for approximately two years. I found myself in need of a solution that could simplify the process and revitalize my passion for podcasting.\nDiscovering NotebookLM: An AI-Powered Podcasting Tool Recently, I stumbled upon NotebookLM (notebooklm.google.com), an innovative application developed by Google. NotebookLM harnesses the power of artificial intelligence to generate podcast content. Users can provide a topic and related documents, and the AI takes over, creating engaging podcast episodes.\nMy Experience with NotebookLM Intrigued, I decided to give NotebookLM a try. The results were nothing short of astounding:\nEffortless Production: The AI effortlessly generated a half-hour episode featuring two speakers discussing the topics in English. Enhanced Content: It went beyond the provided information, utilizing search engines to gather additional relevant data from the internet. Quality Output: The quality of the generated content was exceptionally high, surpassing what I could produce on my own. Incorporated Music: Appropriate background music was added, enhancing the overall listening experience. Cost-Free: All these features were available entirely for free. A Case Study: Deep Dive into TiDB To put NotebookLM to the test, I created an episode about TiDB, a product developed by my current employer. The process was seamless, and the final product was impressive. You can listen to the episode here: Deep Dive into TiDB.\nConclusion The integration of AI into podcast creation through tools like NotebookLM has the potential to revolutionize the way we produce content. It removes many of the barriers that podcasters face, such as time constraints, technical challenges, and the need for consistent quality.\nFor anyone looking to start or rejuvenate their podcast without the traditional hassles, I highly recommend giving NotebookLM a try. It\u0026rsquo;s remarkable to see how AI can not only match but enhance human capabilities in creative endeavors.\nI hope this helps! Let me know if there\u0026rsquo;s anything you\u0026rsquo;d like to add or modify in your blog post.\n","permalink":"https://blog.minifish.org/posts/notebooklm/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs a long-time podcaster, I\u0026rsquo;ve always enjoyed sharing my thoughts and ideas through audio. While the world of video content—and the role of a YouTuber—has its allure, the complexities of video editing have kept me anchored in the realm of podcasting. My journey has involved leveraging platforms like \u003ca href=\"https://creators.spotify.com/\"\u003e\u003cstrong\u003eSpotify Creator\u003c/strong\u003e\u003c/a\u003e (formerly \u003cstrong\u003eAnchor\u003c/strong\u003e) for hosting and distributing my recordings. This platform offers a wide array of features for free, including audio recording, editing capabilities, and automatic promotion to Spotify.\u003c/p\u003e","title":"Harnessing AI to Create High-Quality Podcasts Quickly and for Free"},{"content":"Brompton-like bikes are renowned for their compactness and portability. Unlike the typical two-fold bikes, these are trifold bikes, making them exceptionally convenient for urban commuting and travel.\nKey Features Compact Size: The trifold design results in a very small folded size. Lightweight: Weighing between 10 to 11 kg, making them easy to carry. Easy Transportation: Suitable for air travel, and can be taken on public transport like MRT and buses. Thin Tires: Facilitates easy movement and efficient riding. Front Bracket: Allows for quick changes of front bags. Popularity: Highly popular in Singapore. Non-standard Accessories: May require specific accessories due to unique design. My bike is a Brompton clone, and I\u0026rsquo;ll share some insights on the various brands and models available.\nBrompton Bikes Brompton is the original brand offering high-quality trifold bikes.\nPrice Range: Approximately 10,000 RMB to 40,000 RMB, with common models around 15,000 RMB. Models: C Line: Features 3-speed internal hub gears. P Line: Equipped with 4-speed external shift gears. Brompton Clone Brands For those seeking more affordable options, several brands offer Brompton-like bikes ranging from 3,000 to 7,000 RMB.\nAceoffix Gears: Available in 3 or 5-speed external shift gears. Frame Material: Options of steel or aluminum frames. Compatibility: Standard size compatible with Brompton accessories. Cran Gears: Offers 7 or 9-speed external shift gears. Sizes: 7-Speed: Standard size. 9-Speed: Non-standard size. Frame Material: Primarily steel; aluminum frames may have stability issues. 3Sixty One of the earliest Brompton clones. Features internal shift gears. Yubu (鱼布) Custom assembled by a Brompton enthusiast in Hangzhou. Gears: 3-speed external shift gears. Reputation: Known for high quality and reliability. Dagger A new entrant in the market. Price: Offers bikes at 3,700 RMB with features comparable to higher-priced models (\u0026gt;5,000 RMB). Value: High cost-performance ratio. Choosing the Right Bike Avoid Internal Shift Gears: While reliable, they add weight to the bike. Optimal Gears: 3 or 5-speed external gears are stable and sufficient for most needs. Portability Concerns: 9-speed models might be less convenient for transportation due to size. Standard Size Preference: Ensures compatibility with standard transfer cases and accessories. Pedals: Consider non-foldable pedals with quick-release options for convenience. Essential Accessories Lighting: Front and rear lights are mandatory by law in Singapore. Additional Gear: Bottle Bracket Bell Phone Bracket Helmet Front and Rear Lights Bag: A Brompton two-strap bag for carrying essentials. Riding Tips UV Protection: Use sun cream or wear sun-protective clothing. Saddle Upgrade: Wide Saddle: For rides longer than 1 hour. 90 degrees is ideal for the saddle angle.\nThin Saddle: Suitable for shorter rides. 60 degrees is ideal for the saddle angle.\nAccessory Shopping: Most accessories can be found at stores like Decathlon, though some items may require sourcing elsewhere due to sizing. Note: While many accessories are readily available at retailers like Decathlon, certain items like helmets may have limited sizing options.\n","permalink":"https://blog.minifish.org/posts/brompton/","summary":"\u003cp\u003eBrompton-like bikes are renowned for their compactness and portability. Unlike the typical two-fold bikes, these are trifold bikes, making them exceptionally convenient for urban commuting and travel.\u003c/p\u003e\n\u003ch2 id=\"key-features\"\u003eKey Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCompact Size\u003c/strong\u003e: The trifold design results in a very small folded size.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLightweight\u003c/strong\u003e: Weighing between \u003cstrong\u003e10 to 11 kg\u003c/strong\u003e, making them easy to carry.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEasy Transportation\u003c/strong\u003e: Suitable for air travel, and can be taken on public transport like MRT and buses.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThin Tires\u003c/strong\u003e: Facilitates easy movement and efficient riding.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFront Bracket\u003c/strong\u003e: Allows for quick changes of front bags.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePopularity\u003c/strong\u003e: Highly popular in Singapore.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNon-standard Accessories\u003c/strong\u003e: May require specific accessories due to unique design.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMy bike is a Brompton clone, and I\u0026rsquo;ll share some insights on the various brands and models available.\u003c/p\u003e","title":"Brompton-like Bikes: A Guide to Trifoldable Bicycles"},{"content":"Recently, I decided to expand my collection of folding knives and purchased three models from Sanrenmu. For anyone considering a new knife or looking for suggestions, I wanted to share my experiences with these three models: the 9201, 9008, and 820. Each has its own strengths and weaknesses, and hopefully, my insights can help you make an informed decision.\nSanrenmu 9201 Price: 198 Yuan Lock Type: Axis Lock Handle Material: Plastic Blade Material: D2 Steel First Impressions The 9201 features an axis lock mechanism, which I found to be very smooth and easy to open and close. The D2 steel blade offers good edge retention, and the plastic handle keeps the knife lightweight.\nPros Ease of Use: The axis lock allows for ambidextrous operation, making it convenient for both left and right-handed users. Lightweight: The plastic handle reduces overall weight, making it comfortable to carry. Cons Poor Detailing: Some edges and surfaces were rough, causing minor cuts to my hand during use. Finish Quality: The attention to detail was lacking, which detracted from the overall experience. Personal Modifications I used sandpaper and a rotary tool to smooth out the rough edges. While this improved the handling, I accidentally scratched the blade slightly during the process.\nSanrenmu 9008 Price: 168 Yuan Lock Type: Frame Lock with Safety Lock Handle Material: Steel Blade Material: Unknown Features: Quick-opening flipper First Impressions Out of the box, the 9008 showcased excellent workmanship. The quick-opening flipper worked flawlessly, and the safety lock adds an extra layer of security when the knife is not in use.\nPros Build Quality: The knife feels solid and well-constructed. Quick-opening Flipper: Provides rapid deployment of the blade. Safety Lock: Prevents accidental opening. Cons Weight: The steel handle makes it quite heavy, which isn\u0026rsquo;t ideal for everyday carry. Button Quality: The button mechanism isn\u0026rsquo;t as smooth as I would like. Blade Material: Unspecified steel that doesn\u0026rsquo;t hold an edge as well as higher-end materials. Usage Due to its weight, I decided to keep the 9008 in my car. It features a window-breaking hammer, making it a practical tool for emergencies.\nSanrenmu 820 Price: 298 Yuan Lock Type: Frame Lock Handle Material: Titanium Blade Material: VG10 Steel Features: Ball bearings (no safety lock or flipper) First Impressions The 820 stands out with its titanium handle and VG10 blade, offering a premium feel. It operates on ball bearings, ensuring a smooth action when opening and closing.\nPros Material Quality: The VG10 blade and titanium handle are high-quality materials that enhance durability and aesthetics. Workmanship: Excellent fit and finish. Cons Opening Mechanism Issues:\nThe stop pin is too small. The lock interface is large and deep, making one-handed opening difficult. Pocket Clip:\nThe clip is excessively tight, making it hard to attach to pockets easily. Personal Modifications I disassembled the 820 and polished the lock interface, significantly improving the ease of one-handed opening. Post-modification, the knife opens smoothly, though there\u0026rsquo;s a slight looseness when closed.\nReflections and Insights Axis Lock vs. Frame Lock:\nAxis Lock (9201): Offers ambidextrous use and works well with a plastic handle but has a complex structure that\u0026rsquo;s challenging to disassemble. Frame Lock (820 \u0026amp; 9008): Simpler design and easier to disassemble (especially the 820 after some practice). However, it\u0026rsquo;s more suited to one-handed use and relies heavily on precise craftsmanship for smooth operation. Handle Materials:\nTitanium Handle (820): Initially underestimated, the titanium handle provides a superior feel compared to plastic or steel. Once accustomed to it, switching back to other materials feels like a downgrade. Steel Handle (9008): Durable but adds unnecessary weight, making it less ideal for carrying around. Plastic Handle (9201): Lightweight but doesn\u0026rsquo;t offer the same premium feel as metal handles. Practicality vs. Aesthetics:\nThe 9008, while heavy, serves a practical purpose in the car for emergencies. The 820, after modifications, has become my preferred everyday carry despite the tight pocket clip. DIY Modifications:\nPersonalizing and improving the knives was a rewarding experience, even if there were minor mishaps like scratching the blade on the 9201. Disassembling and reassembling the 820 has turned it into not just a tool but a hobbyist\u0026rsquo;s toy. Conclusion Each knife has its place:\nSanrenmu 9008: Stays in the car for emergency situations, thanks to its sturdy build and window-breaking feature. Sanrenmu 820: Becomes my go-to carry knife after modifications. Its high-quality materials and improved functionality make it stand out. Sanrenmu 9201: Although I\u0026rsquo;m unsure of its future use, it might serve as an additional car knife or a backup. If you\u0026rsquo;re considering a Sanrenmu folding knife, think about what features matter most to you—be it the locking mechanism, handle material, or ease of carry. Also, don\u0026rsquo;t shy away from making personal adjustments to tailor the knife to your preferences.\nNote: This review is based on personal experiences and modifications. Results may vary depending on individual usage and skills.\n","permalink":"https://blog.minifish.org/posts/folding-knives/","summary":"\u003cp\u003eRecently, I decided to expand my collection of folding knives and purchased three models from \u003cstrong\u003e\u003ca href=\"#\"\u003eSanrenmu\u003c/a\u003e\u003c/strong\u003e. For anyone considering a new knife or looking for suggestions, I wanted to share my experiences with these three models: the 9201, 9008, and 820. Each has its own strengths and weaknesses, and hopefully, my insights can help you make an informed decision.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"sanrenmu-9201\"\u003eSanrenmu 9201\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePrice:\u003c/strong\u003e 198 Yuan\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLock Type:\u003c/strong\u003e Axis Lock\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHandle Material:\u003c/strong\u003e Plastic\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBlade Material:\u003c/strong\u003e D2 Steel\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"first-impressions\"\u003eFirst Impressions\u003c/h3\u003e\n\u003cp\u003eThe 9201 features an axis lock mechanism, which I found to be \u003cstrong\u003every smooth and easy to open and close\u003c/strong\u003e. The D2 steel blade offers good edge retention, and the plastic handle keeps the knife lightweight.\u003c/p\u003e","title":"My Recent Purchases: A Review of Three Sanrenmu Folding Knives"},{"content":"When working with Go, it\u0026rsquo;s important to know the proper way to compile your programs to avoid common errors. Here are some tips on using the go build command effectively.\nRecommended Usage Compile all Go files in the current directory:\ngo build Compile all Go files explicitly:\ngo build *.go Common Pitfalls Compiling a Single File Running:\ngo build main.go will only build main.go. This can lead to errors if main.go depends on other Go files in the same package, as those files won\u0026rsquo;t be included in the build process.\nIncluding Non-Go Files Using:\ngo build * will cause an error if the * wildcard includes non-Go files. The compiler will output an error message stating that only Go files can be compiled. This serves as a precise reminder to exclude non-Go files from the build command.\nBy using go build correctly, you can ensure that all necessary files in your package are compiled together, avoiding missing dependencies and other common compilation issues.\n","permalink":"https://blog.minifish.org/posts/go-build/","summary":"\u003cp\u003eWhen working with Go, it\u0026rsquo;s important to know the proper way to compile your programs to avoid common errors. Here are some tips on using the \u003ccode\u003ego build\u003c/code\u003e command effectively.\u003c/p\u003e\n\u003ch2 id=\"recommended-usage\"\u003eRecommended Usage\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCompile all Go files in the current directory:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ego build\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCompile all Go files explicitly:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ego build *.go\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"common-pitfalls\"\u003eCommon Pitfalls\u003c/h2\u003e\n\u003ch3 id=\"compiling-a-single-file\"\u003eCompiling a Single File\u003c/h3\u003e\n\u003cp\u003eRunning:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ego build main.go\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ewill only build \u003ccode\u003emain.go\u003c/code\u003e. This can lead to errors if \u003ccode\u003emain.go\u003c/code\u003e depends on other Go files in the same package, as those files won\u0026rsquo;t be included in the build process.\u003c/p\u003e","title":"The Correct Way to Use `go build`"},{"content":"Local Large Language Models (LLMs) have been gaining traction as developers and enthusiasts seek more control over their AI tools without relying solely on cloud-based solutions. In this blog post, I\u0026rsquo;ll share my experiences with Ollama, a remarkable tool for running local LLMs, along with other tools like llamaindex and Candle. I\u0026rsquo;ll also discuss various user interfaces (UI) that enhance the local LLM experience.\nTable of Contents Introduction to Ollama A Popular Choice Ease of Use Built with Golang My Practices with Ollama Preferred Models Llama 3.1 Mistral Phi-3 Qwen-2 Hardware Constraints Exploring UIs for Ollama OpenWebUI Page Assist Enchanted AnythingLLM Dify Diving into llamaindex Experimenting with Candle Conclusion Introduction to Ollama A Popular Choice Ollama has rapidly become a favorite among developers interested in local LLMs. Within a year, it has garnered significant attention on GitHub, reflecting its growing user base and community support.\nEase of Use One of Ollama\u0026rsquo;s standout features is its simplicity. It\u0026rsquo;s as easy to use as Docker, making it accessible even to those who may not be deeply familiar with machine learning frameworks. The straightforward command-line interface allows users to download and run models with minimal setup.\nBuilt with Golang Ollama is written in Golang, ensuring performance and efficiency. Golang\u0026rsquo;s concurrency features contribute to Ollama\u0026rsquo;s ability to handle tasks effectively, which is crucial when working with resource-intensive LLMs.\nMy Practices with Ollama Preferred Models Llama 3.1 I\u0026rsquo;ve found that Llama 3.1 works exceptionally well with Ollama. It\u0026rsquo;s my go-to choice due to its performance and compatibility.\nMistral While Mistral also performs well, it hasn\u0026rsquo;t gained as much popularity as Llama. Nevertheless, it\u0026rsquo;s a solid option worth exploring.\nPhi-3 Developed by Microsoft, Phi-3 is both fast and efficient. The 2B parameter model strikes a balance between size and performance, making it one of the best small-sized LLMs available.\nQwen-2 Despite its impressive benchmarks, Qwen-2 didn\u0026rsquo;t meet my expectations in practice. It might work well in certain contexts, but it didn\u0026rsquo;t suit my specific needs.\nHardware Constraints Running large models on hardware with limited resources can be challenging. On my 16GB MacBook, models around 7B to 8B parameters are the upper limit. Attempting to run larger models results in performance issues.\nExploring UIs for Ollama Enhancing the user experience with UIs can make interacting with local LLMs more intuitive. Here\u0026rsquo;s a look at some UIs I\u0026rsquo;ve tried:\nOpenWebUI OpenWebUI offers a smooth and user-friendly interface similar to Ollama\u0026rsquo;s default UI. It requires Docker to run efficiently, which might be a barrier for some users.\nFeatures: Basic Retrieval-Augmented Generation (RAG) capabilities. Connection to OpenAI APIs. Page Assist Page Assist is a Chrome extension that I\u0026rsquo;ve chosen for its simplicity and convenience.\nAdvantages: No requirement for Docker. Accesses the current browser page as input, enabling context-aware interactions. Enchanted Enchanted is unique as it provides an iOS UI for local LLMs with support for Ollama.\nUsage: By using Tailscale, I can connect it to Ollama running on my MacBook. Serves as an alternative to Apple’s native intelligence features. AnythingLLM AnythingLLM offers enhanced RAG capabilities. However, in my experience, it hasn\u0026rsquo;t performed consistently well enough for regular use.\nDify Dify is a powerful and feature-rich option.\nPros: Easy to set up with an extensive feature set. Cons: Resource-intensive, requiring Docker and running multiple containers like Redis and PostgreSQL. Diving into llamaindex llamaindex is geared towards developers who are comfortable writing code. While it offers robust functionalities, it does have a learning curve.\nObservations: Documentation is somewhat limited, often necessitating diving into the source code. The llamaindex-cli tool aims to simplify getting started but isn\u0026rsquo;t entirely stable. Works seamlessly with OpenAI. Requires code modifications to function with Ollama. Experimenting with Candle Candle is an intriguing project written in Rust.\nFeatures:\nUses Hugging Face to download models. Simple to run but exhibits slower performance compared to Ollama. Additional Tools:\nCake: A distributed solution based on Candle, Cake opens up possibilities for scaling and extending use cases. Conclusion Exploring local LLMs has been an exciting journey filled with learning and experimentation. Tools like Ollama, llamaindex, and Candle offer various pathways to harnessing the power of LLMs on personal hardware. While there are challenges, especially with hardware limitations and setup complexities, the control and privacy afforded by local models make the effort worthwhile.\nFeel free to share your experiences or ask questions in the comments below!\n","permalink":"https://blog.minifish.org/posts/local-ai/","summary":"\u003cp\u003eLocal Large Language Models (LLMs) have been gaining traction as developers and enthusiasts seek more control over their AI tools without relying solely on cloud-based solutions. In this blog post, I\u0026rsquo;ll share my experiences with \u003cstrong\u003eOllama\u003c/strong\u003e, a remarkable tool for running local LLMs, along with other tools like \u003cstrong\u003ellamaindex\u003c/strong\u003e and \u003cstrong\u003eCandle\u003c/strong\u003e. I\u0026rsquo;ll also discuss various user interfaces (UI) that enhance the local LLM experience.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#introduction-to-ollama\"\u003eIntroduction to Ollama\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#a-popular-choice\"\u003eA Popular Choice\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ease-of-use\"\u003eEase of Use\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#built-with-golang\"\u003eBuilt with Golang\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#my-practices-with-ollama\"\u003eMy Practices with Ollama\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#preferred-models\"\u003ePreferred Models\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#llama-31\"\u003eLlama 3.1\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#mistral\"\u003eMistral\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#phi-3\"\u003ePhi-3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#qwen-2\"\u003eQwen-2\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#hardware-constraints\"\u003eHardware Constraints\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#exploring-uis-for-ollama\"\u003eExploring UIs for Ollama\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#openwebui\"\u003eOpenWebUI\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#page-assist\"\u003ePage Assist\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#enchanted\"\u003eEnchanted\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#anythingllm\"\u003eAnythingLLM\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dify\"\u003eDify\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#diving-into-llamaindex\"\u003eDiving into llamaindex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#experimenting-with-candle\"\u003eExperimenting with Candle\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#conclusion\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"introduction-to-ollama\"\u003eIntroduction to Ollama\u003c/h2\u003e\n\u003ch3 id=\"a-popular-choice\"\u003eA Popular Choice\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jmorganca/ollama\"\u003eOllama\u003c/a\u003e has rapidly become a favorite among developers interested in local LLMs. Within a year, it has garnered significant attention on GitHub, reflecting its growing user base and community support.\u003c/p\u003e","title":"Exploring Local LLMs with Ollama: My Journey and Practices"},{"content":"I recently started experimenting with Tailscale, a tool that has significantly simplified the way I manage my personal network across devices. In this blog post, I\u0026rsquo;ll share how I discovered Tailscale, its core features, and my personal setup that leverages this powerful tool.\nDiscovering Tailscale Through WebVM My journey with Tailscale began when I came across WebVM, an impressive project that allows you to run a virtual machine directly in your browser. Intrigued by the possibilities, I delved deeper and discovered that Tailscale could help me create a seamless, private network across all my devices.\nWhat is Tailscale? Tailscale is a mesh VPN network built on top of WireGuard, specifically using the WireGuard-go implementation. It allows you to create a secure, encrypted network between your devices, no matter where they are located.\nKey Features Free Plan Available: Tailscale offers a free plan that is sufficient for personal use, allowing up to 20 devices. Ease of Use: Setting up Tailscale is straightforward. With minimal configuration, you can have your own network up and running quickly. Cross-Platform Support: Tailscale works exceptionally well across the Apple ecosystem, including iOS, tvOS, and macOS. Magic DNS Service: It provides a built-in DNS service that makes it easy to address your devices by name. Performance on Different Platforms While Tailscale shines on Apple devices, in my experience, it hasn\u0026rsquo;t performed as well on Windows. I encountered some connectivity and stability issues on Windows machines, which may vary based on individual setups.\nMy Tailscale Setup Here\u0026rsquo;s how I leveraged Tailscale to connect my devices and access my home network seamlessly.\nRunning Tailscale on Apple TV I installed Tailscale on my Apple TV, which stays online 24/7. This makes it an excellent candidate for a consistently available node in my network.\nEnabling Subnet Routing: By enabling subnet routing on the Apple TV, I can access other devices on the same local network, such as my NAS and router, as if I were connected locally. Setting Up an Exit Node: I configured the Apple TV as an exit node, allowing me to route internet traffic through my home network. This is useful when I need to access geo-restricted content or ensure a secure connection. Connecting Other Devices I also installed Tailscale on my MacBook and iPhone, which allows all my personal devices to communicate over the secure network, no matter where I am.\nBenefits I\u0026rsquo;ve Enjoyed Secure Remote Access: I can securely access my home network devices from anywhere. Consistent Environment: All my devices appear on the same network, simplifying file sharing and remote management. No Need for Complex VPN Setups: Tailscale eliminates the need for traditional VPN configurations, port forwarding, or dynamic DNS services. Conclusion Tailscale has transformed the way I interact with my devices across different locations. Its ease of use and robust feature set make it an excellent choice for anyone looking to create a personal, secure network.\nIf you\u0026rsquo;re interested in simplifying your network setup and want a hassle-free way to connect your devices, I highly recommend giving Tailscale a try.\nLinks:\nTailscale Official Website WebVM Project on GitHub WireGuard-go on GitHub Note: This post reflects my personal experiences with Tailscale. Performance may vary based on individual configurations and devices.\n","permalink":"https://blog.minifish.org/posts/tailscale/","summary":"\u003cp\u003eI recently started experimenting with \u003cstrong\u003eTailscale\u003c/strong\u003e, a tool that has significantly simplified the way I manage my personal network across devices. In this blog post, I\u0026rsquo;ll share how I discovered Tailscale, its core features, and my personal setup that leverages this powerful tool.\u003c/p\u003e\n\u003ch2 id=\"discovering-tailscale-through-webvm\"\u003eDiscovering Tailscale Through WebVM\u003c/h2\u003e\n\u003cp\u003eMy journey with Tailscale began when I came across \u003ca href=\"https://github.com/leaningtech/webvm\"\u003eWebVM\u003c/a\u003e, an impressive project that allows you to run a virtual machine directly in your browser. Intrigued by the possibilities, I delved deeper and discovered that Tailscale could help me create a seamless, private network across all my devices.\u003c/p\u003e","title":"Exploring Tailscale: Building Your Own Network Easily"},{"content":"Vietnam is a vibrant country with a rich culture, friendly people, and stunning landscapes. Recently, I spent some time exploring Vietnam, and I wanted to share some of my experiences and tips to help fellow travelers make the most of their visit.\nInternet Access and Blocked Websites Slow Access to Twitter: If you\u0026rsquo;re an avid Twitter user, be prepared for slower connection speeds. The platform doesn\u0026rsquo;t perform as well in Vietnam. Inaccessible Websites: Certain websites are blocked in Vietnam, including: Medium Some porn websites Tip: If you rely on any of these platforms, consider downloading content beforehand or using a VPN to access them during your stay. Visa Process for Chinese Citizens E-Visa Conversion: Travelers from China holding an E-Visa need to convert it into a paper visa upon arrival. Bring Two Photos: Ensure you have two passport-sized photos ready for the visa conversion process. Carry Cash for Tips: It\u0026rsquo;s customary to offer tips during the visa processing, so have some cash (preferably small denominations) on hand. Exploring Saigon (Ho Chi Minh City) City Vibes: Saigon reminded me of a blend between Shanghai and Qingdao. Cleanliness: While the city is bustling with energy, it\u0026rsquo;s not as clean as Shanghai or Qingdao. Abundance of Coffee Shops: Vietnam is famous for its coffee culture. You\u0026rsquo;ll find numerous coffee shops on almost every street corner. Must-Try: Don\u0026rsquo;t miss out on traditional Vietnamese iced coffee (Cà Phê Sữa Đá). Affordability Cheaper Than Bangkok: Compared to cities like Bangkok, Saigon is significantly more affordable. Hotels: Accommodation can be about half the price of equivalent hotels in Bangkok. General Expenses: Food, transportation, and entertainment are also reasonably priced. Currency and Payments Accepted Currencies: Vietnamese Dong (VND) is the official currency. US Dollars (USD) are widely accepted and sometimes preferred. Chinese RMB can be used in some places, but USD is more commonly accepted. Tip: Carrying USD can be convenient, but always have some local currency for small purchases. Local Customs and Observations Clothing Choices: You\u0026rsquo;ll notice many locals wearing long-sleeved clothing and pants, even in hot weather. Reason: This is to prevent sunburn and protect their skin from the sun. Hardworking People: The Vietnamese are known for their strong work ethic. Streets are lively with activity from early morning until late at night. Affordable Goods: Many items, from street food to souvenirs, are priced affordably. Non-Spicy Cuisine: Similar to Cantonese Food: The flavors are mild and focus on the freshness of ingredients. Must-Try Dishes: Pho (noodle soup), Banh Mi (baguette sandwich), and fresh spring rolls. Final Thoughts Vietnam offers a captivating blend of cultural experiences, historical sites, and modern attractions. Whether you\u0026rsquo;re sipping coffee in a cozy café, exploring bustling markets, or simply taking in the sights and sounds of the city, there\u0026rsquo;s something for every traveler.\nTravel Tips:\nPlan Ahead: Be aware of the internet limitations and plan accordingly if you need access to certain websites. Stay Hydrated: The climate can be hot and humid, so drink plenty of water. Respect Local Customs: Dress modestly when visiting temples or rural areas. I hope these insights help you in planning your trip to Vietnam. It\u0026rsquo;s a destination filled with unforgettable experiences!\nSafe travels!\n","permalink":"https://blog.minifish.org/posts/vietnam/","summary":"\u003cp\u003eVietnam is a vibrant country with a rich culture, friendly people, and stunning landscapes. Recently, I spent some time exploring Vietnam, and I wanted to share some of my experiences and tips to help fellow travelers make the most of their visit.\u003c/p\u003e\n\u003ch2 id=\"internet-access-and-blocked-websites\"\u003eInternet Access and Blocked Websites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSlow Access to Twitter\u003c/strong\u003e: If you\u0026rsquo;re an avid Twitter user, be prepared for slower connection speeds. The platform doesn\u0026rsquo;t perform as well in Vietnam.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInaccessible Websites\u003c/strong\u003e: Certain websites are blocked in Vietnam, including:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMedium\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eSome porn websites\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTip\u003c/strong\u003e: If you rely on any of these platforms, consider downloading content beforehand or using a VPN to access them during your stay.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"visa-process-for-chinese-citizens\"\u003eVisa Process for Chinese Citizens\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eE-Visa Conversion\u003c/strong\u003e: Travelers from China holding an E-Visa need to convert it into a paper visa upon arrival.\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBring Two Photos\u003c/strong\u003e: Ensure you have two passport-sized photos ready for the visa conversion process.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCarry Cash for Tips\u003c/strong\u003e: It\u0026rsquo;s customary to offer tips during the visa processing, so have some cash (preferably small denominations) on hand.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"exploring-saigon-ho-chi-minh-city\"\u003eExploring Saigon (Ho Chi Minh City)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCity Vibes\u003c/strong\u003e: Saigon reminded me of a blend between \u003cstrong\u003eShanghai\u003c/strong\u003e and \u003cstrong\u003eQingdao\u003c/strong\u003e.\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCleanliness\u003c/strong\u003e: While the city is bustling with energy, it\u0026rsquo;s not as clean as Shanghai or Qingdao.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAbundance of Coffee Shops\u003c/strong\u003e: Vietnam is famous for its coffee culture. You\u0026rsquo;ll find numerous coffee shops on almost every street corner.\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMust-Try\u003c/strong\u003e: Don\u0026rsquo;t miss out on traditional Vietnamese iced coffee (\u003cstrong\u003eCà Phê Sữa Đá\u003c/strong\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"affordability\"\u003eAffordability\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCheaper Than Bangkok\u003c/strong\u003e: Compared to cities like Bangkok, Saigon is significantly more affordable.\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHotels\u003c/strong\u003e: Accommodation can be about \u003cstrong\u003ehalf the price\u003c/strong\u003e of equivalent hotels in Bangkok.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGeneral Expenses\u003c/strong\u003e: Food, transportation, and entertainment are also reasonably priced.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"currency-and-payments\"\u003eCurrency and Payments\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAccepted Currencies\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eVietnamese Dong (VND)\u003c/strong\u003e is the official currency.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUS Dollars (USD)\u003c/strong\u003e are widely accepted and sometimes preferred.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eChinese RMB\u003c/strong\u003e can be used in some places, but USD is more commonly accepted.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTip\u003c/strong\u003e: Carrying USD can be convenient, but always have some local currency for small purchases.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"local-customs-and-observations\"\u003eLocal Customs and Observations\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eClothing Choices\u003c/strong\u003e: You\u0026rsquo;ll notice many locals wearing long-sleeved clothing and pants, even in hot weather.\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReason\u003c/strong\u003e: This is to \u003cstrong\u003eprevent sunburn\u003c/strong\u003e and protect their skin from the sun.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHardworking People\u003c/strong\u003e: The Vietnamese are known for their strong work ethic. Streets are lively with activity from early morning until late at night.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAffordable Goods\u003c/strong\u003e: Many items, from street food to souvenirs, are priced affordably.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNon-Spicy Cuisine\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSimilar to Cantonese Food\u003c/strong\u003e: The flavors are mild and focus on the freshness of ingredients.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMust-Try Dishes\u003c/strong\u003e: Pho (noodle soup), Banh Mi (baguette sandwich), and fresh spring rolls.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"final-thoughts\"\u003eFinal Thoughts\u003c/h2\u003e\n\u003cp\u003eVietnam offers a captivating blend of cultural experiences, historical sites, and modern attractions. Whether you\u0026rsquo;re sipping coffee in a cozy café, exploring bustling markets, or simply taking in the sights and sounds of the city, there\u0026rsquo;s something for every traveler.\u003c/p\u003e","title":"Tips for Traveling in Vietnam: My Personal Experiences"},{"content":"Note: All prices mentioned are in RMB.\nIntroduction Over the years, I\u0026rsquo;ve gathered a lot of experience in fishing, especially in selecting the right gear. Choosing the appropriate rod, reel, line, and lure can significantly impact your fishing success. In this blog, I want to share my insights to help you make informed decisions when selecting fishing equipment.\nRod Selection Understanding Rod Actions L (Light) Action Rods: Best for casting lures weighing 4-5g. Ideal for using lures within half of the rod\u0026rsquo;s lure weight range (e.g., for a rod rated 2-7g, the optimal lure weight is around 4-5g). UL (Ultra-Light) Action Rods: Suitable for casting lures weighing 3g or less. One-Piece vs. Multi-Piece Rods One-Piece Rods: Offer better sensitivity and casting performance. However, they are less portable and harder to transport. Not as easy to sell if you decide to upgrade. Multi-Piece Rods: More convenient for travel. Slight trade-off in sensitivity compared to one-piece rods. Recommended Action The best action for spinning cast rods is Light (L) action. Reel Selection Spinning Reels vs. Baitcasting Reels Spinning Reels: Easier for casting, especially for beginners. Better suited for saltwater fishing than baitcasting reels. Can use heavier action rods to cast small lures effectively. Baitcasting Reels: Look cool and offer precision, but have a steeper learning curve. Not as user-friendly for casting light lures. Reel Size and Line Compatibility For a 2500S reel: 0.8 PE line is sufficient. Using 0.6 PE line on a 2500S reel may require too much line to fill the spool. For a 2000S reel: 0.6 PE line is appropriate. It\u0026rsquo;s not common to use PE lines smaller than 0.6, as they are less widely available. Light Weight Reels Always choose lightweight reels for better balance and less fatigue during fishing. Line Selection PE Line vs. Fluorocarbon Casting Difficulty: 1.5 PE and fluorocarbon lines offer similar casting challenges. PE Line Advantages: Thinner diameter for the same strength compared to fluorocarbon. Essential for longer casting distances. Avoiding Line Connections: Use a Carolina rig to eliminate the need to connect PE line and fluorocarbon leader. Recommended Lines YGK PE Line: Highly recommended for its quality. Performs much better than Sufix lines. Consider using 0.6 or 0.8 PE from YGK. Fluorocarbon Line: 1.5 Fluorocarbon can be used as a leader if necessary. Rigs and Lures Carolina Rig: Allows you to use PE line without needing to connect it to a fluorocarbon leader. Effective for various fishing conditions. Recommendations Final Gear Choices Rod: L Action Spinning Rod with multiple pieces for portability. Reel: 2000S or 2500S spinning reel, prioritize lightweight models. Line: 0.6 or 0.8 YGK PE line (lean towards 0.6 for more capacity). Optional 1.5 Fluorocarbon leader. Rig: Use a Carolina rig setup. Brands and Models Chinese Domestic Market (CDM) Rods Few high-quality CDM spinning rods are available. The best CDM spinning rod is the 翠鸟 (Kingfisher) from 钓之屋 (Fishing House). The 游侠 (the larger version of 翠鸟) is not as good. Example: A 2500S reel on a 游侠 lacks proper rod skewness, affecting casting performance. Shimano Rods and Reels Shimano spinning rods and reels are superior in performance. The reels have features that prevent accidental rolling after opening the bail, allowing you to stop the line with your finger for better control. Reels I\u0026rsquo;ve Used Shimano Vanquish 2500S (2023 model): Extremely lightweight and smooth. Feels almost too slim, raising concerns about durability. The infinite loop system is excellent. Shimano Stradic 2500S (2023 model): Heavier compared to Vanquish. Handle design is not as comfortable. Appearance gives a sense of reliability. Shimano Vanford C2000SHG (2020 model): Great performance but similar concerns about durability due to plastic components. Feels less robust than the Stradic but is lighter. Price Comparison (in RMB) 钓之屋 翠鸟 (Kingfisher): 360 Shimano Stradic: 900 Shimano Vanford: 1,100 Shimano Vanquish: 2,600 Shimano Stella: 3,800 Conclusion Choosing the right fishing gear is crucial for an enjoyable and successful fishing experience. By considering the rod action, reel type and size, line selection, and the right rigs, you can optimize your setup for the best performance. While high-end brands like Shimano offer top-quality equipment, there are also cost-effective options available. Remember to prioritize what suits your fishing style and comfort.\nHappy fishing!\n","permalink":"https://blog.minifish.org/posts/fishing-gears/","summary":"\u003cp\u003e\u003cem\u003eNote: All prices mentioned are in RMB.\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eOver the years, I\u0026rsquo;ve gathered a lot of experience in fishing, especially in selecting the right gear. Choosing the appropriate rod, reel, line, and lure can significantly impact your fishing success. In this blog, I want to share my insights to help you make informed decisions when selecting fishing equipment.\u003c/p\u003e\n\u003ch2 id=\"rod-selection\"\u003eRod Selection\u003c/h2\u003e\n\u003ch3 id=\"understanding-rod-actions\"\u003eUnderstanding Rod Actions\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eL (Light) Action Rods\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eBest for casting lures weighing \u003cstrong\u003e4-5g\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eIdeal for using lures within half of the rod\u0026rsquo;s lure weight range (e.g., for a rod rated \u003cstrong\u003e2-7g\u003c/strong\u003e, the optimal lure weight is around \u003cstrong\u003e4-5g\u003c/strong\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUL (Ultra-Light) Action Rods\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eSuitable for casting lures weighing \u003cstrong\u003e3g\u003c/strong\u003e or less.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"one-piece-vs-multi-piece-rods\"\u003eOne-Piece vs. Multi-Piece Rods\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eOne-Piece Rods\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eOffer better sensitivity and casting performance.\u003c/li\u003e\n\u003cli\u003eHowever, they are less portable and harder to transport.\u003c/li\u003e\n\u003cli\u003eNot as easy to sell if you decide to upgrade.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMulti-Piece Rods\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eMore convenient for travel.\u003c/li\u003e\n\u003cli\u003eSlight trade-off in sensitivity compared to one-piece rods.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"recommended-action\"\u003eRecommended Action\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003cstrong\u003ebest action\u003c/strong\u003e for spinning cast rods is \u003cstrong\u003eLight (L)\u003c/strong\u003e action.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"reel-selection\"\u003eReel Selection\u003c/h2\u003e\n\u003ch3 id=\"spinning-reels-vs-baitcasting-reels\"\u003eSpinning Reels vs. Baitcasting Reels\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpinning Reels\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eEasier for casting, especially for beginners.\u003c/li\u003e\n\u003cli\u003eBetter suited for saltwater fishing than baitcasting reels.\u003c/li\u003e\n\u003cli\u003eCan use heavier action rods to cast small lures effectively.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBaitcasting Reels\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eLook cool and offer precision, but have a steeper learning curve.\u003c/li\u003e\n\u003cli\u003eNot as user-friendly for casting light lures.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"reel-size-and-line-compatibility\"\u003eReel Size and Line Compatibility\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFor a \u003cstrong\u003e2500S\u003c/strong\u003e reel:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e0.8 PE\u003c/strong\u003e line is sufficient.\u003c/li\u003e\n\u003cli\u003eUsing \u003cstrong\u003e0.6 PE\u003c/strong\u003e line on a 2500S reel may require too much line to fill the spool.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eFor a \u003cstrong\u003e2000S\u003c/strong\u003e reel:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e0.6 PE\u003c/strong\u003e line is appropriate.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eIt\u0026rsquo;s not common to use PE lines smaller than \u003cstrong\u003e0.6\u003c/strong\u003e, as they are less widely available.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"light-weight-reels\"\u003eLight Weight Reels\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAlways choose \u003cstrong\u003elightweight reels\u003c/strong\u003e for better balance and less fatigue during fishing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"line-selection\"\u003eLine Selection\u003c/h2\u003e\n\u003ch3 id=\"pe-line-vs-fluorocarbon\"\u003ePE Line vs. Fluorocarbon\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCasting Difficulty\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e1.5 PE\u003c/strong\u003e and fluorocarbon lines offer similar casting challenges.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePE Line Advantages\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eThinner diameter for the same strength compared to fluorocarbon.\u003c/li\u003e\n\u003cli\u003eEssential for longer casting distances.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAvoiding Line Connections\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eUse a \u003cstrong\u003eCarolina rig\u003c/strong\u003e to eliminate the need to connect PE line and fluorocarbon leader.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"recommended-lines\"\u003eRecommended Lines\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eYGK PE Line\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eHighly recommended for its quality.\u003c/li\u003e\n\u003cli\u003ePerforms much better than Sufix lines.\u003c/li\u003e\n\u003cli\u003eConsider using \u003cstrong\u003e0.6 or 0.8 PE\u003c/strong\u003e from YGK.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFluorocarbon Line\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e1.5 Fluorocarbon\u003c/strong\u003e can be used as a leader if necessary.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"rigs-and-lures\"\u003eRigs and Lures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCarolina Rig\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eAllows you to use PE line without needing to connect it to a fluorocarbon leader.\u003c/li\u003e\n\u003cli\u003eEffective for various fishing conditions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"recommendations\"\u003eRecommendations\u003c/h2\u003e\n\u003ch3 id=\"final-gear-choices\"\u003eFinal Gear Choices\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRod\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eL Action Spinning Rod\u003c/strong\u003e with multiple pieces for portability.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReel\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e2000S or 2500S\u003c/strong\u003e spinning reel, prioritize lightweight models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLine\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e0.6 or 0.8 YGK PE line\u003c/strong\u003e (lean towards 0.6 for more capacity).\u003c/li\u003e\n\u003cli\u003eOptional \u003cstrong\u003e1.5 Fluorocarbon\u003c/strong\u003e leader.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRig\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eUse a \u003cstrong\u003eCarolina rig\u003c/strong\u003e setup.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"brands-and-models\"\u003eBrands and Models\u003c/h2\u003e\n\u003ch3 id=\"chinese-domestic-market-cdm-rods\"\u003eChinese Domestic Market (CDM) Rods\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFew high-quality CDM spinning rods are available.\u003c/li\u003e\n\u003cli\u003eThe best CDM spinning rod is the \u003cstrong\u003e翠鸟 (Kingfisher)\u003c/strong\u003e from \u003cstrong\u003e钓之屋 (Fishing House)\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003e游侠\u003c/strong\u003e (the larger version of 翠鸟) is not as good.\n\u003cul\u003e\n\u003cli\u003eExample: A 2500S reel on a 游侠 lacks proper rod skewness, affecting casting performance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"shimano-rods-and-reels\"\u003eShimano Rods and Reels\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eShimano\u003c/strong\u003e spinning rods and reels are superior in performance.\u003c/li\u003e\n\u003cli\u003eThe reels have features that prevent accidental rolling after opening the bail, allowing you to stop the line with your finger for better control.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"reels-ive-used\"\u003eReels I\u0026rsquo;ve Used\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eShimano Vanquish 2500S (2023 model)\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eExtremely lightweight and smooth.\u003c/li\u003e\n\u003cli\u003eFeels almost too slim, raising concerns about durability.\u003c/li\u003e\n\u003cli\u003eThe infinite loop system is excellent.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShimano Stradic 2500S (2023 model)\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eHeavier compared to Vanquish.\u003c/li\u003e\n\u003cli\u003eHandle design is not as comfortable.\u003c/li\u003e\n\u003cli\u003eAppearance gives a sense of reliability.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShimano Vanford C2000SHG (2020 model)\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eGreat performance but similar concerns about durability due to plastic components.\u003c/li\u003e\n\u003cli\u003eFeels less robust than the Stradic but is lighter.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"price-comparison-in-rmb\"\u003ePrice Comparison (in RMB)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e钓之屋 翠鸟 (Kingfisher)\u003c/strong\u003e: \u003cstrong\u003e360\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShimano Stradic\u003c/strong\u003e: \u003cstrong\u003e900\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShimano Vanford\u003c/strong\u003e: \u003cstrong\u003e1,100\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShimano Vanquish\u003c/strong\u003e: \u003cstrong\u003e2,600\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShimano Stella\u003c/strong\u003e: \u003cstrong\u003e3,800\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eChoosing the right fishing gear is crucial for an enjoyable and successful fishing experience. By considering the rod action, reel type and size, line selection, and the right rigs, you can optimize your setup for the best performance. While high-end brands like Shimano offer top-quality equipment, there are also cost-effective options available. Remember to prioritize what suits your fishing style and comfort.\u003c/p\u003e","title":"How to Choose the Right Fishing Gear: Tips from My Fishing Experiences"},{"content":"Today, I planned to use Warp to select an IP exit for my VPS, following the Cloudflare official documentation. When executing the warp-cli connect step, the server immediately lost connection, and the problem persisted even after rebooting.\nAfter researching, I found that this problem is not unique. For instance, in a discussion on V2EX, many users encountered similar issues. The solution is to run warp-cli set-mode proxy before executing warp-cli connect to bypass the local address. Surprisingly, Cloudflare\u0026rsquo;s official documentation does not mention this crucial step, undoubtedly increasing the complexity and risk of configuration.\nIn the process of exploring solutions, I found that some users suggested repairing by rebuilding the instance or using VNC connection. However, since I am using AWS Lightsail, VNC is not applicable. Ultimately, I decided to try the method mentioned in this article: creating a snapshot backup of the current VPS, then creating a new instance from the snapshot, and loading a script to execute warp-cli set-mode proxy when the new instance starts.\nAfter checking the existing instance, I found that no snapshot had been created. This discovery reminded me of the importance of regular backups. Without other options, I could only attempt a snapshot backup as guided by the aforementioned article. However, no matter what startup script command I tried, it failed to execute successfully. The execution result of AWS Lightsail\u0026rsquo;s startup script is not visible, making problem-solving more difficult.\nIn near desperation, I found an old snapshot dated 2022 on the snapshot page. Although this snapshot was created using old technology, and many important updates might be lost after recovery, it was my last hope. After starting the snapshot recovery process, I unexpectedly discovered through the history command that this snapshot contained all the important updates. This discovery allowed the entire recovery process to be completed smoothly.\nThis experience re-emphasized the importance of backups. Careful backups from the past ultimately avoided severe data loss. Furthermore, AWS\u0026rsquo;s static IP retention feature also played a crucial role. The new instance could immediately bind to the IP once the old instance released the static IP, achieving a seamless switch.\nConclusion Backups are essential: Regular backups are key to ensuring stable system operations. Operate with caution: Before executing critical commands, thoroughly review and understand relevant documentation and user feedback to avoid potential risks. Trust your past self: Meticulous work done in the past can often prove invaluable at critical moments. I hope this experience can serve as a reference and help for others, preventing similar issues from occurring.\n","permalink":"https://blog.minifish.org/posts/warp-cli/","summary":"\u003cp\u003eToday, I planned to use Warp to select an IP exit for my VPS, following the \u003ca href=\"https://developers.cloudflare.com/warp-client/get-started/linux/\"\u003eCloudflare official documentation\u003c/a\u003e. When executing the \u003ccode\u003ewarp-cli connect\u003c/code\u003e step, the server immediately lost connection, and the problem persisted even after rebooting.\u003c/p\u003e\n\u003cp\u003eAfter researching, I found that this problem is not unique. For instance, in a \u003ca href=\"https://www.v2ex.com/t/933725\"\u003ediscussion on V2EX\u003c/a\u003e, many users encountered similar issues. The solution is to run \u003ccode\u003ewarp-cli set-mode proxy\u003c/code\u003e before executing \u003ccode\u003ewarp-cli connect\u003c/code\u003e to bypass the local address. Surprisingly, Cloudflare\u0026rsquo;s official documentation does not mention this crucial step, undoubtedly increasing the complexity and risk of configuration.\u003c/p\u003e","title":"Encounter with a Major Issue with Cloudflare Warp: A Life and Death Rescue for VPS"},{"content":"Background Many customers have TLS enabled, which is different from the lab environment.\nCurl Curl requires a specified CA certificate, otherwise it will report an error.\ncurl --cacert ca.crt https://127.0.0.1:10080/status Wget Many containers do not have curl, so wget is used instead. Wget is better as it does not require a CA certificate.\nwget --no-check-certificate http://127.0.0.1:10080/status ","permalink":"https://blog.minifish.org/posts/tls-api/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eMany customers have TLS enabled, which is different from the lab environment.\u003c/p\u003e\n\u003ch2 id=\"curl\"\u003eCurl\u003c/h2\u003e\n\u003cp\u003eCurl requires a specified CA certificate, otherwise it will report an error.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecurl --cacert ca.crt https://127.0.0.1:10080/status\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"wget\"\u003eWget\u003c/h2\u003e\n\u003cp\u003eMany containers do not have curl, so wget is used instead. Wget is better as it does not require a CA certificate.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ewget --no-check-certificate http://127.0.0.1:10080/status\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"How to Use the HTTP API in TiDB with TLS Enabled"},{"content":"Background After moving house, there are many more devices at home that need internet access. However, I don\u0026rsquo;t want to configure a proxy on each device, so I thought of using a transparent gateway.\nTransparent Gateway After some research, I found that the easiest way is to use the premium version of Clash, although I didn\u0026rsquo;t know when Clash released a premium version. I mainly referred to this article. It\u0026rsquo;s much simpler than setting up iptables.\nNetwork Topology I have a 10-year-old Thinkpad x230 at home, which is perfect for this purpose. Here is a simple topology diagram.\nRouter1 is a fiber-optic modem with routing capabilities, Router2 is a regular router, with the gateway and DNS pointing to the Thinkpad, where Linux is running to act as a transparent gateway with Clash on top.\n+------------+ | | | Internet | | | +-----+------+ | +-----+------+ | | +----------+ Router1 +-----------+ | | | | | +------------+ | | | | | +-----+-----+ +-----+------+ | | | | +----------+ Router2 +----------+ | Thinkpad | | | | | | | | +-----+-----+ | +------------+ | | | | | | | | | +----+-----+ +----+-----+ +-----+-----+ | | | | | | | Mac | | iPad | | iPhone | | | | | | | +----------+ +----------+ +-----------+ Add DNS Section in Clash Configuration dns: enable: true listen: 0.0.0.0:53 enhanced-mode: fake-ip nameserver: - 114.114.114.114 fallback: - 8.8.8.8 Clash tun Feature Section tun: enable: true stack: system # or gvisor dns-hijack: - any:53 - tcp://any:53 auto-route: true auto-detect-interface: true For traffic forwarding, simply edit /etc/sysctl.conf on the Thinkpad and add net.ipv4.ip_forward=1, then execute sysctl -p to apply it. After that, point the gateway and DNS of Router2 to the Thinkpad, and you\u0026rsquo;re done.\nNetwork Protocols Initially, I used native HTTP2 for unblocking, but it cannot proxy UDP. When only a few devices need unblocking, it doesn\u0026rsquo;t matter whether UDP is used, but with many devices at home, some of them can only use UDP. I considered socks + tls, but it didn\u0026rsquo;t feel secure and required opening odd ports like UDP 443. It felt like giving away my intentions. Eventually, I chose Trojan, which essentially mimics native HTTPS. Trojan has two versions; I used Trojan-go simply because I didn\u0026rsquo;t want to manage dependencies. Also, I\u0026rsquo;m more familiar with Go.\nTrojan-go has a requirement for a genuinely accessible HTTP server, so I used the simplest Python http.server. Back in Python 2, it was called simplehttp. You can simply use python3 -m http.server 80 and optionally add --directory to specify a directory.\nAdditionally, Trojan-go requires the client to fill in the SNI, which means using the domain used during key application. Therefore, prerequisites like applying for the domain, applying for Let\u0026rsquo;s Encrypt certificates, and configuring crontab must all be completed. There\u0026rsquo;s a learning curve, but I had done it before, so I just skipped that part.\nFor the client part, you can use Clash directly, and refer to here for guidance.\n","permalink":"https://blog.minifish.org/posts/transparent-gateway/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eAfter moving house, there are many more devices at home that need internet access. However, I don\u0026rsquo;t want to configure a proxy on each device, so I thought of using a transparent gateway.\u003c/p\u003e\n\u003ch2 id=\"transparent-gateway\"\u003eTransparent Gateway\u003c/h2\u003e\n\u003cp\u003eAfter some research, I found that the easiest way is to use the premium version of Clash, although I didn\u0026rsquo;t know when Clash released a premium version. I mainly referred to \u003ca href=\"https://www.cfmem.com/2022/05/clash.html\"\u003ethis article\u003c/a\u003e. It\u0026rsquo;s much simpler than setting up iptables.\u003c/p\u003e","title":"How to Deploy a Secure Transparent Gateway"},{"content":"Background Recently, both of my kids have become interested in Minecraft, and some of their peers are also playing it. We previously bought the Switch version, but its online capabilities are quite poor, and the device performance is subpar, resulting in a less-than-ideal experience. Thus, I began considering the idea of setting up our own server. Of course, you can play in multiplayer with friends, but the game ends as soon as the host goes offline, which is not as good as having a server that is always online.\nFor version selection, there is the NetEase version, the official Java version, and the Bedrock version. Based on my previous understanding, the NetEase version has all sorts of anti-addiction regulations, so that\u0026rsquo;s a no-go. The Java version server setup seems to rely on third-party launchers, resembling piracy to some extent. Therefore, I decided on the Bedrock version. Another reason for choosing the Bedrock version is its origins as the mobile version of Minecraft, and Microsoft later expanded support to more platforms, making it the most versatile. Additionally, its core is written in C++, which should offer better performance and resource efficiency.\nDownloading the Server Software I downloaded the Ubuntu version. While I also downloaded the Windows version and managed to run it, I’m not experienced in managing Windows Server, and the Windows version has some quirks. I won’t delve into those here.\nLocal Network Server Initially, I intended to use an old Thinkpad, but couldn\u0026rsquo;t find it, and ended up using an old Macbook. After updating everything, I found that the old Macbook was still quite robust, outperforming the Thinkpad. So, I set up a virtual machine. I discovered that Ubuntu Server no longer supports direct ISO downloads, requiring Multipass to launch instead.\nWhat is Multipass?\nDeveloped by the Ubuntu team, it is a lightweight virtualization platform. Multipass consists of multipass and multipassd. The former provides both GUI and CLI, while the latter requires root permissions and runs in the background. It can be downloaded directly or installed via brew cask.\nMultipass seems to only install Ubuntu Server, with some customization, such as automatically creating a user named \u0026ldquo;ubuntu\u0026rdquo; and key pairing. Once installed, you get a 1C 1GB virtual machine called \u0026ldquo;primary\u0026rdquo; that automatically mounts the user’s home directory.\nmultipassd is more of a pluggable virtual hypervisor supporting hyperkit and qemu by default, with hyperkit intended for Intel macOS and qemu for M1.\nmultipassd can also use Virtualbox as a backend hypervisor, which is more recommended because it offers bridging capabilities, whereas relying solely on port mapping would be cumbersome. It\u0026rsquo;s not that qemu doesn\u0026rsquo;t support bridging, but there’s an easy-to-follow Virtualbox bridging tutorial on Multipass’s official documentation.\nI set up Virtualbox first. I must say, Oracle is generous here as it\u0026rsquo;s still free to use. Then, I followed the steps from the link above. It\u0026rsquo;s essential to reboot the machine after running the first step sudo multipass set local.driver=virtualbox, as the variable might not take effect immediately. Otherwise, the primary created will still use the old backend. Since there\u0026rsquo;s already a default virtual machine, I didn\u0026rsquo;t want to create another one to avoid extra resource usage. Additionally, note a few things:\nFor modifying primary configuration, I didn’t find a way to do it using Multipass, so I used vbox’s command sudo VBoxManage controlvm \u0026quot;primary\u0026quot; --cpus 2 --memory 4096 (where memory is in MB). The primary mount point in Multipass can automatically unmount in some situations, resulting in errors. Therefore, Minecraft data shouldn’t be stored in the mount point permanently to avoid core dumps. After extracting the server files, just start it in tmux/screen.\nLastly, I used Amphetamine from the App Store to ensure that the laptop continues working when closed by disabling the default sleep setting after the screen is closed. You’ll see some warnings when doing this, but just be aware of them.\nUpdate:\nFor systemd startup, you can refer to this gist. The server\u0026rsquo;s console has some commands, like \u0026ldquo;stop\u0026rdquo;, that perform graceful shutdowns, so it\u0026rsquo;s necessary to rely on a screen session to create it. Additionally, the server startup has some environmental dependencies, so it’s best to write a small script for starting it, like:\n#!/bin/bash cd /home/user/bedrock-server LD_LIBRARY_PATH=. ./bedrock_server Using absolute paths can lead to core dumps.\nPublic Network Server For public network servers, the choices are to map the internal network to the public network or purchase a cloud server. Due to security concerns, I opted to buy a cloud server. Although the official site mentioned Ubuntu support, I found that Debian servers also run without issues. The only thing to note is that the Bedrock version uses UDP, and China Unicom’s 5G network disables UDP, so without WiFi, you cannot connect.\nServer Monitoring To monitor the server’s status, monitoring is necessary, especially for cloud servers, as provider information might be insufficient. I used Grafana Cloud.\nUsing the free version is fine, just follow the guide to select a Linux server integration, then run the Grafana Agent installation and verification on the server to be monitored. Note to change the hostname in the Grafana Agent configuration file to differentiate between different servers, which acts as a label.\nI understand that Grafana Agent is a lightweight node exporter with some Prometheus functionalities, but it can also remote write, so there\u0026rsquo;s no need to worry about the disk filling up.\n","permalink":"https://blog.minifish.org/posts/mc-server/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eRecently, both of my kids have become interested in Minecraft, and some of their peers are also playing it. We previously bought the Switch version, but its online capabilities are quite poor, and the device performance is subpar, resulting in a less-than-ideal experience. Thus, I began considering the idea of setting up our own server. Of course, you can play in multiplayer with friends, but the game ends as soon as the host goes offline, which is not as good as having a server that is always online.\u003c/p\u003e","title":"How to Set Up a Minecraft Bedrock Server Using Multipass"},{"content":"Background One day, I impulsively turned on GitHub\u0026rsquo;s Vigilant mode.\nAs a result, all my commits started looking like this.\nTo figure out how to make them Verified, I found the following method.\nMethod I actually referred to this link. However, it wasn\u0026rsquo;t quite enough, as there might be authentication-related issues on MacBooks that lead to commit errors. So, I found this solution.\nIn summary, to verify, you need to enter a password. The issue on a Mac is the prompt for entering the password, which needs to be replaced with pinentry-mac, which most people install via homebrew.\nMoreover, this solution thoughtfully provides a way to verify:\necho \u0026#34;test\u0026#34; | gpg --clearsign GPG Experience It doesn\u0026rsquo;t replace the ssh key. After successfully setting it up, I deleted my GitHub ssh key and discovered that I couldn\u0026rsquo;t log in. Actually, it only verifies the legitimacy of commits. On the local machine, in any repo, you only need to enter the password once, and that makes it a verified commit. It doesn\u0026rsquo;t affect daily use; it just adds a green check mark for verification. Using the https protocol + token seems more reliable than this method, but I\u0026rsquo;m not sure if it provides a verified mark. ","permalink":"https://blog.minifish.org/posts/gpg/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eOne day, I impulsively turned on GitHub\u0026rsquo;s Vigilant mode.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"test\" loading=\"lazy\" src=\"/posts/images/2022-02-12-12.02.07.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAs a result, all my commits started looking like this.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"test\" loading=\"lazy\" src=\"/posts/images/2022-02-12-12.11.01.png\"\u003e\u003c/p\u003e\n\u003cp\u003eTo figure out how to make them Verified, I found the following method.\u003c/p\u003e\n\u003ch2 id=\"method\"\u003eMethod\u003c/h2\u003e\n\u003cp\u003eI actually referred to this \u003ca href=\"https://zhuanlan.zhihu.com/p/76861431\"\u003elink\u003c/a\u003e. However, it wasn\u0026rsquo;t quite enough, as there might be authentication-related issues on MacBooks that lead to commit errors. So, I found this \u003ca href=\"https://stackoverflow.com/a/40066889\"\u003esolution\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn summary, to verify, you need to enter a password. The issue on a Mac is the prompt for entering the password, which needs to be replaced with pinentry-mac, which most people install via homebrew.\u003c/p\u003e","title":"How to Create GitHub Verified Commits on a MacBook M1"},{"content":"Background During the Chinese New Year holiday, I spent some time at home playing on the Switch with my child, and I ended up purchasing Minecraft (Bedrock Edition). In order to play together, I also bought the Windows version. Additionally, I wanted to test if the GitHub Actions for this blog are still working properly.\nPreparation You need two Microsoft accounts, so I dug out my wife\u0026rsquo;s long-unused Hotmail account.\nMultiplayer Restart the game on the Switch and log into the Microsoft account. It has to be restarted each time, and I\u0026rsquo;m not sure why. Add each other as friends using the short ID. Create a world on the Switch; Windows should join (it doesn\u0026rsquo;t work the other way around, and I\u0026rsquo;m not sure why). Enjoy playing together! This guide is brief and straightforward, with nothing much more to add.\n","permalink":"https://blog.minifish.org/posts/mc/","summary":"\u003ch3 id=\"background\"\u003eBackground\u003c/h3\u003e\n\u003cp\u003eDuring the Chinese New Year holiday, I spent some time at home playing on the Switch with my child, and I ended up purchasing Minecraft (Bedrock Edition). In order to play together, I also bought the Windows version. Additionally, I wanted to test if the GitHub Actions for this blog are still working properly.\u003c/p\u003e\n\u003ch3 id=\"preparation\"\u003ePreparation\u003c/h3\u003e\n\u003cp\u003eYou need two Microsoft accounts, so I dug out my wife\u0026rsquo;s long-unused Hotmail account.\u003c/p\u003e","title":"How to Play Minecraft (Bedrock Edition) Cross-Platform"},{"content":"Background Recently, I encountered a problem where a user wanted to synchronize data from PostgreSQL to TiDB (which uses the same protocol as MySQL) and wanted to know whether the data after synchronization is consistent. I hadn\u0026rsquo;t dealt with this kind of issue before, so I did a bit of research.\nTypically, to verify data consistency, you compute a checksum on both sides and compare them.\nTiDB (MySQL) Side For the verification of a specific table, the following SQL is used:\nSELECT bit_xor( CAST(crc32( concat_ws(\u0026#39;,\u0026#39;, col1, col2, col3, …, colN, concat(isnull(col1), isnull(col2), …, isnull(colN)) ) ) AS UNSIGNED) ) FROM t; Let\u0026rsquo;s look at a specific example:\nDROP TABLE IF EXISTS t; CREATE TABLE t (i INT, j INT); INSERT INTO t VALUES (2, 3), (NULL, NULL); SELECT bit_xor( CAST(crc32( concat_ws(\u0026#39;,\u0026#39;, i, j, concat(isnull(i), isnull(j)) ) ) AS UNSIGNED) ) FROM t; The result is:\n+-------------------------------------------------------------------------------------------------------------------------------------------+ | bit_xor( CAST(crc32( concat_ws(\u0026#39;,\u0026#39;, i, j, concat(isnull(i), isnull(j)) ) ) AS UNSIGNED) ) | +-------------------------------------------------------------------------------------------------------------------------------------------+ | 5062371 | +-------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.00 sec) PostgreSQL Side The goal is simply to write the same SQL as above, but PostgreSQL does not support bit_xor, crc32, isnull, nor does it have unsigned types. Therefore, the solution is relatively straightforward—relying on UDFs (User-Defined Functions).\nAfter some research, the main missing functions can be addressed with a few custom implementations.\nbit_xor:\nCREATE OR REPLACE AGGREGATE bit_xor(IN v bigint) (SFUNC = int8xor, STYPE = bigint); crc32:\nCREATE OR REPLACE FUNCTION crc32(text_string text) RETURNS bigint AS $$ DECLARE tmp bigint; i int; j int; byte_length int; binary_string bytea; BEGIN IF text_string = \u0026#39;\u0026#39; THEN RETURN 0; END IF; i = 0; tmp = 4294967295; byte_length = bit_length(text_string) / 8; binary_string = decode(replace(text_string, E\u0026#39;\\\\\u0026#39;, E\u0026#39;\\\\\\\\\u0026#39;), \u0026#39;escape\u0026#39;); LOOP tmp = (tmp # get_byte(binary_string, i))::bigint; i = i + 1; j = 0; LOOP tmp = ((tmp \u0026gt;\u0026gt; 1) # (3988292384 * (tmp \u0026amp; 1)))::bigint; j = j + 1; IF j \u0026gt;= 8 THEN EXIT; END IF; END LOOP; IF i \u0026gt;= byte_length THEN EXIT; END IF; END LOOP; RETURN (tmp # 4294967295); END $$ IMMUTABLE LANGUAGE plpgsql; isnull:\nCREATE OR REPLACE FUNCTION isnull(anyelement) RETURNS int AS $$ BEGIN RETURN CAST(($1 IS NULL) AS INT); END $$ LANGUAGE plpgsql; After creating the three UDFs above, let\u0026rsquo;s test the previous example. Note that UNSIGNED should be changed to BIGINT.\nDROP TABLE IF EXISTS t; CREATE TABLE t (i INT, j INT); INSERT INTO t VALUES (2, 3), (NULL, NULL); SELECT bit_xor( CAST(crc32( concat_ws(\u0026#39;,\u0026#39;, i, j, concat(isnull(i), isnull(j)) ) ) AS BIGINT) ) FROM t; The result:\nbit_xor --------- 5062371 (1 row) It\u0026rsquo;s exactly the same as on the TiDB (MySQL) side.\nPostscript I haven\u0026rsquo;t tested more extensively; this is just a simple test. UDFs are indeed a great feature that greatly enhance flexibility. ","permalink":"https://blog.minifish.org/posts/diff-mysql-pg/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eRecently, I encountered a problem where a user wanted to synchronize data from PostgreSQL to TiDB (which uses the same protocol as MySQL) and wanted to know whether the data after synchronization is consistent. I hadn\u0026rsquo;t dealt with this kind of issue before, so I did a bit of research.\u003c/p\u003e\n\u003cp\u003eTypically, to verify data consistency, you compute a checksum on both sides and compare them.\u003c/p\u003e\n\u003ch2 id=\"tidb-mysql-side\"\u003eTiDB (MySQL) Side\u003c/h2\u003e\n\u003cp\u003eFor the verification of a specific table, the following SQL is used:\u003c/p\u003e","title":"How to Compare Data Consistency between MySQL and PostgreSQL"},{"content":"Background Initially, I didn\u0026rsquo;t think this was something worth writing about, because CTex was previously working smoothly for everyone. However, it turns out that CTex hasn\u0026rsquo;t been updated since 2016. So, I wanted to find a replacement for Chinese LaTeX on Windows in 2021.\nConfiguration Method Install MiKTeX (TexLive should work as well). MiKTeX can automatically download dependency packages and you can also proactively install the ctex package. Install the VSCode LaTeX extension. Configure the LaTeX extension. I found a powerful configuration on Zhihu, but it’s quite complex, so I simplified it a bit. A brief explanation: the core components here are the recipes and tools. Tools are the compilation toolchain, specifying which tools to use for compilation and the options to use, without regard to order. Recipes define how to combine the above tools to generate the final document, where the order does matter. I\u0026rsquo;ve put XeLaTex first here because it\u0026rsquo;s the most compatible for compiling Chinese. If you use pdflatex to compile Chinese documents, you\u0026rsquo;re likely to encounter issues.\n{ \u0026#34;latex-workshop.latex.recipes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;XeLaTeX\u0026#34;, \u0026#34;tools\u0026#34;: [ \u0026#34;xelatex\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;latexmk 🔃\u0026#34;, \u0026#34;tools\u0026#34;: [ \u0026#34;latexmk\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;latexmk (latexmkrc)\u0026#34;, \u0026#34;tools\u0026#34;: [ \u0026#34;latexmk_rconly\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;latexmk (lualatex)\u0026#34;, \u0026#34;tools\u0026#34;: [ \u0026#34;lualatexmk\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;pdflatex ➞ bibtex ➞ pdflatex × 2\u0026#34;, \u0026#34;tools\u0026#34;: [ \u0026#34;pdflatex\u0026#34;, \u0026#34;bibtex\u0026#34;, \u0026#34;pdflatex\u0026#34;, \u0026#34;pdflatex\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;Compile Rnw files\u0026#34;, \u0026#34;tools\u0026#34;: [ \u0026#34;rnw2tex\u0026#34;, \u0026#34;latexmk\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;Compile Jnw files\u0026#34;, \u0026#34;tools\u0026#34;: [ \u0026#34;jnw2tex\u0026#34;, \u0026#34;latexmk\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;tectonic\u0026#34;, \u0026#34;tools\u0026#34;: [ \u0026#34;tectonic\u0026#34; ] } ], \u0026#34;latex-workshop.latex.tools\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;xelatex\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;xelatex\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-synctex=1\u0026#34;, \u0026#34;-interaction=nonstopmode\u0026#34;, \u0026#34;-file-line-error\u0026#34;, \u0026#34;%DOCFILE%\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;latexmk\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;latexmk\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-synctex=1\u0026#34;, \u0026#34;-interaction=nonstopmode\u0026#34;, \u0026#34;-file-line-error\u0026#34;, \u0026#34;-pdf\u0026#34;, \u0026#34;-outdir=%OUTDIR%\u0026#34;, \u0026#34;%DOC%\u0026#34; ], \u0026#34;env\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;lualatexmk\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;latexmk\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-synctex=1\u0026#34;, \u0026#34;-interaction=nonstopmode\u0026#34;, \u0026#34;-file-line-error\u0026#34;, \u0026#34;-lualatex\u0026#34;, \u0026#34;-outdir=%OUTDIR%\u0026#34;, \u0026#34;%DOC%\u0026#34; ], \u0026#34;env\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;latexmk_rconly\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;latexmk\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;%DOC%\u0026#34; ], \u0026#34;env\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;pdflatex\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;pdflatex\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-synctex=1\u0026#34;, \u0026#34;-interaction=nonstopmode\u0026#34;, \u0026#34;-file-line-error\u0026#34;, \u0026#34;%DOC%\u0026#34; ], \u0026#34;env\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;bibtex\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;bibtex\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;%DOCFILE%\u0026#34; ], \u0026#34;env\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;rnw2tex\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;Rscript\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-e\u0026#34;, \u0026#34;knitr::opts_knit$set(concordance = TRUE); knitr::knit(\u0026#39;%DOCFILE_EXT%\u0026#39;)\u0026#34; ], \u0026#34;env\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;jnw2tex\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;julia\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-e\u0026#34;, \u0026#34;using Weave; weave(\\\u0026#34;%DOC_EXT%\\\u0026#34;, doctype=\\\u0026#34;tex\\\u0026#34;)\u0026#34; ], \u0026#34;env\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;jnw2texmintex\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;julia\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-e\u0026#34;, \u0026#34;using Weave; weave(\\\u0026#34;%DOC_EXT%\\\u0026#34;, doctype=\\\u0026#34;texminted\\\u0026#34;)\u0026#34; ], \u0026#34;env\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;tectonic\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;tectonic\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;--synctex\u0026#34;, \u0026#34;--keep-logs\u0026#34;, \u0026#34;%DOC%.tex\u0026#34; ], \u0026#34;env\u0026#34;: {} } ], \u0026#34;latex-workshop.view.pdf.viewer\u0026#34;: \u0026#34;tab\u0026#34; } Create a folder. Use VSCode to open this folder and create a .tex file with the following content: \\documentclass[UTF8]{ctexart} \\begin{document} OK, it’s all set... \\end{document} It should automatically start compiling. Confirm to download any required dependencies if prompted. The compiled effect is as follows:\n","permalink":"https://blog.minifish.org/posts/latex-win/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eInitially, I didn\u0026rsquo;t think this was something worth writing about, because CTex was previously working smoothly for everyone. However, it turns out that CTex hasn\u0026rsquo;t been updated since 2016. So, I wanted to find a replacement for Chinese LaTeX on Windows in 2021.\u003c/p\u003e\n\u003ch2 id=\"configuration-method\"\u003eConfiguration Method\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eInstall MiKTeX (TexLive should work as well). MiKTeX can automatically download dependency packages and you can also proactively install the ctex package.\u003c/li\u003e\n\u003cli\u003eInstall the VSCode LaTeX extension.\u003c/li\u003e\n\u003cli\u003eConfigure the LaTeX extension. I found a powerful configuration on Zhihu, but it’s quite complex, so I simplified it a bit.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eA brief explanation: the core components here are the recipes and tools. Tools are the compilation toolchain, specifying which tools to use for compilation and the options to use, without regard to order. Recipes define how to combine the above tools to generate the final document, where the order does matter. I\u0026rsquo;ve put XeLaTex first here because it\u0026rsquo;s the most compatible for compiling Chinese. If you use pdflatex to compile Chinese documents, you\u0026rsquo;re likely to encounter issues.\u003c/p\u003e","title":"How to Configure a Chinese LaTeX Environment on Windows"},{"content":"In this post, I used Travis to enable automatic blog publishing. However, I recently discovered that Travis does not run automatically anymore (though it works manually). I haven’t looked into it closely because GitHub Actions have been introduced, so I decided to move all dependencies to GitHub.\nGo Action Click on Actions on the repository page, and then New Workflow to see recommended actions. Since this blog uses Go code, it shows the Go Action.\nThe rest involves following Travis\u0026rsquo;s approach to set up the workflow.\nCheck out the blog\u0026rsquo;s repo Check out the publishing site repo make Commit the changes to the publishing site Note that write permissions are required for the publishing site, so you need to configure a token, similar to Travis.\nGenerate a token with only repo permissions Go to a particular repo and set up secrets (enter the token). Ideally, this should be set up on the publishing site, but it works when set in the blog repo. I haven\u0026rsquo;t explored why yet. You will need two actions in total: one is GitHub\u0026rsquo;s own checkout, and the other is a third-party action called “Push directory to another repository”. There might be better options available, and I’ll explore them when I have more time.\nFinally, here is my simple GitHub Action CI file:\nname: CI on: push: branches: [ master ] jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Set up Go 1.x uses: actions/setup-go@v2 with: go-version: ^1.13 - name: Check out code into the Go module directory uses: actions/checkout@v2 - name: Get dependencies run: | go get -v -t -d ./... if [ -f Gopkg.toml ]; then curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh dep ensure fi - name: Check out my other private repo uses: actions/checkout@v2 with: repository: jackysp/jackysp.github.io token: ${{ secrets.UPDATE_BLOG }} path: public - name: Build run: make - name: Pushes to another repository id: public uses: cpina/github-action-push-to-another-repository@cp_instead_of_deleting env: API_TOKEN_GITHUB: ${{ secrets.UPDATE_BLOG }} with: source-directory: \u0026#39;public\u0026#39; destination-github-username: \u0026#39;jackysp\u0026#39; destination-repository-name: \u0026#39;jackysp.github.io\u0026#39; user-email: your@email.com commit-message: See ORIGIN_COMMIT ","permalink":"https://blog.minifish.org/posts/github-action/","summary":"\u003cp\u003eIn \u003ca href=\"/posts/travis-git-push\"\u003ethis post\u003c/a\u003e, I used Travis to enable automatic blog publishing. However, I recently discovered that Travis does not run automatically anymore (though it works manually). I haven’t looked into it closely because GitHub Actions have been introduced, so I decided to move all dependencies to GitHub.\u003c/p\u003e\n\u003ch2 id=\"go-action\"\u003eGo Action\u003c/h2\u003e\n\u003cp\u003eClick on Actions on the repository page, and then New Workflow to see recommended actions. Since this blog uses Go code, it shows the Go Action.\u003c/p\u003e","title":"How to Automatically Publish a Blog Using GitHub Actions"},{"content":"Sysbench is a tool commonly used in database testing. Since version 1.0, it has supported more powerful custom functions, allowing users to conveniently write some Lua scripts to simulate load. The purpose of writing this article is, firstly, because I wanted to explore Sysbench\u0026rsquo;s custom load usage. Secondly, because I tried the mysqlslap tool provided by MySQL\u0026rsquo;s official source, and found that it freezes easily during database performance testing, which could mislead users into thinking there is an issue with the database, causing trouble for many. Therefore, I want to help people avoid these pitfalls.\nA Simple Example #!/usr/bin/env sysbench require(\u0026#34;oltp_common\u0026#34;) function prepare_statements() end function event() con:query(\u0026#34;set autocommit = 1\u0026#34;) end The first line require includes Sysbench\u0026rsquo;s built-in basic library; the empty prepare_statement is a callback function from oltp_common that must be present; the specific execution of a single load is implemented in the event function.\nSave this script as a Lua file, for example, named set.lua, and then execute it using sysbench.\nsysbench --config-file=config --threads=100 set.lua --tables=1 --table_size=1000000 run You can use the above command. Of course, here --tables=1 and --table_size=1000000 are not useful for this load, so they are optional. --threads controls concurrency.\n$ cat config time=120 db-driver=mysql mysql-host=172.16.5.33 mysql-port=34000 mysql-user=root mysql-db=sbtest report-interval=10 In the config file, parameters you don\u0026rsquo;t frequently adjust are written once to avoid having a long string of parameters in the command line. These are required fields: time represents the test duration, report-interval is used to observe real-time performance results, and the others pertain to how to connect to the database.\nThe running output generally looks like:\n[ 10s ] thds: 100 tps: 94574.34 qps: 94574.34 (r/w/o: 0.00/0.00/94574.34) lat (ms,95%): 3.68 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 100 tps: 77720.30 qps: 77720.30 (r/w/o: 0.00/0.00/77720.30) lat (ms,95%): 5.28 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 100 tps: 56080.10 qps: 56080.10 (r/w/o: 0.00/0.00/56080.10) lat (ms,95%): 9.22 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 100 tps: 93315.90 qps: 93315.90 (r/w/o: 0.00/0.00/93315.90) lat (ms,95%): 4.82 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 100 tps: 97491.02 qps: 97491.02 (r/w/o: 0.00/0.00/97491.02) lat (ms,95%): 4.65 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 100 tps: 94034.27 qps: 94034.27 (r/w/o: 0.00/0.00/94034.27) lat (ms,95%): 4.91 err/s: 0.00 reconn/s: 0.00 [ 70s ] thds: 100 tps: 74707.37 qps: 74707.37 (r/w/o: 0.00/0.00/74707.37) lat (ms,95%): 6.79 err/s: 0.00 reconn/s: 0.00 [ 80s ] thds: 100 tps: 89485.10 qps: 89485.10 (r/w/o: 0.00/0.00/89485.10) lat (ms,95%): 5.18 err/s: 0.00 reconn/s: 0.00 [ 90s ] thds: 100 tps: 109296.44 qps: 109296.44 (r/w/o: 0.00/0.00/109296.44) lat (ms,95%): 2.91 err/s: 0.00 reconn/s: 0.00 Finally, there will be a summary report.\nSQL statistics: queries performed: read: 0 write: 0 other: 10424012 total: 10424012 transactions: 10424012 (86855.65 per sec.) queries: 10424012 (86855.65 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) Throughput: events/s (eps): 86855.6517 time elapsed: 120.0154s total number of events: 10424012 Latency (ms): min: 0.09 avg: 1.15 max: 1527.74 95th percentile: 4.91 sum: 11994122.49 Threads fairness: events (avg/stddev): 104240.1200/600.21 execution time (avg/stddev): 119.9412/0.01 ","permalink":"https://blog.minifish.org/posts/sysbench/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/akopytov/sysbench\"\u003eSysbench\u003c/a\u003e is a tool commonly used in database testing. Since version 1.0, it has supported more powerful custom functions, allowing users to conveniently write some Lua scripts to simulate load. The purpose of writing this article is, firstly, because I wanted to explore Sysbench\u0026rsquo;s custom load usage. Secondly, because I tried the mysqlslap tool provided by MySQL\u0026rsquo;s official source, and found that it freezes easily during database performance testing, which could mislead users into thinking there is an issue with the database, causing trouble for many. Therefore, I want to help people avoid these pitfalls.\u003c/p\u003e","title":"How to Implement a Simple Load Using Sysbench"},{"content":"When using TiDB, you may occasionally encounter some exceptions, such as the \u0026ldquo;Lost connection to MySQL server during query\u0026rdquo; error. This indicates that the connection between the client and the database has been disconnected (not due to user action). The reasons for disconnection can vary. This article attempts to analyze some common TiDB errors from the perspective of exception handling and code analysis. Additionally, some exceptions are not errors but performance issues due to slow execution. In the second half of this article, we will also introduce common tools for tracking performance.\nLost Connection There are generally three reasons for a Lost Connection:\nA timeout occurs either directly between the client and the database or at some point along the intermediate link, such as from the client to the Proxy or from the Proxy to the database. A bug occurs during SQL execution, which can generally be recovered, thus preventing the TiDB server from crashing completely (panic). TiDB itself crashes, often due to excessive memory use, causing an OOM (Out of Memory), or a user deliberately kills TiDB. Another possibility is an unrecovered bug, which typically appears more frequently in background threads. Timeout Direct Timeout TiDB supports the MySQL-compatible wait_timeout variable, with a default value of 0, meaning no timeout is set, unlike MySQL\u0026rsquo;s default of 8 hours.\nThe only place it is used in the code is in getSessionVarsWaitTimeout. In the connection\u0026rsquo;s Run section, its value is set for packet IO. If the variable is non-zero, a timeout is set before each readPacket.\nIf the client does not send data beyond the specified time, the connection will be disconnected. At this time, a log message \u0026ldquo;read packet timeout, close this connection\u0026rdquo; will appear, along with the specific timeout duration.\nIntermediate Link Timeout Another scenario is an intermediate link timeout. A normal timeout in an intermediate link (proxy) typically returns an EOF error to the database. In older versions, at least a connection closed log would be output.\nIn the newer master version, product managers suggested changing this log to a debug level, so it is generally no longer output.\nHowever, in the new version, a monitoring item called DisconnectionCounter has been added,\nwhich records normal and abnormal disconnections as a supplement to downgraded logging.\nBugs that Are Recovered TiDB \u0026ldquo;basically\u0026rdquo; can recover from panics caused by unknown bugs. However, if there is an array out-of-bounds, a null pointer reference, or intentional panic, it cannot guarantee correct results for the current and subsequent SQL, so terminating the current connection is a wise choice.\nAt this time, an error log \u0026ldquo;connection running loop panic\u0026rdquo; will appear, along with a lastSQL field that outputs the current erroneous SQL.\nPanic Not Recovered Whether it\u0026rsquo;s an unrecovered panic or a system-level OOM-induced panic, they do not leave a log in TiDB\u0026rsquo;s logs. TiDB clusters managed by deployment tools like Ansible or TiUP will automatically restart a crashed TiDB server. Consequently, the log will contain a new \u0026ldquo;Welcome\u0026rdquo; message, which might be overlooked. However, the Uptime in monitoring will show TiDB\u0026rsquo;s Uptime reset to zero, making this issue relatively easy to detect. Of course, it\u0026rsquo;s better to have accompanying alerts.\nUnrecovered panic outputs are Golang\u0026rsquo;s default outputs, usually redirected to tidb_stderr.log by deployment tools. Older versions of Ansible overwrite this file every restart, but now use an append mode.\nNevertheless, it has some other drawbacks, like lacking timestamps. This makes it difficult to timestamp-match with TiDB logs. This PR implemented distinguishing tidb_stderr.log based on PID but hasn\u0026rsquo;t been coordinated with the deployment tools and is temporarily disabled.\nTo get this standard panic output, you can use the panicparse introduced in the previous article to parse the panic results. Typically, you can look at the topmost stack. The example in the image evidently shows an out-of-memory error, commonly referred to as OOM. To identify which SQL caused the OOM, check TiDB\u0026rsquo;s logs for resource-heavy SQL, which are usually logged with the expensive_query tag, and can be checked by grepping the logs. This will not be exemplified here.\nTracing TiDB has supported tracing since version 2.1, but it hasn\u0026rsquo;t been widely used. I think there are two main reasons:\nThe initial version of tracing only supported the JSON format, requiring the output to be copied and pasted into a TiDB-specific web page at a special host port to view it. Although novel, the multiple steps involved prevented widespread adoption.\nAnother issue is that tracing provides insight only after a problem is known. If developers suspect a problem or slow execution in advance, they must proactively add events at those points. Often, unforeseen issues cannot be covered, leaving gaps.\nOnce the framework of tracing is in place, adding events is relatively straightforward and involves adding code like the snippet below at the desired points:\nInterested individuals can add events to TiDB as needed, offering a good hands-on experience.\nEventually, tracing added format='row' and format='log' features,\nI personally favor format='log'.\nDifference between Tracing and Explain (Analyze) Tracing operates at the function level, while Explain operates at the operator level. Tracing is easier to add and more granular and does not need to be part of a plan. Tracing can trace any SQL, while Explain only shows data reading parts. For example, with an Insert, Explain shows almost nothing, whereas tracing provides detailed insights from SQL parsing to the full transaction commit. ","permalink":"https://blog.minifish.org/posts/tidb5/","summary":"\u003cp\u003eWhen using TiDB, you may occasionally encounter some exceptions, such as the \u0026ldquo;Lost connection to MySQL server during query\u0026rdquo; error. This indicates that the connection between the client and the database has been disconnected (not due to user action). The reasons for disconnection can vary. This article attempts to analyze some common TiDB errors from the perspective of exception handling and code analysis. Additionally, some exceptions are not errors but performance issues due to slow execution. In the second half of this article, we will also introduce common tools for tracking performance.\u003c/p\u003e","title":"How to Read TiDB Source Code (Part 5)"},{"content":"This article will introduce some key functions and the interpretation of logs in TiDB.\nKey Functions The definition of key functions varies from person to person, so the content of this section is subjective.\nexecute The execute function is the necessary pathway for text protocol execution. It also nicely demonstrates the various processes of SQL handling.\nParseSQL analyzes the SQL. The final implementation is in the parser, where SQL is parsed according to the rules introduced in the second article. Note that the parsed SQL may be a single statement or multiple statements. TiDB itself supports the multi-SQL feature, allowing multiple SQL statements to be executed at once.\nAfter parsing, a stmtNodes array is returned, which is processed one-by-one in the for loop below. The first step is to compile, where the core of compile is optimization, generating a plan. By following the Optimize function, you can find logic similar to logical and physical optimization found in other common databases.\nThe last part is execution, where executeStatement and particularly the runStmt function are key functions.\nrunStmt Judging from the call graph of runStmt, this function is almost the mandatory pathway for all SQL execution. Except for point query statements using the binary protocol with automatic commit, all other statements go through this function. This function is responsible for executing SQL, excluding SQL parsing and compilation (the binary protocol does not need repeated SQL parsing, nor does SQL compilation require plan caching).\nThe core part of the runStmt function is as shown above. From top to bottom:\ncheckTxnAborted\nWhen a transaction is already corrupted and cannot be committed, the user must actively close the transaction to end the already corrupted transaction. During execution, transactions may encounter errors that cannot be handled and must be terminated. The transaction cannot be silently closed because the user may continue to execute SQL and assume it is still within the transaction. This function ensures that all subsequent SQL commands by the user are not executed and directly return an error until the user uses rollback or commit to explicitly close the transaction for normal execution.\nExec\nExecute the SQL and return the result set (rs).\nIsReadOnly\nAfter executing a SQL, it\u0026rsquo;s necessary to determine whether it is a read-only SQL. If it is not read-only, it must be temporarily stored in the transaction\u0026rsquo;s execution history. This execution history is used when a transaction conflict or other errors require the transaction to be retried. Read-only SQL is bypassed because the retry of the transaction is done during the commit phase, and at this point, the only feedback to the client can be success or failure of the commit; reading results is meaningless.\nThis section also includes StmtCommit and StmtRollback. TiDB supports MySQL-like statement commits and rollbacks—if a statement fails during a transaction, that single statement will be atomically rolled back, while other successfully executed statements will eventually commit with the transaction.\nIn TiDB, the feature of statement commit is implemented with a two-layer buffer: both the transaction and the statement have their own buffers. After a statement executes successfully, the statement’s buffer is merged into the transaction buffer. If a statement fails, the statement’s buffer is discarded, thus ensuring the atomicity of statement commits. Of course, a statement commit may fail, in which case the entire transaction buffer becomes unusable, and the transaction can only be rolled back.\nfinishStmt\nOnce a statement is executed, should it be committed? This depends on whether the transaction was explicitly started (i.e., with begin or start transaction) and whether autocommit is enabled. The role of finishStmt is to, post-execution, check if it should be committed based on the above conditions. It\u0026rsquo;s essentially for cleaning up and checking after each statement execution.\npending section\nSome SQLs in TiDB do not require a transaction (e.g., the set statement). However, before parsing, the database doesn’t know whether the statement requires a transaction. The latency of starting a transaction in TiDB is relatively high because it requires obtaining a TSO (timestamp oracle) from PD. TiDB has an optimization to asynchronously obtain a TSO, meaning a TSO is prepared regardless of whether a transaction is eventually needed. If a statement indeed doesn’t require a TSO and a transaction is not activated, remaining in a pending status, the pending transaction must be closed.\nLogs Let\u0026rsquo;s first look at a section of logs from TiDB at initial startup, divided into several parts:\n[2020/08/12 16:12:07.282 +08:00] [INFO] [printer.go:42] [\u0026#34;Welcome to TiDB.\u0026#34;] [\u0026#34;Release Version\u0026#34;=None] [Edition=None] [\u0026#34;Git Commit Hash\u0026#34;=None] [\u0026#34;Git Branch\u0026#34;=None] [\u0026#34;UTC Build Time\u0026#34;=None] [GoVersion=go1.15] [\u0026#34;Race Enabled\u0026#34;=false] [\u0026#34;Check Table Before Drop\u0026#34;=false] [\u0026#34;TiKV Min Version\u0026#34;=v3.0.0-60965b006877ca7234adaced7890d7b029ed1306] [2020/08/12 16:12:07.300 +08:00] [INFO] [printer.go:56] [\u0026#34;loaded config\u0026#34;] [config=\u0026#34;{\\\u0026#34;host\\\u0026#34;:\\\u0026#34;0.0.0.0\\\u0026#34;,\\\u0026#34;advertise-address\\\u0026#34;:\\\u0026#34;0.0.0.0\\\u0026#34;,\\\u0026#34;port\\\u0026#34;:4000,\\\u0026#34;cors\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;store\\\u0026#34;:\\\u0026#34;mocktikv\\\u0026#34;,\\\u0026#34;path\\\u0026#34;:\\\u0026#34;/tmp/tidb\\\u0026#34;,\\\u0026#34;socket\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;lease\\\u0026#34;:\\\u0026#34;45s\\\u0026#34;,\\\u0026#34;run-ddl\\\u0026#34;:true,\\\u0026#34;split-table\\\u0026#34;:true,\\\u0026#34;token-limit\\\u0026#34;:1000,\\\u0026#34;oom-use-tmp-storage\\\u0026#34;:true,\\\u0026#34;tmp-storage-path\\\u0026#34;:\\\u0026#34;C:\\\\\\\\Users\\\\\\\\yushu\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\S-1-5-21-4064392927-3477209728-2136073142-1001_tidb\\\\\\\\MC4wLjAuMDo0MDAwLzAuMC4wLjA6MTAwODA=\\\\\\\\tmp-storage\\\u0026#34;,\\\u0026#34;oom-action\\\u0026#34;:\\\u0026#34;log\\\u0026#34;,\\\u0026#34;mem-quota-query\\\u0026#34;:1073741824,\\\u0026#34;tmp-storage-quota\\\u0026#34;:-1,\\\u0026#34;enable-streaming\\\u0026#34;:false,\\\u0026#34;enable-batch-dml\\\u0026#34;:false,\\\u0026#34;lower-case-table-names\\\u0026#34;:2,\\\u0026#34;server-version\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;log\\\u0026#34;:{\\\u0026#34;level\\\u0026#34;:\\\u0026#34;info\\\u0026#34;,\\\u0026#34;format\\\u0026#34;:\\\u0026#34;text\\\u0026#34;,\\\u0026#34;disable-timestamp\\\u0026#34;:null,\\\u0026#34;enable-timestamp\\\u0026#34;:null,\\\u0026#34;disable-error-stack\\\u0026#34;:null,\\\u0026#34;enable-error-stack\\\u0026#34;:null,\\\u0026#34;file\\\u0026#34;:{\\\u0026#34;filename\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;max-size\\\u0026#34;:300,\\\u0026#34;max-days\\\u0026#34;:0,\\\u0026#34;max-backups\\\u0026#34;:0},\\\u0026#34;enable-slow-log\\\u0026#34;:true,\\\u0026#34;slow-query-file\\\u0026#34;:\\\u0026#34;tidb-slow.log\\\u0026#34;,\\\u0026#34;slow-threshold\\\u0026#34;:300,\\\u0026#34;expensive-threshold\\\u0026#34;:10000,\\\u0026#34;query-log-max-len\\\u0026#34;:4096,\\\u0026#34;record-plan-in-slow-log\\\u0026#34;:1},\\\u0026#34;security\\\u0026#34;:{\\\u0026#34;skip-grant-table\\\u0026#34;:false,\\\u0026#34;ssl-ca\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;ssl-cert\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;ssl-key\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;require-secure-transport\\\u0026#34;:false,\\\u0026#34;cluster-ssl-ca\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;cluster-ssl-cert\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;cluster-ssl-key\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;cluster-verify-cn\\\u0026#34;:null},\\\u0026#34;status\\\u0026#34;:{\\\u0026#34;status-host\\\u0026#34;:\\\u0026#34;0.0.0.0\\\u0026#34;,\\\u0026#34;metrics-addr\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;status-port\\\u0026#34;:10080,\\\u0026#34;metrics-interval\\\u0026#34;:15,\\\u0026#34;report-status\\\u0026#34;:true,\\\u0026#34;record-db-qps\\\u0026#34;:false},\\\u0026#34;performance\\\u0026#34;:{\\\u0026#34;max-procs\\\u0026#34;:0,\\\u0026#34;max-memory\\\u0026#34;:0,\\\u0026#34;stats-lease\\\u0026#34;:\\\u0026#34;3s\\\u0026#34;,\\\u0026#34;stmt-count-limit\\\u0026#34;:5000,\\\u0026#34;feedback-probability\\\u0026#34;:0.05,\\\u0026#34;query-feedback-limit\\\u0026#34;:1024,\\\u0026#34;pseudo-estimate-ratio\\\u0026#34;:0.8,\\\u0026#34;force-priority\\\u0026#34;:\\\u0026#34;NO_PRIORITY\\\u0026#34;,\\\u0026#34;bind-info-lease\\\u0026#34;:\\\u0026#34;3s\\\u0026#34;,\\\u0026#34;txn-total-size-limit\\\u0026#34;:104857600,\\\u0026#34;tcp-keep-alive\\\u0026#34;:true,\\\u0026#34;cross-join\\\u0026#34;:true,\\\u0026#34;run-auto-analyze\\\u0026#34;:true,\\\u0026#34;agg-push-down-join\\\u0026#34;:false,\\\u0026#34;committer-concurrency\\\u0026#34;:16,\\\u0026#34;max-txn-ttl\\\u0026#34;:600000},\\\u0026#34;prepared-plan-cache\\\u0026#34;:{\\\u0026#34;enabled\\\u0026#34;:false,\\\u0026#34;capacity\\\u0026#34;:100,\\\u0026#34;memory-guard-ratio\\\u0026#34;:0.1},\\\u0026#34;opentracing\\\u0026#34;:{\\\u0026#34;enable\\\u0026#34;:false,\\\u0026#34;rpc-metrics\\\u0026#34;:false,\\\u0026#34;sampler\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;const\\\u0026#34;,\\\u0026#34;param\\\u0026#34;:1,\\\u0026#34;sampling-server-url\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;max-operations\\\u0026#34;:0,\\\u0026#34;sampling-refresh-interval\\\u0026#34;:0},\\\u0026#34;reporter\\\u0026#34;:{\\\u0026#34;queue-size\\\u0026#34;:0,\\\u0026#34;buffer-flush-interval\\\u0026#34;:0,\\\u0026#34;log-spans\\\u0026#34;:false,\\\u0026#34;local-agent-host-port\\\u0026#34;:\\\u0026#34;\\\u0026#34;}},\\\u0026#34;proxy-protocol\\\u0026#34;:{\\\u0026#34;networks\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;header-timeout\\\u0026#34;:5},\\\u0026#34;tikv-client\\\u0026#34;:{\\\u0026#34;grpc-connection-count\\\u0026#34;:4,\\\u0026#34;grpc-keepalive-time\\\u0026#34;:10,\\\u0026#34;grpc-keepalive-timeout\\\u0026#34;:3,\\\u0026#34;commit-timeout\\\u0026#34;:\\\u0026#34;41s\\\u0026#34;,\\\u0026#34;max-batch-size\\\u0026#34;:128,\\\u0026#34;overload-threshold\\\u0026#34;:200,\\\u0026#34;max-batch-wait-time\\\u0026#34;:0,\\\u0026#34;batch-wait-size\\\u0026#34;:8,\\\u0026#34;enable-chunk-rpc\\\u0026#34;:true,\\\u0026#34;region-cache-ttl\\\u0026#34;:600,\\\u0026#34;store-limit\\\u0026#34;:0,\\\u0026#34;store-liveness-timeout\\\u0026#34;:\\\u0026#34;120s\\\u0026#34;,\\\u0026#34;copr-cache\\\u0026#34;:{\\\u0026#34;enable\\\u0026#34;:false,\\\u0026#34;capacity-mb\\\u0026#34;:1000,\\\u0026#34;admission-max-result-mb\\\u0026#34;:10,\\\u0026#34;admission-min-process-ms\\\u0026#34;:5}},\\\u0026#34;binlog\\\u0026#34;:{\\\u0026#34;enable\\\u0026#34;:false,\\\u0026#34;ignore-error\\\u0026#34;:false,\\\u0026#34;write-timeout\\\u0026#34;:\\\u0026#34;15s\\\u0026#34;,\\\u0026#34;binlog-socket\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;strategy\\\u0026#34;:\\\u0026#34;range\\\u0026#34;},\\\u0026#34;compatible-kill-query\\\u0026#34;:false,\\\u0026#34;plugin\\\u0026#34;:{\\\u0026#34;dir\\\u0026#34;:\\\u0026#34;\\\u0026#34;,\\\u0026#34;load\\\u0026#34;:\\\u0026#34;\\\u0026#34;},\\\u0026#34;pessimistic-txn\\\u0026#34;:{\\\u0026#34;enable\\\u0026#34;:true,\\\u0026#34;max-retry-count\\\u0026#34;:256},\\\u0026#34;check-mb4-value-in-utf8\\\u0026#34;:true,\\\u0026#34;max-index-length\\\u0026#34;:3072,\\\u0026#34;alter-primary-key\\\u0026#34;:false,\\\u0026#34;treat-old-version-utf8-as-utf8mb4\\\u0026#34;:true,\\\u0026#34;enable-table-lock\\\u0026#34;:false,\\\u0026#34;delay-clean-table-lock\\\u0026#34;:0,\\\u0026#34;split-region-max-num\\\u0026#34;:1000,\\\u0026#34;stmt-summary\\\u0026#34;:{\\\u0026#34;enable\\\u0026#34;:true,\\\u0026#34;enable-internal-query\\\u0026#34;:false,\\\u0026#34;max-stmt-count\\\u0026#34;:200,\\\u0026#34;max-sql-length\\\u0026#34;:4096,\\\u0026#34;refresh-interval\\\u0026#34;:1800,\\\u0026#34;history-size\\\u0026#34;:24},\\\u0026#34;repair-mode\\\u0026#34;:false,\\\u0026#34;repair-table-list\\\u0026#34;:[],\\\u0026#34;isolation-read\\\u0026#34;:{\\\u0026#34;engines\\\u0026#34;:[\\\u0026#34;tikv\\\u0026#34;,\\\u0026#34;tiflash\\\u0026#34;,\\\u0026#34;tidb\\\u0026#34;]},\\\u0026#34;max-server-connections\\\u0026#34;:0,\\\u0026#34;new_collations_enabled_on_first_bootstrap\\\u0026#34;:false,\\\u0026#34;experimental\\\u0026#34;:{\\\u0026#34;allow-auto-random\\\u0026#34;:false,\\\u0026#34;allow-expression-index\\\u0026#34;:false}}\u0026#34;] Mandatory startup outputs: \u0026ldquo;Welcome to TiDB,\u0026rdquo; git hash, Golang version, etc. Actually loaded configuration (this section is somewhat difficult to read) The remainder are some routine startup logs. The process can be referenced from the main function section introduced in the first article, mainly outputting the initial system table creation process.\n[2020/08/12 16:12:07.300 +08:00] [INFO] [main.go:341] [\u0026#34;disable Prometheus push client\u0026#34;] [2020/08/12 16:12:07.300 +08:00] [INFO] [store.go:68] [\u0026#34;new store\u0026#34;] [path=mocktikv:///tmp/tidb] [2020/08/12 16:12:07.300 +08:00] [INFO] [systime_mon.go:25] [\u0026#34;start system time monitor\u0026#34;] [2020/08/12 16:12:07.310 +08:00] [INFO] [store.go:74] [\u0026#34;new store with retry success\u0026#34;] [2020/08/12 16:12:07.310 +08:00] [INFO] [tidb.go:71] [\u0026#34;new domain\u0026#34;] [store=8d19232e-a273-4e31-ba9b-a3467998345c] [\u0026#34;ddl lease\u0026#34;=45s] [\u0026#34;stats lease\u0026#34;=3s] [2020/08/12 16:12:07.315 +08:00] [INFO] [ddl.go:321] [\u0026#34;[ddl] start DDL\u0026#34;] [ID=0e1bd28e-03ed-4900-bf71-f58b3b9d954a] [runWorker=true] [2020/08/12 16:12:07.315 +08:00] [INFO] [ddl.go:309] [\u0026#34;[ddl] start delRangeManager OK\u0026#34;] [\u0026#34;is a emulator\u0026#34;=true] [2020/08/12 16:12:07.315 +08:00] [INFO] [ddl_worker.go:130] [\u0026#34;[ddl] start DDL worker\u0026#34;] [worker=\u0026#34;worker 1, tp general\u0026#34;] [2020/08/12 16:12:07.315 +08:00] [INFO] [ddl_worker.go:130] [\u0026#34;[ddl] start DDL worker\u0026#34;] [worker=\u0026#34;worker 2, tp add index\u0026#34;] [2020/08/12 16:12:07.315 +08:00] [INFO] [delete_range.go:133] [\u0026#34;[ddl] start delRange emulator\u0026#34;] [2020/08/12 16:12:07.317 +08:00] [INFO] [domain.go:144] [\u0026#34;full load InfoSchema success\u0026#34;] [usedSchemaVersion=0] [neededSchemaVersion=0] [\u0026#34;start time\u0026#34;=2.0015ms] [2020/08/12 16:12:07.317 +08:00] [INFO] [domain.go:368] [\u0026#34;full load and reset schema validator\u0026#34;] [2020/08/12 16:12:07.317 +08:00] [INFO] [tidb.go:199] [\u0026#34;rollbackTxn for ddl/autocommit failed\u0026#34;] ```Here is the translation of the provided text: --- [2020/08/12 16:12:07.317 +08:00] [WARN] [session.go:1040] [\u0026#34;run statement failed\u0026#34;] [schemaVersion=0] [error=\u0026#34;[schema:1049]Unknown database \u0026#39;mysql\u0026#39;\u0026#34;] [session=\u0026#34;{\\n \\\u0026#34;currDBName\\\u0026#34;: \\\u0026#34;\\\u0026#34;,\\n \\\u0026#34;id\\\u0026#34;: 0,\\n \\\u0026#34;status\\\u0026#34;: 2,\\n \\\u0026#34;strictMode\\\u0026#34;: true,\\n \\\u0026#34;user\\\u0026#34;: null\\n}\u0026#34;] [2020/08/12 16:12:07.318 +08:00] [WARN] [session.go:1136] [\u0026#34;compile SQL failed\u0026#34;] [error=\u0026#34;[schema:1146]Table \u0026#39;mysql.tidb\u0026#39; doesn\u0026#39;t exist\u0026#34;] [SQL=\u0026#34;SELECT HIGH_PRIORITY VARIABLE_VALUE FROM mysql.tidb WHERE VARIABLE_NAME=\\\u0026#34;bootstrapped\\\u0026#34;\u0026#34;] [2020/08/12 16:12:07.318 +08:00] [INFO] [session.go:2121] [\u0026#34;CRUCIAL OPERATION\u0026#34;] [conn=0] [schemaVersion=0] [cur_db=] [sql=\u0026#34;CREATE DATABASE IF NOT EXISTS test\u0026#34;] [user=] [2020/08/12 16:12:07.320 +08:00] [INFO] [ddl_worker.go:253] [\u0026#34;[ddl] add DDL jobs\u0026#34;] [\u0026#34;batch count\u0026#34;=1] [jobs=\u0026#34;ID:2, Type:create schema, State:none, SchemaState:none, SchemaID:1, TableID:0, RowCount:0, ArgLen:1, start time: 2020-08-12 16:12:07.318 +0800 CST, Err:\u0026lt;nil\u0026gt;, ErrCount:0, SnapshotVersion:0; \u0026#34;] [2020/08/12 16:12:07.320 +08:00] [INFO] [ddl.go:500] [\u0026#34;[ddl] start DDL job\u0026#34;] [job=\u0026#34;ID:2, Type:create schema, State:none, SchemaState:none, SchemaID:1, TableID:0, RowCount:0, ArgLen:1, start time: 2020-08-12 16:12:07.318 +0800 CST, Err:\u0026lt;nil\u0026gt;, ErrCount:0, SnapshotVersion:0\u0026#34;] [query=\u0026#34;CREATE DATABASE IF NOT EXISTS test\u0026#34;] [2020/08/12 16:12:07.320 +08:00] [INFO] [ddl_worker.go:568] [\u0026#34;[ddl] run DDL job\u0026#34;] [worker=\u0026#34;worker 1, tp general\u0026#34;] [job=\u0026#34;ID:2, Type:create schema, State:none, SchemaState:none, SchemaID:1, TableID:0, RowCount:0, ArgLen:0, start time: 2020-08-12 16:12:07.318 +0800 CST, Err:\u0026lt;nil\u0026gt;, ErrCount:0, SnapshotVersion:0\u0026#34;] [2020/08/12 16:12:07.322 +08:00] [INFO] [domain.go:144] [\u0026#34;full load InfoSchema success\u0026#34;] [usedSchemaVersion=0] [neededSchemaVersion=1] [\u0026#34;start time\u0026#34;=1.0003ms] [2020/08/12 16:12:07.322 +08:00] [INFO] [domain.go:368] [\u0026#34;full load and reset schema validator\u0026#34;] [2020/08/12 16:12:07.324 +08:00] [INFO] [ddl_worker.go:757] [\u0026#34;[ddl] wait latest schema version changed\u0026#34;] [worker=\u0026#34;worker 1, tp general\u0026#34;] [ver=1] [\u0026#34;take time\u0026#34;=3.0094ms] [job=\u0026#34;ID:2, Type:create schema, State:done, SchemaState:public, SchemaID:1, TableID:0, RowCount:0, ArgLen:1, start time: 2020-08-12 16:12:07.318 +0800 CST, Err:\u0026lt;nil\u0026gt;, ErrCount:0, SnapshotVersion:0\u0026#34;] [2020/08/12 16:12:07.324 +08:00] [INFO] [ddl_worker.go:359] [\u0026#34;[ddl] finish DDL job\u0026#34;] [worker=\u0026#34;worker 1, tp general\u0026#34;] [job=\u0026#34;ID:2, Type:create schema, State:synced, SchemaState:public, SchemaID:1, TableID:0, RowCount:0, ArgLen:0, start time: 2020-08-12 16:12:07.318 +0800 CST, Err:\u0026lt;nil\u0026gt;, ErrCount:0, SnapshotVersion:0\u0026#34;] [2020/08/12 16:12:07.325 +08:00] [INFO] [ddl.go:532] [\u0026#34;[ddl] DDL job is finished\u0026#34;] [jobID=2] [2020/08/12 16:12:07.325 +08:00] [INFO] [domain.go:619] [\u0026#34;performing DDL change, must reload\u0026#34;] [2020/08/12 16:12:07.325 +08:00] [INFO] [session.go:2121] [\u0026#34;CRUCIAL OPERATION\u0026#34;] [conn=0] [schemaVersion=1] [cur_db=] [sql=\u0026#34;CREATE DATABASE IF NOT EXISTS mysql;\u0026#34;] [user=] [2020/08/12 16:12:07.325 +08:00] [INFO] [ddl_worker.go:253] [\u0026#34;[ddl] add DDL jobs\u0026#34;] [\u0026#34;batch count\u0026#34;=1] [jobs=\u0026#34;ID:4, Type:create schema, State:none, SchemaState:none, SchemaID:3, TableID:0, RowCount:0, ArgLen:1, start time: 2020-08-12 16:12:07.325 +0800 CST, Err:\u0026lt;nil\u0026gt;, ErrCount:0, SnapshotVersion:0; \u0026#34;] [2020/08/12 16:12:07.325 +08:00] [INFO] [ddl.go:500] [\u0026#34;[ddl] start DDL job\u0026#34;] [job=\u0026#34;ID:4, Type:create schema, State:none, SchemaState:none, SchemaID:3, TableID:0, RowCount:0, ArgLen:1, start time: 2020-08-12 16:12:07.325 +0800 CST, Err:\u0026lt;nil\u0026gt;, ErrCount:0, SnapshotVersion:0\u0026#34;] [query=\u0026#34;CREATE DATABASE IF NOT EXISTS mysql;\u0026#34;] [2020/08/12 16:12:07.326 +08:00] [INFO] [ddl_worker.go:568] [\u0026#34;[ddl] run DDL job\u0026#34;] [worker=\u0026#34;worker 1, tp general\u0026#34;] [job=\u0026#34;ID:4, Type:create schema, State:none, SchemaState:none, SchemaID:3, TableID:0, RowCount:0, ArgLen:0, start time: 2020-08-12 16:12:07.325 +0800 CST, Err:\u0026lt;nil\u0026gt;, ErrCount:0, SnapshotVersion:0\u0026#34;] [2020/08/12 16:12:07.326 +08:00] [INFO] [domain.go:126] [\u0026#34;diff load InfoSchema success\u0026#34;] [usedSchemaVersion=1] [neededSchemaVersion=2] [\u0026#34;start time\u0026#34;=0s] [tblIDs=\u0026#34;[]\u0026#34;] [2020/08/12 16:12:07.329 +08:00] [INFO] [ddl_worker.go:757] [\u0026#34;[ddl] wait latest schema version changed\u0026#34;] [worker=\u0026#34;worker 1, tp general\u0026#34;] [ver=2] [\u0026#34;take time\u0026#34;=2.9965ms] [job=\u0026#34;ID:4, Type:create schema, State:done, SchemaState:public, SchemaID:3, TableID:0, RowCount:0, ArgLen:1, start time: 2020-08-12 16:12:07.325 +0800 CST, Err:\u0026lt;nil\u0026gt;, ErrCount:0, SnapshotVersion:0\u0026#34;] [2020/08/12 16:12:07.329 +08:00] [INFO] [ddl_worker.go:359] [\u0026#34;[ddl] finish DDL job\u0026#34;] [worker=\u0026#34;worker 1, tp general\u0026#34;] [job=\u0026#34;ID:4, Type:create schema, State:synced, SchemaState:public, SchemaID:3, TableID:0, RowCount:0, ArgLen:0, start time: 2020-08-12 16:12:07.325 Because DDL logs are very numerous, the TiDB logs basically record each step of the DDL execution, so I\u0026#39;ve truncated this part of the log here. However, the basic outline can be sorted out. Firstly, the DDL execution is initiated from ddl_api, at this time recording `[\u0026#34;CRUCIAL OPERATION\u0026#34;]` style logs. DDL is a crucial operation, so it belongs to CRUCIAL type logs. Then, we can see a series of logs with the ddl keyword linked together, such as `[ddl] add DDL jobs`, `[ddl] start DDL job`, `[ddl] run DDL job`, `[ddl] finish DDL job`, and `[ddl] DDL job is finished`. These represent the process from when the DDL owner acquires a job to its final execution completion. Moreover, they have a unique job ID, which can be used to link a DDL in the log with something like `jobs=\u0026#34;ID:2`. --- [2020/08/12 16:12:07.518 +08:00] [INFO] [server.go:235] [\u0026#34;server is running MySQL protocol\u0026#34;] [addr=0.0.0.0:4000] [2020/08/12 16:12:07.518 +08:00] [INFO] [http_status.go:80] [\u0026#34;for status and metrics report\u0026#34;] [\u0026#34;listening on addr\u0026#34;=0.0.0.0:10080] [2020/08/12 16:12:07.520 +08:00] [INFO] [domain.go:1015] [\u0026#34;init stats info time\u0026#34;] [\u0026#34;take time\u0026#34;=3.0126ms] [2020/08/12 16:15:41.482 +08:00] [INFO] [server.go:388] [\u0026#34;new connection\u0026#34;] [conn=1] [remoteAddr=127.0.0.1:64888] [2020/08/12 21:03:19.954 +08:00] [INFO] [server.go:391] [\u0026#34;connection closed\u0026#34;] [conn=1] Thereafter, the appearance of `server is running MySQL protocol` means that TiDB can provide services externally. Later, there are logs corresponding to the creation and closing of each connection, namely `new connection` and `connection closed`. Of course, they also have their corresponding connection ID, which is unique for a TiDB. You can use the keyword `conn=1` in the log to contextually link them together. ### Stack Logs Most of TiDB\u0026#39;s SQL errors (except for duplicate entry and syntax errors) will output the complete stack information. Due to the requirements of unified log format, the stack now looks very unsightly... ```text For this stack trace, I believe no one really enjoys reading it. Therefore, we need to paste it into Vim and execute `%s/\\\\n/\\r/g` and `%s/\\\\t/ /g` to turn it into a Golang-style stack. ![func](/posts/images/20200812211203.png) When you see which module it\u0026#39;s stuck in, like the plan part here, you can find the corresponding colleague for support. However, there is a more user-friendly tool for dealing with Golang’s lengthy stack called [panicparse](https://github.com/maruel/panicparse). To install it, simply run `go get github.com/maruel/panicparse/v2/cmd/pp`. The effect is as follows: ![func](/posts/images/20200813172149.png) Whether it\u0026#39;s TiDB running goroutines or panic outputs, it can be parsed using this. It has several features: 1. It can display active and inactive goroutines. 2. It can show the relationships between goroutines. 3. Keyword highlighting. 4. Supports Windows. The latest 2.0.0 version supports race detector and HTML formatted output. This concludes the introduction to the analysis of key functions and logs (startup, DDL, connection, error stack). ","permalink":"https://blog.minifish.org/posts/tidb4/","summary":"\u003cp\u003eThis article will introduce some key functions and the interpretation of logs in TiDB.\u003c/p\u003e\n\u003ch2 id=\"key-functions\"\u003eKey Functions\u003c/h2\u003e\n\u003cp\u003eThe definition of key functions varies from person to person, so the content of this section is subjective.\u003c/p\u003e\n\u003ch3 id=\"execute\"\u003eexecute\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"func\" loading=\"lazy\" src=\"/posts/images/20200812152326.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eexecute\u003c/code\u003e function is the necessary pathway for text protocol execution. It also nicely demonstrates the various processes of SQL handling.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eParseSQL analyzes the SQL. The final implementation is in the parser, where SQL is parsed according to the rules introduced in the second article. Note that the parsed SQL may be a single statement or multiple statements. TiDB itself supports the multi-SQL feature, allowing multiple SQL statements to be executed at once.\u003c/p\u003e","title":"How to Read TiDB Source Code (Part 4)"},{"content":"In the previous article, we introduced methods for viewing syntax and configurations. In this article, we will discuss how to view system variables, including default values, scopes, and how to monitor metrics.\nSystem Variables The system variable names in TiDB are defined in tidb_vars.go. This file also includes some default values for variables, but the place where they are actually assembled is defaultSysVars.\nThis large struct array defines the scope, variable names, and default values for all variables in TiDB. Besides TiDB\u0026rsquo;s own system variables, it also includes compatibility with MySQL\u0026rsquo;s system variables.\nScope In TiDB, there are three types of variable scopes literally:\nThey are ScopeNone, ScopeGlobal, and ScopeSession. They represent:\nScopeNone: Read-only variables ScopeGlobal: Global variables ScopeSession: Session variables The actual effect of these scopes is that when you use SQL to read or write them, you need to use the corresponding syntax. If the SQL fails, the SQL operation does not take effect. If the SQL executes successfully, it merely means the setting is complete, but it does not mean that it takes effect according to the corresponding scope.\nLet\u0026rsquo;s use the method mentioned in the first article to start a single-node TiDB for demonstration:\nScopeNone Take performance_schema_max_mutex_classes as an example,\nMySQL 127.0.0.1:4000 SQL \u0026gt; select @@performance_schema_max_mutex_classes; +----------------------------------------+ | @@performance_schema_max_mutex_classes | +----------------------------------------+ | 200 | +----------------------------------------+ 1 row in set (0.0002 sec) MySQL 127.0.0.1:4000 SQL \u0026gt; select @@global.performance_schema_max_mutex_classes; +-----------------------------------------------+ | @@global.performance_schema_max_mutex_classes | +-----------------------------------------------+ | 200 | +-----------------------------------------------+ 1 row in set (0.0004 sec) MySQL 127.0.0.1:4000 SQL \u0026gt; select @@session.performance_schema_max_mutex_classes; ERROR: 1238 (HY000): Variable \u0026#39;performance_schema_max_mutex_classes\u0026#39; is a GLOBAL variable As you can see, the scope of ScopeNone can be read as a global variable,\nMySQL 127.0.0.1:4000 SQL \u0026gt; set global performance_schema_max_mutex_classes = 1; ERROR: 1105 (HY000): Variable \u0026#39;performance_schema_max_mutex_classes\u0026#39; is a read-only variable MySQL 127.0.0.1:4000 SQL \u0026gt; set performance_schema_max_mutex_classes = 1; ERROR: 1105 (HY000): Variable \u0026#39;performance_schema_max_mutex_classes\u0026#39; is a read-only variable MySQL 127.0.0.1:4000 SQL \u0026gt; set session performance_schema_max_mutex_classes = 1; ERROR: 1105 (HY000): Variable \u0026#39;performance_schema_max_mutex_classes\u0026#39; is a read-only variable But it cannot be set in any way.\nTo trace the usage of ScopeNone, you will see\nIn setSysVariable, when this type of scope variable is encountered, an error is directly returned.\nIn ValidateGetSystemVar, it is handled as a global variable. From a theoretical standpoint, these ScopeNone variables are essentially a single copy in the code. Once TiDB is started, they exist in memory as read-only and are not actually stored in TiKV.\nScopeGlobal Using gtid_mode as an example,\nMySQL 127.0.0.1:4000 SQL \u0026gt; select @@gtid_mode; +-------------+ | @@gtid_mode | +-------------+ | OFF | +-------------+ 1 row in set (0.0003 sec) MySQL 127.0.0.1:4000 SQL \u0026gt; select @@global.gtid_mode; +--------------------+ | @@global.gtid_mode | +--------------------+ | OFF | +--------------------+ 1 row in set (0.0006 sec) MySQL 127.0.0.1:4000 SQL \u0026gt; select @@session.gtid_mode; ERROR: 1238 (HY000): Variable \u0026#39;gtid_mode\u0026#39; is a GLOBAL variable It works the same way as MySQL global variable reading,\nMySQL 127.0.0.1:4000 SQL \u0026gt; set gtid_mode=on; ERROR: 1105 (HY000): Variable \u0026#39;gtid_mode\u0026#39; is a GLOBAL variable and should be set with SET GLOBAL MySQL 127.0.0.1:4000 SQL \u0026gt; set session gtid_mode=on; ERROR: 1105 (HY000): Variable \u0026#39;gtid_mode\u0026#39; is a GLOBAL variable and should be set with SET GLOBAL MySQL 127.0.0.1:4000 SQL \u0026gt; set global gtid_mode=on; Query OK, 0 rows affected (0.0029 sec) MySQL 127.0.0.1:4000 SQL \u0026gt; select @@global.gtid_mode; +--------------------+ | @@global.gtid_mode | +--------------------+ | ON | +--------------------+ 1 row in set (0.0005 sec) MySQL 127.0.0.1:4000 SQL \u0026gt; select @@gtid_mode; +-------------+ | @@gtid_mode | +-------------+ | ON | +-------------+ 1 row in set (0.0006 sec) The setting method is also compatible with MySQL. At this point, we can shut down the single-instance TiDB and restart it,\nMySQL 127.0.0.1:4000 SQL \u0026gt; select @@gtid_mode; +-------------+ | @@gtid_mode | +-------------+ | ON | +-------------+ 1 row in set (0.0003 sec) And you can see that the result can still be read, meaning that this setting was persisted to the storage engine. Looking closely at the code, you can see:\nThe actual implementation involves executing an internal replace statement to update the original value. This constitutes a complete transaction involving acquiring two TSOs and committing the entire process, making it slower compared to setting session variables.\nScopeSession Using rand_seed2 as an example,\nMySQL 127.0.0.1:4000 SQL \u0026gt; select @@rand_seed2; +--------------+ | @@rand_seed2 | +--------------+ | | +--------------+ 1 row in set (0.0005 sec) MySQL 127.0.0.1:4000 SQL \u0026gt; select @@session.rand_seed2; +----------------------+ | @@session.rand_seed2 | +----------------------+ | | +----------------------+ 1 row in set (0.0003 sec) MySQL 127.0.0.1:4000 SQL \u0026gt; select @@global.rand_seed2; ERROR: 1238 (HY000): Variable \u0026#39;rand_seed2\u0026#39; is a SESSION variable Reading is compatible with MySQL.\nMySQL 127.0.0.1:4000 SQL \u0026gt; set rand_seed2=\u0026#39;abc\u0026#39;; Query OK, 0 rows affected (0.0006 sec) MySQL 127.0.0.1:4000 SQL \u0026gt; set session rand_seed2=\u0026#39;bcd\u0026#39;; Query OK, 0 rows affected (0.0004 sec) MySQL 127.0.0.1:4000 SQL \u0026gt; set global rand_seed2=\u0026#39;cde\u0026#39;; ERROR: 1105 (HY000): Variable \u0026#39;rand_seed2\u0026#39; is a SESSION variable and can\u0026#39;t be used with SET GLOBAL MySQL 127.0.0.1:4000 SQL \u0026gt; select @@rand_seed2; +--------------+ | @@rand_seed2 | +--------------+ | bcd | +--------------+ The setting is also compatible with MySQL. It can be simply observed that this operation only changes the session\u0026rsquo;s memory. The actual place where it finally takes effect is SetSystemVar.\nThere are some tricks here.\nActual Scope of Variables The previous section covered setting session variables. Based on MySQL\u0026rsquo;s variable rules, setting a global variable does not affect the current session. Only newly created sessions will load global variables for session variable assignment. Ultimately, the active session variable take effect. Global variables without session properties still have unique characteristics, and this chapter will cover:\nActivation of session variables Activation of pure global variables Mechanism of global variable function These three aspects.\nActivation of Session Variables Whether a session variable is also a global variable only affects whether it needs to load global variable data from the storage engine when the session starts. The default value in the code is the initial value for eternity if no loading is required.\nThe actual range where a variable operates can only be observed in SetSystemVar.\nFor example, in this part, s.MemQuotaNestedLoopApply = tidbOptInt64(val, DefTiDBMemQuotaNestedLoopApply) changes the s structure, effectively changing the current session,\nWhereas atomic.StoreUint32(\u0026amp;ProcessGeneralLog, uint32(tidbOptPositiveInt32(val, DefTiDBGeneralLog))) changes the value of the global variable ProcessGeneralLog, thereby affecting the entire TiDB instance when set tidb_general_log = 1 is executed.\nActivation of Pure Global Variables Pure global variables in current TiDB are used for background threads like DDL, statistics, etc.\nBecause only one TiDB server requires them, session-level variables hold no meaning for these.\nMechanism of Global Variable Function Global variables in TiDB don\u0026rsquo;t activate immediately after setting. A connection fetches the latest global system variables from TiKV to assign them to the current session the first time it\u0026rsquo;s established. Concurrent connection creation results in frequent access to the TiKV node holding a few global variables. Thus, TiDB caches global variables, updating them every two seconds, significantly reducing TiKV load. The problem arises that after setting a global variable, a brief wait is necessary before creating a new connection, ensuring new connections will read the latest global variable. This is one of the few eventual consistency locations within TiDB.\nFor specific details, see this commentary in loadCommonGlobalVariablesIfNeeded.\nMetrics Compared to system variables, Metrics in TiDB are simpler, or straightforward. The most common Metrics are Histogram and Counter, the former is used to record actual values for an operation and the latter records occurrences of fixed events. All Metrics in TiDB are uniformly located here, with AlertManager and Grafana scripts also available separately under alertmanager and grafana.\nThere are many Metrics, and from a beginner\u0026rsquo;s perspective, it\u0026rsquo;s best to focus on a specific monitoring example. Let\u0026rsquo;s take the TPS (transactions per second) panel as an example.\nClick EDIT and you will see the monitoring formula is:\n[The remaining text seems cut off]Translate the following text to English:\nThe tidb_session_transaction_duration_seconds is the name of this specific metric. Since it is a histogram, it can actually be expressed as three types of values: sum, count, and bucket, which represent the total sum of values, the count (which functions the same as a counter), and the distribution by bucket, respectively.\nIn this context, [1m] represents a time window of 1 minute, indicating the precision of the measurement. The rate function calculates the slope, essentially the rate of change, indicating how many times something occurs per second. The sum function is used for aggregation, and when combined with by (type, txn_mode), it represents aggregation by the dimensions of type and txn_mode.\nThe Legend below displays the dimensions above using {{type}}-{{txn_mode}}. When surrounded by {{}}, it can display the actual label names.\nIn this representation, the final states of transactions are commit, abort, and rollback. A commit indicates a successful user-initiated transaction, rollback indicates a user-initiated rollback (which cannot fail), and abort indicates a user-initiated commit that failed.\nThe second label, txn_mode, refers to two modes: optimistic and pessimistic transactions. There\u0026rsquo;s nothing further to explain about these modes.\nCorresponding to the code:\nThis segment of code shows that tidb_session_transaction_duration_seconds is divided into several parts, including namespace and subsystem. Generally, to find a variable in a formula like tidb_session_transaction_duration_seconds_count within TiDB code, you need to remove the first two words and the last word.\nFrom this code snippet, you can see it\u0026rsquo;s a histogram, specifically a HistogramVec, which is an array of histograms because it records data with several different labels. The labels LblTxnMode and LblType are these two labels.\nChecking the references, there is a place for registration, which is in the main function we discussed in the first article, where metrics are registered.\nOther references show how metrics are instantiated. Why do we do this? Mainly because as the number of labels increases, the performance of metrics becomes poorer, which is related to Prometheus\u0026rsquo;s implementation. We had no choice but to create many instantiated global variables.\nTaking the implementation of Rollback as an example, its essence is to record the actual execution time of a transaction when Rollback is truly executed. Since it’s a histogram, it is also used as a counter in this instance.\n","permalink":"https://blog.minifish.org/posts/tidb3/","summary":"\u003cp\u003eIn the previous article, we introduced methods for viewing syntax and configurations. In this article, we will discuss how to view system variables, including default values, scopes, and how to monitor metrics.\u003c/p\u003e\n\u003ch2 id=\"system-variables\"\u003eSystem Variables\u003c/h2\u003e\n\u003cp\u003eThe system variable names in TiDB are defined in \u003ca href=\"https://github.com/pingcap/tidb/blob/db0310b17901b1a59f7f728294455ed9667f88ac/sessionctx/variable/tidb_vars.go\"\u003etidb_vars.go\u003c/a\u003e. This file also includes some default values for variables, but the place where they are actually assembled is \u003ca href=\"https://github.com/pingcap/tidb/blob/12aac547a9068c404ad18093ae4d0ea4d060a465/sessionctx/variable/sysvar.go#L96\"\u003edefaultSysVars\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"defaultSysVars\" loading=\"lazy\" src=\"/posts/images/20200728151254.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThis large struct array defines the scope, variable names, and default values for all variables in TiDB. Besides TiDB\u0026rsquo;s own system variables, it also includes compatibility with MySQL\u0026rsquo;s system variables.\u003c/p\u003e","title":"How to Read TiDB Source Code (Part 3)"},{"content":"Continuing from the previous article, we learned how to set up the environment for reading code and where to start reading the code. In this part, we\u0026rsquo;ll introduce methods for viewing code based on some common needs.\nHow to Check the Support Level of a Syntax There are usually two methods:\nCheck through the parser repo Directly check within the TiDB repo Both of these methods require the environment setup from the previous article. If you haven\u0026rsquo;t tried that yet, give it a go.\nPreparation Install GoYacc Support\nThe GoYacc Support plugin is a creation by a colleague at our company, a third-party plugin officially accepted by JetBrains, a well-regarded product. It includes syntax highlighting and intelligence, which is great!\nDownload parser repo\nIf you\u0026rsquo;re checking syntax directly from the parser, you need to download it manually. If you\u0026rsquo;re navigating from TiDB, IDEA will automatically download the code, so no extra steps are needed.\nCheck via parser repo Open the parser using IDEA, switch to the branch you need, and locate the parser.y file. However, it is more recommended to check from within TiDB.\nCheck via TiDB repo Open the TiDB project with IDEA and switch to the required branch\nFind the parser.y file; make sure to select the broadest search scope\nAlternatively, you can find it in the file list,\nLet\u0026rsquo;s take checking the SHOW ENGINES SQL statement as an example.\nThe entry point for the entire statement parsing is Start. Below it is the StatementList, followed by Statement. Under the large list of Statements, you can find ShowStmt.\nHowever, ShowStmt is actually quite complex. Another way is to directly search for ShowEngines within parser.y, since naming follows Golang conventions, with camel case and capitalized letters for public exposure. Naturally, if familiar with the code, you\u0026rsquo;d know ShowEngines is under ShowTargetFilterable. Its first branch is ShowEngines.\nWhat is the level of support for SHOW ENGINES?\nYou can look at how ast.ShowEngines is processed. Here, you can\u0026rsquo;t just jump to it; you need to copy and search.\nYou only need to see how it\u0026rsquo;s processed under TiDB, and you can skip test files.\nOne is the actual implementation,\nThe other is the build schema, which you can ignore for now,\nEntering fetchShowEngines, you can see the specific implementation is simple, running an internal SQL to read a system table.\nChecking SHOW ENGINES ends here. You can see that it\u0026rsquo;s fully supported.\nWhich statements only have syntax support?\nTaking the temporary table creation syntax as an example, find its position in the parser.y file.\nIt\u0026rsquo;s an option.\nYou can see that if the temporary table option is specified, it simply returns true with an attached warning, stating that the table is still treated as a regular table. Previously, the parser had a lot of operations that only returned without doing anything, not even a warning, but these are now rare.\nAdvantages of Querying via TiDB repo You can see that checking via the TiDB repo allows you to find the parser\u0026rsquo;s detailed hash using IDEA. If you check directly via the parser, you need to first look up the parser’s hash in TiDB’s go.mod, then check out to the corresponding hash in the parser. If you need to check specific implementations, you have to go back to TiDB, making back-and-forth checks less convenient compared to looking within a single project. The only advantage is the ease of blaming commit history.\nViewing and Modifying Default Configuration The default configurations can be easily viewed in TiDB, specifically the variable defaultConf. The configurations listed here are the actual default settings.\nTaking the first Host configuration as an example, it has a mapping to toml and json files.\nThis essentially shows how it\u0026rsquo;s written in a toml file. The DefHost following it is the specific default value.\nSomething important to note is that configurations have a hierarchical relationship. For example, the log-related configuration in the configuration file is:\nIn the code, it is represented as:\nThis denotes a configuration called \u0026ldquo;level\u0026rdquo; under the log configuration.\nWhat if you want to add more levels? For instance, the most complex configuration for CopCache adds another layer under tikv-client called copr-cache.\nSince toml files do not support multi-level nesting, this leads to the most complex configuration writing in TiDB.\nTo use non-default configurations with the TiDB started through IDEA as mentioned above, the simplest way is to modify this defaultConf.\nSummary From this, you can see that checking whether a statement is supported, and whether it’s just syntax support or has a specific implementation, can be achieved with the described methods. You also learned how to view and modify default configurations, allowing you to conduct some verifications yourself. In the next article, I plan to introduce TiDB’s system variables.\n","permalink":"https://blog.minifish.org/posts/tidb2/","summary":"\u003cp\u003eContinuing from \u003ca href=\"/posts/tidb1\"\u003ethe previous article\u003c/a\u003e, we learned how to set up the environment for reading code and where to start reading the code. In this part, we\u0026rsquo;ll introduce methods for viewing code based on some common needs.\u003c/p\u003e\n\u003ch2 id=\"how-to-check-the-support-level-of-a-syntax\"\u003eHow to Check the Support Level of a Syntax\u003c/h2\u003e\n\u003cp\u003eThere are usually two methods:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCheck through the parser repo\u003c/li\u003e\n\u003cli\u003eDirectly check within the TiDB repo\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBoth of these methods require the \u003ca href=\"/posts/tidb1#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA\"\u003eenvironment setup from the previous article\u003c/a\u003e. If you haven\u0026rsquo;t tried that yet, give it a go.\u003c/p\u003e","title":"How to Read TiDB Source Code (Part 2)"},{"content":"Background There are many articles on reading the source code of TiDB, often referred to as the \u0026ldquo;Twenty-Four Chapters Scriptures\u0026rdquo;. However, these introductions typically proceed from a macro to a micro perspective. This series attempts to introduce how to read TiDB\u0026rsquo;s source code from an easier angle. The goals we aim to achieve are:\nEnable readers to start reading TiDB\u0026rsquo;s code themselves, rather than understanding it passively through pre-written articles. Provide some common examples of looking into the details of the code, such as examining the scope of a variable. After all, teaching people to fish is better than giving them fish. While the code changes often, the methods remain mostly unchanged.\nWhy choose TiDB to read?\nI am not familiar with TiKV or PD.\nTiDB is the entry point directly interacting with users and is also the most likely to be questioned.\nTiDB can run independently and be debugged. If you want to run some SQL after reading the code to verify your understanding, it can be easily done.\nPreparations A development machine\nTiDB is a pure Golang project. It can be conveniently developed on Linux, MacOS, and even Windows. My environment is Windows 10.\nA copy of the TiDB source code, available for download at the official repo.\nGolang environment, following the official guide is straightforward.\nGoland or IntelliJ IDEA + Golang plugin\nI personally feel there\u0026rsquo;s no difference between the two. Why not recommend VSCode + Golang plugin? Mainly because I\u0026rsquo;m used to the JetBrains suite, and indeed commercial software tends to be higher quality than community software. For long-term use, it\u0026rsquo;s recommended to pay for it. Students can use it for free, but need to renew the license every year.\nEnvironment Setup After installing the Golang environment, remember to set the GOPATH, which is usually:\nThe TiDB code doesn\u0026rsquo;t need to be developed under the GOPATH, so you can place it anywhere. I usually create a directory called work and throw various codes in there.\nOpen Goland/IDEA. I use IDEA because I often look at code in other languages.\nOpen with IDEA, select the tidb directory.\nAt this point, IDEA typically prompts you to set up GOROOT and enable Go Modules. Follow the recommendations.\nThe environment setup is now complete.\nEntry Points At the beginning, someone advised me to start with the session package. However, after some experience, I personally feel there are two better entry points: the main function and the dispatch function.\nmain Function The main function of TiDB can be seen at link. You can roughly go through what happens when starting a tidb-server from top to bottom.\nFrom top to bottom:\nParse flags\nOutput version information and exit\nRegister store and monitoring\nConfiguration file check\nInitialize temporary folders, etc.\nSet global variables, CPU affinity, log, trace, print server information, set binlog, set monitoring\nCreate store and domain\nThe createStoreAndDomain method is important, as critical background threads are created here.\nCreate server and register stop signal function\nStart the server\nWithin runServer, the srv.Run() actually brings up the tidb-server. In the Run() function here, the server continuously listens to network requests, creating a new connection for each new request and using a new goroutine to serve it continually.\nAfter this, cleanup work is done when the server needs to stop, ultimately writing out the logs.\nThus, the entire main function process ends. Through the main function, you can see the complete lifecycle of a server from creation to destruction.\nAdditionally, with IDEA, you can easily start and debug TiDB. Click on this triangle symbol as shown in the image below:\nA pop-up with options to run and debug the main function will appear. Essentially, this starts a TiDB with default configurations. TiDB defaults to using mocktikv as the storage engine, so it can be started on a single machine for various testing and validation.\nAs for how to modify the configuration for starting and debugging, this will be introduced in subsequent articles in the series.\ndispatch Function From here, we can proceed further to another suitable entry point function, dispatch.\nThe dispatch function has several characteristics:\nRequests coming from clients only enter the dispatch function, meaning from this point onward, user requests are executed. If you set breakpoints here, you can conveniently filter out SQL executed by internal threads.\nFrom here, various requests are dispatched into different processing logic, ensuring you don’t miss any user requests. It avoids situations like spending significant time reading text protocol code only to find out the user is actually using a binary protocol.\ndispatch itself is located at a very early stage, meaning its parameters mostly come directly from the client\u0026rsquo;s initial information. If it\u0026rsquo;s a text protocol, directly reading parameters can parse out the SQL text.\nAt the start, dispatch primarily focuses on obtaining tokens corresponding to the token-limit parameter. Requests that can\u0026rsquo;t get a token won\u0026rsquo;t execute, which explains why you can create many connections but only 1000 SQL executions are allowed simultaneously by default.\nNext, we enter the most crucial switch case:\nThese commands are MySQL protocol commands, so it\u0026rsquo;s apparent from here exactly what TiDB implements. For comparison, you can refer to this link (this link is only for the text protocol). For full details, see the figure below:\nWithin dispatch, the most important are mysql.ComQuery, as well as the trio mysql.ComStmtPrepare, mysql.ComStmtExecute, and mysql.ComStmtClose. The latter trio is more frequently used in actual production, hence even more important. In contrast, mysql.ComQuery is generally used only for some simple tests and validations.\nSince dispatch is the entry point for interfacing with clients, it can conveniently tally how many requests the database has handled. The so-called QPS derived from monitoring statistics is essentially the number of times this function executes per second. Here arises an issue: in cases like multi-query requests, such as select 1; select 1; select 1;, multiple statements sent together are regarded as a single request by dispatch, but as multiple by clients. While using the binary protocol, some clients prepare a statement, then execute, and finally close it. Seemingly equivalent to executing a single SQL from the client\u0026rsquo;s perspective, the database actually completes three requests.\nIn summary, users’ perceived QPS may not necessarily align with the number of dispatch function calls. In later versions, the QPS panel in TiDB\u0026rsquo;s monitoring was changed to CPS, which stands for Commands Per Second, representing the number of commands executed per second.\nLooking at the callers of dispatch can also reveal information that helps explain some frequently asked questions:\nAn EOF error in dispatch typically means the client has actively disconnected, so there\u0026rsquo;s no need to maintain the database connection, and it is severed.\nIn case of an undetermined error (indicating a transaction\u0026rsquo;s commit is uncertain—whether it has succeeded or failed needs manual intervention for verification), manual intervention is required immediately, and the connection will be closed.\nIf writing binlog fails and ignore-error = false, previously the tidb-server process wouldn\u0026rsquo;t exit but couldn\u0026rsquo;t provide services. Now, the tidb-server will exit directly.\nFor all other dispatch errors, the connection will not be closed, allowing service to continue, but the failure information will be logged as \u0026ldquo;command dispatched failed\u0026rdquo;, which is arguably one of the most critical logs for TiDB.\nConclusion This concludes the introduction from setting up the environment to finding a reasonable entry point to start reading code. Subsequent posts in the series will cover aspects such as configuration (adjustments, default values), variables (default values, scope, actual range, activation), supported syntax, etc. Stay tuned.\n","permalink":"https://blog.minifish.org/posts/tidb1/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eThere are many articles on reading the source code of TiDB, often referred to as \u003ca href=\"https://pingcap.com/blog-cn/#TiDB-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB\"\u003ethe \u0026ldquo;Twenty-Four Chapters Scriptures\u0026rdquo;\u003c/a\u003e. However, these introductions typically proceed from a macro to a micro perspective. This series attempts to introduce how to read TiDB\u0026rsquo;s source code from an easier angle. The goals we aim to achieve are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eEnable readers to start reading TiDB\u0026rsquo;s code themselves, rather than understanding it passively through pre-written articles.\u003c/li\u003e\n\u003cli\u003eProvide some common examples of looking into the details of the code, such as examining the scope of a variable.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAfter all, teaching people to fish is better than giving them fish. While the code changes often, the methods remain mostly unchanged.\u003c/p\u003e","title":"How to Read TiDB Source Code (Part 1)"},{"content":"Background I had to install Docker on Windows to reproduce a bug.\nProcess Using Windows 10 as an example, if you have the Home Basic version, you\u0026rsquo;ll need to pay to upgrade to the Pro version because you need to enable Hyper-V and Container features, which costs about 800 RMB. Install everything with the default settings, and do not switch to Windows Containers, since most images are still under Linux. If you do switch, you can restore it after starting up. If you encounter permission issues with shared folders, follow the instructions at link. However, this might not solve the problem, and you might encounter a sharing failure. In that case, go to the settings, troubleshoot, and reset to factory defaults. After resetting, ensure the shared folders are selected. When you encounter errors during use, just try a few more times. It might work; if not, reset it. Impressions Initially, there were no issues using Docker on Linux; Docker itself was simple back then. Later, using it on Mac brought changes, including a user interface, various colors, and numerous bugs. Right from the start, I encountered bugs. Docker did not support Windows a long time ago, and given the various bugs on Mac, I didn\u0026rsquo;t have high expectations. The results were still quite surprising. In summary, here are a few points:\nIt\u0026rsquo;s easier to use. There are more bugs. Do not expect much, and be prepared to reset at any time. Fortunately, resetting offers a shortcut, making it a pretty usable tool. It\u0026rsquo;s incredibly slow. At this point, I have no optimism for Docker, Kubernetes, or similar technologies. Done~\n","permalink":"https://blog.minifish.org/posts/docker-win/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eI had to install Docker on Windows to reproduce a bug.\u003c/p\u003e\n\u003ch2 id=\"process\"\u003eProcess\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eUsing Windows 10 as an example, if you have the Home Basic version, you\u0026rsquo;ll need to pay to upgrade to the Pro version because you need to enable Hyper-V and Container features, which costs about 800 RMB.\u003c/li\u003e\n\u003cli\u003eInstall everything with the default settings, and do not switch to Windows Containers, since most images are still under Linux. If you do switch, you can restore it after starting up.\u003c/li\u003e\n\u003cli\u003eIf you encounter permission issues with shared folders, follow the instructions at \u003ca href=\"https://github.com/docker/for-win/issues/3174\"\u003elink\u003c/a\u003e. However, this might not solve the problem, and you might encounter a sharing failure. In that case, go to the settings, troubleshoot, and reset to factory defaults. After resetting, ensure the shared folders are selected.\u003c/li\u003e\n\u003cli\u003eWhen you encounter errors during use, just try a few more times. It might work; if not, reset it.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"impressions\"\u003eImpressions\u003c/h2\u003e\n\u003cp\u003eInitially, there were no issues using Docker on Linux; Docker itself was simple back then. Later, using it on Mac brought changes, including a user interface, various colors, and numerous bugs. Right from the start, I encountered bugs. Docker did not support Windows a long time ago, and given the various bugs on Mac, I didn\u0026rsquo;t have high expectations. The results were still quite surprising. In summary, here are a few points:\u003c/p\u003e","title":"How to Use Docker on Windows"},{"content":"Initially, I thought it would be a simple setting adjustment, so I casually Googled it. Sure enough, there was a unanimous solution: modify /etc/systemd/logind.conf, change HandleLidSwitch to ignore or lock, and then restart logind or reboot.\nI tried this, but it didn\u0026rsquo;t work at all on my Thinkpad X230. I then tried changing some other options in the aforementioned file, but none worked, and surprisingly, Ubuntu even reported errors.\nSo, I reinstalled the more preferred Debian. Tried again, and it still didn\u0026rsquo;t work. Finally, I found a more brutal method.\nsystemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target This directly points these units to /dev/null\u0026hellip;\nTo revert, simply use:\nsystemctl unmask sleep.target suspend.target hibernate.target hybrid-sleep.target It\u0026rsquo;s simple and effective.\nUpdate:\nIf you only mask them, the CPU usage of systemd-logind will be very high because it continuously attempts to sleep. Therefore, you also need to change HandleLidSwitch and others to ignore. As follows:\nHandleSuspendKey=ignore HandleHibernateKey=ignore HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleLidSwitchDocked=ignore Then, execute systemctl restart systemd-logind. For more details, refer to this: SystemD-LoginD High CPU Usage.\n","permalink":"https://blog.minifish.org/posts/nosleeplinux/","summary":"\u003cp\u003eInitially, I thought it would be a simple setting adjustment, so I casually Googled it. Sure enough, there was a unanimous solution: modify \u003ccode\u003e/etc/systemd/logind.conf\u003c/code\u003e, change \u003ccode\u003eHandleLidSwitch\u003c/code\u003e to \u003ccode\u003eignore\u003c/code\u003e or \u003ccode\u003elock\u003c/code\u003e, and then restart \u003ccode\u003elogind\u003c/code\u003e or reboot.\u003c/p\u003e\n\u003cp\u003eI tried this, but it didn\u0026rsquo;t work at all on my Thinkpad X230. I then tried changing some other options in the aforementioned file, but none worked, and surprisingly, \u003ccode\u003eUbuntu\u003c/code\u003e even reported errors.\u003c/p\u003e\n\u003cp\u003eSo, I reinstalled the more preferred \u003ccode\u003eDebian\u003c/code\u003e. Tried again, and it still didn\u0026rsquo;t work. Finally, I found a more brutal method.\u003c/p\u003e","title":"How to Prevent a Linux Laptop from Entering Sleep Mode When the Lid is Closed"},{"content":"Dynamically Updating DNS Records The purpose of dynamic updates is pretty simple. As a long-term user of China Unicom\u0026rsquo;s broadband, although Unicom provides a public address, it is essentially a dynamic address, not a fixed one. If you want to access your home devices using an IP address from outside, you need to use dynamic DNS (DDNS).\nMany routers come with built-in DDNS functionality, but they mostly wrap the interfaces of the commonly used service providers. These providers generally have a few characteristics: 1. Blocked by the Great Firewall; 2. Not cheap; 3. Domestic providers may have security issues; 4. The company might have gone out of business. Rather than relying on these unreliable services, it\u0026rsquo;s better to write your own script for updating.\nThus, the scripting approach comes into play. Initially, I planned to use Cloudflare, as it is the recommended way. Later, I discovered that my domain provider Namesilo offers an API to update DNS. However, it returns data in XML format, and I wasn\u0026rsquo;t sure how to parse this with shell scripts.\nThen, I thought of using Go. Since this DDNS client would likely be deployed on a router-like device, languages like Python or Java require a runtime environment, and C might need some dynamic libraries to run, which I wasn\u0026rsquo;t sure how to handle. The fact that Go doesn\u0026rsquo;t require dynamic libraries was a significant advantage. So, I handwrote a tool called und.\nFirst, create a DNS record in Namesilo. Obtain the binary of und suitable for your platform. I only provide binaries for arm64|linux and amd64|three mainstream OS in this case. For other platforms, you\u0026rsquo;ll need to compile it yourself. You can refer to the Makefile. Generate an API key from Namesilo, then start und according to its usage documentation. You might need to run it in the background, so use nohup. GitHub Features Experience GitHub Actions It feels like a replacement for chaotic third-party CI services. I directly chose the Go option for und. By default, it simply runs go build -v . in Ubuntu.\nRelease I used this feature when releasing TiDB before, but didn\u0026rsquo;t remember to upload/automatically generate binaries. Since TiDB is not something that can run completely with just one component, releasing a single binary doesn\u0026rsquo;t make much sense.\nThis time, the experience led me to believe:\nWhen releasing, tagging is best done directly using the release feature. After the release, since you can edit it, it\u0026rsquo;s a good time to make each binary and upload them. Based on the Makefile setup, you can generate a version. This is quite an important feature. These two steps are quite convenient.\n","permalink":"https://blog.minifish.org/posts/und/","summary":"\u003ch2 id=\"dynamically-updating-dns-records\"\u003eDynamically Updating DNS Records\u003c/h2\u003e\n\u003cp\u003eThe purpose of dynamic updates is pretty simple. As a long-term user of China Unicom\u0026rsquo;s broadband, although Unicom provides a public address, it is essentially a dynamic address, not a fixed one. If you want to access your home devices using an IP address from outside, you need to use dynamic DNS (DDNS).\u003c/p\u003e\n\u003cp\u003eMany routers come with built-in DDNS functionality, but they mostly wrap the interfaces of the commonly used service providers. These providers generally have a few characteristics: 1. Blocked by the Great Firewall; 2. Not cheap; 3. Domestic providers may have security issues; 4. The company might have gone out of business. Rather than relying on these unreliable services, it\u0026rsquo;s better to write your own script for updating.\u003c/p\u003e","title":"How to Dynamically Update DNS Records for Namesilo"},{"content":" Create a script and place it in\nC:\\Users\\name\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\\` Fill the script with:\nStart-Process -FilePath \u0026#34;C:\\Users\\name\\bin\\gost-windows-amd64.exe\u0026#34; -ArgumentList \u0026#34;-L=\u0026#34;, \u0026#34;-F=\u0026#34; -RedirectStandardOutput \u0026#34;C:\\Users\\name\\bin\\gost-windows-amd64.log\u0026#34; -RedirectStandardError \u0026#34;C:\\Users\\name\\bin\\gost-windows-amd64.err\u0026#34; -WindowStyle Hidden Note: Start-Process seems to perform a fork-like action, and by default, it opens a new PowerShell window to execute. That\u0026rsquo;s why -WindowStyle Hidden is added at the end. You can\u0026rsquo;t use -NoNewWindow here because it only prevents the creation of a new window for executing Start-Process, but the old window will not exit.\nNote 2: After the old window exits, the forked process seems to become an orphan and is managed elsewhere, so permissions, such as network connection permissions, might need to be requested again.\n","permalink":"https://blog.minifish.org/posts/powershell-startup/","summary":"\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a script and place it in\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-powershell\" data-lang=\"powershell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eC:\\Users\\name\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\\`  \n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFill the script with:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eStart-Process -FilePath \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;C:\\Users\\name\\bin\\gost-windows-amd64.exe\u0026#34;\u003c/span\u003e -ArgumentList \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;-L=\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;-F=\u0026#34;\u003c/span\u003e -RedirectStandardOutput \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;C:\\Users\\name\\bin\\gost-windows-amd64.log\u0026#34;\u003c/span\u003e -RedirectStandardError \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;C:\\Users\\name\\bin\\gost-windows-amd64.err\u0026#34;\u003c/span\u003e -WindowStyle Hidden\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote: \u003ccode\u003eStart-Process\u003c/code\u003e seems to perform a fork-like action, and by default, it opens a new PowerShell window to execute. That\u0026rsquo;s why \u003ccode\u003e-WindowStyle Hidden\u003c/code\u003e is added at the end. You can\u0026rsquo;t use \u003ccode\u003e-NoNewWindow\u003c/code\u003e here because it only prevents the creation of a new window for executing \u003ccode\u003eStart-Process\u003c/code\u003e, but the old window will not exit.\u003cbr\u003e\nNote 2: After the old window exits, the forked process seems to become an orphan and is managed elsewhere, so permissions, such as network connection permissions, might need to be requested again.\u003c/p\u003e","title":"How to Start a PowerShell Script in the Background at Windows Startup"},{"content":"Preface One day, I came across an article by Chen Hao on Twitter. Having benefited from several of his blog posts, I instinctively felt it was reliable, so I read it and decided to write this practical guide.\nWhy Use an HTTPS Proxy In the guide, it’s clearly explained why, plus my own experiences of several shadowsocks being banned, I felt it was necessary to switch to a more secure proxy method.\nHow to Deploy an HTTPS Proxy gost gost is the tool most recommended in the guide. At first, I misunderstood it as a method similar to kcptun, still relying on shadowsocks. In fact, gost implements multiple proxy types, meaning you don’t need other proxies if you have it. I never liked the method of continuously wrapping to accelerate/obfuscate shadowsocks, always feeling that longer pathways bring more problems.\nSteps Directly download the latest release from the gost repo. Although I have a Golang environment both locally and on the VPS, downloading directly is the easiest. I downloaded version 2.9.0 here.\nFollowing certbot on a bare VPS doesn\u0026rsquo;t work\u0026hellip; it requires:\nStarting an nginx server, as referenced in this guide. Of course, this requires having a domain name pointing an A record to the VPS. Verifying access through the domain. Stopping nginx. Using certbot\u0026rsquo;s \u0026ndash;standalone mode, which will generate the certificates upon success. Here, I didn\u0026rsquo;t use Docker for deployment but used systemd instead, directly creating a systemd unit similar to kcptun. The difference is, because the certificate needs updating, the unit requires a reload method. This tutorial teaches a lot about using systemd, and the author\u0026rsquo;s article quality is also high, highly recommended for subscription.\nCreate a /lib/systemd/system/gost.service file with the following content, replacing the domain with your own: [Unit] Description=gost service After=network.target StartLimitIntervalSec=0 [Service] Type=simple Restart=always RestartSec=1 User=root PIDFile=/home/admin/gost.pid ExecStart=/home/admin/bin/gost -L \u0026#34;http2://xxx:yyy@0.0.0.0:443?cert=/etc/letsencrypt/live/example.com/fullchain.pem\u0026amp;key=/etc/letsencrypt/live/example.com/privkey.pem\u0026amp;probe_resist=code:404\u0026#34; ExecReload=/bin/kill -HUP $MAINPID [Install] WantedBy=multi-user.target ExecStart is a simplified version of the Docker method in the guide. ExecReload just kills the process.\nTest whether it’s successful using systemctl start|status|restart|enable gost.\nConfigure crontab to update the certificate. I didn\u0026rsquo;t use systemd because I\u0026rsquo;m not familiar with it.\n0 0 1 * * /usr/bin/certbot renew --force-renewal 5 0 1 * * systemctl restart gost After completing the above, nginx can be directly stopped and disabled.\nConfigure the client. This is simple; just refer to the guide. The principle is straightforward because gost implements the shadowsocks protocol using the shadowsocks Golang version. Therefore, the following command starts a local shadowsocks server, and you configure your client to add a local server configuration that matches the password.\n.\\bin\\gost-windows-amd64.exe -L=ss://aes-128-cfb:passcode@:1984 -F=https://xxx:yyy@example.com:443 PS: I still don\u0026rsquo;t know how to configure a global HTTPS proxy on Android without root, or how to set it up on iOS without a U.S. account. Also, I\u0026rsquo;m unsure how to elegantly configure startup scripts on Windows 10. These are issues to explore further\u0026hellip;\nContinuation Regarding the mobile problem mentioned above, I found that HTTPS proxy client support is generally poor. Gost itself seems to have problems, possibly due to my usage. In short, if not using a local gost to connect remotely, authentication errors occur.\nDuring the holiday break, I tinkered a bit more. First, I deployed a gost HTTP proxy on my home NAS using the simplest nohup + ctrl-D method to maintain it. It\u0026rsquo;s compiled with GOARCH=arm64. After a trial run for a day, Android\u0026rsquo;s weak built-in HTTP proxy worked well, but globally routing through it wasn\u0026rsquo;t great. Hence, I switched from HTTP to using SS to connect to HTTPS remotely. I essentially moved the local service on Windows to my NAS. Additionally, through simple double-port forwarding from NAS -\u0026gt; internal router -\u0026gt; optical modem router, I could also use the NAS as an SS server via the public IP.\nThe remaining issue is the DDNS. After researching, it seems Cloudflare\u0026rsquo;s API is a more reliable option. Seeing an official flarectl, I compiled it to the NAS and wrote a small script, revisiting the various (pitfalls) wonders of bash, especially remembering special writing for string comparisons such as [ $a != $b ] to [ $a != $b* ] to handle trailing \u0026ldquo;\\r\u0026rdquo; \u0026ldquo;\\n\u0026rdquo; characters. However, detaching the name server still takes some time. The final effect is to be tested.\nAdditionally, on the NAS, I currently use curl to fetch my public IP from a third-party. I have a hunch that this method might not work someday or might cause issues.\n","permalink":"https://blog.minifish.org/posts/gost/","summary":"\u003ch2 id=\"preface\"\u003ePreface\u003c/h2\u003e\n\u003cp\u003eOne day, I came across an article by Chen Hao on Twitter. Having benefited from several of his blog posts, I instinctively felt it was reliable, so I read it and decided to write this practical guide.\u003c/p\u003e\n\u003ch2 id=\"why-use-an-https-proxy\"\u003eWhy Use an HTTPS Proxy\u003c/h2\u003e\n\u003cp\u003eIn the \u003ca href=\"https://haoel.github.io/\"\u003eguide\u003c/a\u003e, it’s clearly explained why, plus my own experiences of several shadowsocks being banned, I felt it was necessary to switch to a more secure proxy method.\u003c/p\u003e","title":"How to Deploy an HTTPS Proxy Service"},{"content":"Background When I first started writing Golang, I was always looking for a convenient debugging tool. Back then, I came across documentation about using gdb to debug and also tried delve, but neither felt easy to use. Later, on someone\u0026rsquo;s advice, I went back to the good old print statements\u0026hellip;\nOver the past couple of days, I was debugging go test and found that tests would always hang when run per package. I couldn\u0026rsquo;t think of a suitable method at first, so I thought of delve again. After giving it a try, I found it has become much more mature than before.\nUsage dlv attach ${pid} is the method I use most often. After attaching, you can use debugging commands similar to gdb. You can use help to view specific commands.\n(dlv) help The following commands are available: args ------------------------ Print function arguments. break (alias: b) ------------ Sets a breakpoint. breakpoints (alias: bp) ----- Print out info for active breakpoints. call ------------------------ Resumes process, injecting a function call (EXPERIMENTAL!!!) clear ----------------------- Deletes breakpoint. clearall -------------------- Deletes multiple breakpoints. condition (alias: cond) ----- Set breakpoint condition. config ---------------------- Changes configuration parameters. continue (alias: c) --------- Run until breakpoint or program termination. deferred -------------------- Executes command in the context of a deferred call. disassemble (alias: disass) - Disassembler. down ------------------------ Move the current frame down. edit (alias: ed) ------------ Open where you are in $DELVE_EDITOR or $EDITOR exit (alias: quit | q) ------ Exit the debugger. frame ----------------------- Set the current frame, or execute command on a different frame. funcs ----------------------- Print list of functions. goroutine (alias: gr) ------- Shows or changes current goroutine. goroutines (alias: grs) ----- List program goroutines. help (alias: h) ------------- Prints the help message. libraries ------------------- List loaded dynamic libraries. list (alias: ls | l) -------- Show source code. locals ---------------------- Print local variables. next (alias: n) ------------- Step over to next source line. on -------------------------- Executes a command when a breakpoint is hit. print (alias: p) ------------ Evaluate an expression. regs ------------------------ Print contents of CPU registers. restart (alias: r) ---------- Restart process. set ------------------------- Changes the value of a variable. source ---------------------- Executes a file containing a list of delve commands. sources --------------------- Print list of source files. stack (alias: bt) ----------- Print stack trace. step (alias: s) ------------- Single step through program. step-instruction (alias: si) Single step a single CPU instruction. stepout (alias: so) --------- Step out of the current function. thread (alias: tr) ---------- Switch to the specified thread. threads --------------------- Print out info for every traced thread. trace (alias: t) ------------ Set tracepoint. types ----------------------- Print list of types. up -------------------------- Move the current frame up. vars ------------------------ Print package variables. whatis ---------------------- Prints type of an expression. Type help followed by a command for full documentation. Many of these commands are the same as those in gdb. Another command I use frequently is grs, which outputs all goroutines. You can also use grs -t, which is equivalent to gdb\u0026rsquo;s t a a bt. The only slight drawback is that it only outputs 10 stack frames, and any additional ones are truncated.\nAdditionally, it seems that processes forked by go test can\u0026rsquo;t be attached to. If you want to test, you must first compile it into a test file and then execute it. You can refer to this issue for more details.\n$ dlv attach 19654 could not attach to pid 19654: decoding dwarf section info at offset 0x0: too short Furthermore, by default, Go\u0026rsquo;s test caches results, which can be controlled via environment variables. However, with Go modules (go mod), it\u0026rsquo;s recommended to use ./ddl.test -test.count=1 to disable caching. It doesn\u0026rsquo;t feel very elegant.\n","permalink":"https://blog.minifish.org/posts/dlv/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eWhen I first started writing Golang, I was always looking for a convenient debugging tool. Back then, I came across documentation about using \u003ccode\u003egdb\u003c/code\u003e to debug and also tried \u003ccode\u003edelve\u003c/code\u003e, but neither felt easy to use. Later, on someone\u0026rsquo;s advice, I went back to the good old \u003ccode\u003eprint\u003c/code\u003e statements\u0026hellip;\u003c/p\u003e\n\u003cp\u003eOver the past couple of days, I was debugging \u003ccode\u003ego test\u003c/code\u003e and found that tests would always hang when run per package. I couldn\u0026rsquo;t think of a suitable method at first, so I thought of \u003ccode\u003edelve\u003c/code\u003e again. After giving it a try, I found it has become much more mature than before.\u003c/p\u003e","title":"Using delve to Debug Golang Programs"},{"content":"There are multiple versions of the Shadowsocks server side implementation. The original version was written in Python, and later, enthusiasts implemented it in various programming languages of their liking.\nAmong all these implementations, I personally think the most reliable and stable one is the original Python version. The reason is simple - it has the most users. The Golang version is said to have the most features and also performs very well, making it quite powerful. This might be due to Golang’s inherent high performance and ease of implementation. There\u0026rsquo;s also an implementation using libev, a pure C implementation, which also offers good performance and is very lightweight.\nAdditionally, updating the server is a necessary task for Shadowsocks users due to well-known reasons. The server should be updated frequently. If you’re using the Python implementation, you might be able to install updates via pip, although I haven’t confirmed this. The Golang version may require a Golang build environment, and then you can use go get -u. For updating libev, you can use apt on Debian-based systems, as apt includes shadowsocks-libev. I haven’t checked if it is available in the Red Hat-based yum repositories.\nAfter this introduction, let\u0026rsquo;s go over the deployment steps, which are quite straightforward:\nDeploy a Debian 9 or Ubuntu 17 VPS. Mainstream providers like Vultr should have these options available. Assume we are using Debian 9 here. Run apt install shadowsocks-libev to install. Edit the configuration file using vim /etc/shadowsocks-libev/config.json. It\u0026rsquo;s best to set the Server IP to 0.0.0.0 to avoid IP issues similar to those on AWS Lightsail. *. For AWS Lightsail, you need to bind a static IP and open firewall ports. Specific steps can be found on Google. Restart the service using systemctl restart shadowsocks-libev to apply the changes. Enable TCP BBR. Specific instructions can be found on Google. ","permalink":"https://blog.minifish.org/posts/shadowsocks/","summary":"\u003cp\u003eThere are multiple versions of the Shadowsocks server side implementation. The original version was written in Python, and later, enthusiasts implemented it in various programming languages of their liking.\u003c/p\u003e\n\u003cp\u003eAmong all these implementations, I personally think the most reliable and stable one is the original Python version. The reason is simple - it has the most users. The Golang version is said to have the most features and also performs very well, making it quite powerful. This might be due to Golang’s inherent high performance and ease of implementation. There\u0026rsquo;s also an implementation using libev, a pure C implementation, which also offers good performance and is very lightweight.\u003c/p\u003e","title":"How to Deploy a Shadowsocks Server"},{"content":"Let\u0026rsquo;s first look at the following code snippet:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; ) func main() { runtime.GOMAXPROCS(2) a := make(map[int]int) go func() { i := 0 for { a[1] = i i++ time.Sleep(1000) } }() for { if a[1] \u0026gt; 1000000 { fmt.Println(a[1]) os.Exit(1) } } } After compiling and running it (assuming your machine has 2 or more cores), you will get the following error:\nfatal error: concurrent map read and map write goroutine 1 [running]: runtime.throw(0x10c3e05, 0x21) /usr/local/Cellar/go/1.10.3/libexec/src/runtime/panic.go:616 +0x81 fp=0xc42004bf00 sp=0xc42004bee0 pc=0x10263f1 runtime.mapaccess1_fast64(0x10a5b60, 0xc42007e180, 0x1, 0xc42008e048) /usr/local/Cellar/go/1.10.3/libexec/src/runtime/hashmap_fast.go:101 +0x197 fp=0xc42004bf28 sp=0xc42004bf00 pc=0x1008d27 main.main() /Users/yusp/test/panic3/main.go:22 +0x7c fp=0xc42004bf88 sp=0xc42004bf28 pc=0x108e28c runtime.main() /usr/local/Cellar/go/1.10.3/libexec/src/runtime/proc.go:198 +0x212 fp=0xc42004bfe0 sp=0xc42004bf88 pc=0x1027c62 runtime.goexit() /usr/local/Cellar/go/1.10.3/libexec/src/runtime/asm_amd64.s:2361 +0x1 fp=0xc42004bfe8 sp=0xc42004bfe0 pc=0x104e501 goroutine 5 [runnable]: time.Sleep(0x3e8) /usr/local/Cellar/go/1.10.3/libexec/src/runtime/time.go:102 +0x166 main.main.func1(0xc42007e180) /Users/yusp/test/panic3/main.go:18 +0x61 created by main.main /Users/yusp/test/panic3/main.go:13 +0x59 It seems straightforward; Golang\u0026rsquo;s map is not thread-safe, and concurrent read and write cause a panic. However, look at the error information on line /Users/yusp/test/panic3/main.go:18 +0x61, which points to line 18 of main.go where Sleep is called, not the actual point of concurrency issue. In a vast stack trace, it becomes even harder to locate the problem.\nA workaround that comes to mind is if you only see the read stack and want to see the write stack, set a variable at the read position, reset it after reading, and check the value of this variable at the write position. If reading is currently happening, panic will be triggered.\n","permalink":"https://blog.minifish.org/posts/golang-panic/","summary":"\u003cp\u003eLet\u0026rsquo;s first look at the following code snippet:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-golang\" data-lang=\"golang\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003epackage\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e (\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;fmt\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;os\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;runtime\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;time\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eruntime\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eGOMAXPROCS\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e make(\u003cspan style=\"color:#66d9ef\"\u003emap\u003c/span\u003e[\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e]\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003ego\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003ei\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e] = \u003cspan style=\"color:#a6e22e\"\u003ei\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#a6e22e\"\u003ei\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e++\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#a6e22e\"\u003etime\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eSleep\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1000\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t}()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e] \u0026gt; \u003cspan style=\"color:#ae81ff\"\u003e1000000\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eExit\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAfter compiling and running it (assuming your machine has 2 or more cores), you will get the following error:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-text\" data-lang=\"text\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efatal error: concurrent map read and map write\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egoroutine 1 [running]:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eruntime.throw(0x10c3e05, 0x21)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        /usr/local/Cellar/go/1.10.3/libexec/src/runtime/panic.go:616 +0x81 fp=0xc42004bf00 sp=0xc42004bee0 pc=0x10263f1\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eruntime.mapaccess1_fast64(0x10a5b60, 0xc42007e180, 0x1, 0xc42008e048)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        /usr/local/Cellar/go/1.10.3/libexec/src/runtime/hashmap_fast.go:101 +0x197 fp=0xc42004bf28 sp=0xc42004bf00 pc=0x1008d27\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emain.main()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        /Users/yusp/test/panic3/main.go:22 +0x7c fp=0xc42004bf88 sp=0xc42004bf28 pc=0x108e28c\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eruntime.main()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        /usr/local/Cellar/go/1.10.3/libexec/src/runtime/proc.go:198 +0x212 fp=0xc42004bfe0 sp=0xc42004bf88 pc=0x1027c62\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eruntime.goexit()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        /usr/local/Cellar/go/1.10.3/libexec/src/runtime/asm_amd64.s:2361 +0x1 fp=0xc42004bfe8 sp=0xc42004bfe0 pc=0x104e501\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egoroutine 5 [runnable]:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etime.Sleep(0x3e8)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        /usr/local/Cellar/go/1.10.3/libexec/src/runtime/time.go:102 +0x166\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emain.main.func1(0xc42007e180)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        /Users/yusp/test/panic3/main.go:18 +0x61\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecreated by main.main\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        /Users/yusp/test/panic3/main.go:13 +0x59\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIt seems straightforward; Golang\u0026rsquo;s map is not thread-safe, and concurrent read and write cause a panic. However, look at the error information on line \u003ccode\u003e/Users/yusp/test/panic3/main.go:18 +0x61\u003c/code\u003e, which points to line 18 of main.go where \u003ccode\u003eSleep\u003c/code\u003e is called, not the actual point of concurrency issue. In a vast stack trace, it becomes even harder to locate the problem.\u003c/p\u003e","title":"How Immediate is Golang's Panic"},{"content":"In a previous article “TiDB Source Code Reading Series (4) Overview of INSERT Statement”, we introduced the general process of the INSERT statement. Why write a separate article for INSERT? Because in TiDB, simply inserting a piece of data is the simplest and most common case. It becomes more complex when defining various behaviors within the INSERT statement, such as how to handle situations with Unique Key conflicts: Should we return an error? Ignore the current data insertion? Or overwrite existing data? Therefore, this article will continue to delve into the INSERT statement.\nThis article will first introduce the classification of INSERT statements in TiDB, along with the syntax and semantics of each statement, and then describe the source code implementation of the five types of INSERT statements.\nTypes of INSERT Statements In broad terms, TiDB has the following six types of INSERT statements:\nBasic INSERT INSERT IGNORE INSERT ON DUPLICATE KEY UPDATE INSERT IGNORE ON DUPLICATE KEY UPDATE REPLACE LOAD DATA In theory, all six statements belong to the category of INSERT statements.\nThe first one, Basic INSERT, is the most common INSERT statement, using the syntax INSERT INTO VALUES (). It implies inserting a record, and if a unique constraint conflict occurs (such as primary key conflict, unique index conflict), it returns an execution failure.\nThe second, with the syntax INSERT IGNORE INTO VALUES (), ignores the current INSERT row if a unique constraint conflict occurs and logs a warning. After the statement execution finishes, you can use SHOW WARNINGS to see which rows were not inserted.\nThe third one, with the syntax INSERT INTO VALUES () ON DUPLICATE KEY UPDATE, updates the conflicting row, then inserts data if there is a conflict. If the updated row conflicts with another row in the table, it returns an error.\nThe fourth one, similar to the previous case, if the updated row conflicts with another row, this does not insert the row and shows a warning.\nThe fifth one, with the syntax REPLACE INTO VALUES (), deletes the conflicting row in the table after a conflict and continues to attempt data insertion. If another conflict occurs again, it continues to delete conflicting data on the table until there is no conflicting data left in the table, then inserts the data.\nThe last one, using the syntax LOAD DATA INFILE INTO, has semantics similar to INSERT IGNORE, both ignoring conflicts. The difference is that LOAD DATA imports data files into a table, meaning its data source is a CSV data file.\nSince INSERT IGNORE ON DUPLICATE KEY UPDATE involves special processing on INSERT ON DUPLICATE KEY UPDATE, it won\u0026rsquo;t be explained in detail separately but will be covered in the same section. Due to the unique nature of LOAD DATA, it will be discussed in other chapters.\nBasic INSERT Statement The major differences among the several INSERT statements lie in the execution level. Continuing from the “TiDB Source Code Reading Series (4) Overview of INSERT Statement”, here is the statement execution process. Those who do not remember the previous content can refer back to the original article.\nINSERT\u0026rsquo;s execution logic is located in executor/insert.go. In fact, the execution logic for all four types of INSERT statements covered previously is in this file. Here, we first discuss the most basic Basic INSERT.\nInsertExec is an implementation of the INSERT executor, conforming to the Executor interface. The most important methods are the following three interfaces:\nOpen: Performs some initialization Next: Executes the write operation Close: Performs some cleanup tasks Among them, the most important and complex is the Next method. Depending on whether a SELECT statement is used to retrieve data (INSERT SELECT FROM), the Next process is divided into two branches: insertRows and insertRowsFromSelect. Both processes eventually lead to the exec function to execute the INSERT.\nIn the exec function, the first four types of INSERT statements are processed together. The standard INSERT covered in this section directly enters insertOneRow.\nBefore discussing insertOneRow, let\u0026rsquo;s look at a segment of SQL.\nCREATE TABLE t (i INT UNIQUE); INSERT INTO t VALUES (1); BEGIN; INSERT INTO t VALUES (1); COMMIT; Paste these lines of SQL sequentially into MySQL and TiDB to see the results.\nMySQL:\nmysql\u0026gt; CREATE TABLE t (i INT UNIQUE); Query OK, 0 rows affected (0.15 sec) mysql\u0026gt; INSERT INTO t VALUES (1); Query OK, 1 row affected (0.01 sec) mysql\u0026gt; BEGIN; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; INSERT INTO t VALUES (1); ERROR 1062 (23000): Duplicate entry \u0026#39;1\u0026#39; for key \u0026#39;i\u0026#39; mysql\u0026gt; COMMIT; Query OK, 0 rows affected (0.11 sec) TiDB:\nmysql\u0026gt; CREATE TABLE t (i INT UNIQUE); Query OK, 0 rows affected (1.04 sec) mysql\u0026gt; INSERT INTO t VALUES (1); Query OK, 1 row affected (0.12 sec) mysql\u0026gt; BEGIN; Query OK, 0 rows affected (0.01 sec) mysql\u0026gt; INSERT INTO t VALUES (1); Query OK, 1 row affected (0.00 sec) mysql\u0026gt; COMMIT; ERROR 1062 (23000): Duplicate entry \u0026#39;1\u0026#39; for key \u0026#39;i\u0026#39; As you can see, for INSERT statements, TiDB performs conflict detection at the time of transaction commit, whereas MySQL does it when the statement is executed. The reason for this is that TiDB is designed with a layered structure with TiKV; to ensure efficient execution, only read operations within a transaction must retrieve data from the storage engine, while all write operations are initially placed within the transaction\u0026rsquo;s own memDbBuffer in a single TiDB instance. The data is then written to TiKV as a batch during transaction commit. In the implementation, the PresumeKeyNotExists option is set within insertOneRow, assuming that insertions will not encounter conflicts if no conflicts are detected locally, without needing to check for conflicting data in TiKV. These data are marked as pending verification, and the BatchGet interface is used during the commit process to batch check the whole transaction\u0026rsquo;s pending data.\nAfter all the data goes through insertOneRow and completes the insertion, the INSERT statement essentially concludes. The remaining tasks involve setting the lastInsertID and other return information, and then returning the results to the client.\nINSERT IGNORE Statement The semantics of INSERT IGNORE were introduced earlier. It was mentioned how a standard INSERT checks at the time of commit, but can INSERT IGNORE do the same? The answer is no, because:\nIf INSERT IGNORE is checked at the commit, the transaction module will need to know which rows should be ignored and which should immediately raise errors and roll back, undoubtedly increasing module coupling. Users want to immediately know which rows were not inserted through INSERT IGNORE. In other words, they would like to see which rows were not actually inserted immediately through SHOW WARNINGS. This requires checking data conflicts promptly when executing INSERT IGNORE. One obvious approach is to try reading the data intended for insertion, logging a warning when finding a conflict, and proceeding to the next row. However, if the statement inserts multiple rows, it would require repetitive reads from TiKV for conflict detection, which would be inefficient. Therefore, TiDB implements a batchChecker, with the code located in executor/batch_checker.go.\nIn the batchChecker, first, prepare the data for insertion, constructing possible conflicting unique constraints into a key within getKeysNeedCheck. TiDB implements unique constraints by constructing unique keys, as detailed in “Three Articles to Understand TiDB\u0026rsquo;s Technical Inside Story – On Computation”.\nThen, pass the constructed keys through BatchGetValues to read them all at once, resulting in a key-value map where those read are the conflicting data.\nFinally, check the keys of the data intended for insertion against the results from BatchGetValues. If a conflicting row is found, prepare a warning message and proceed to the next row. If a conflicting row isn’t found, a safe INSERT can proceed. The implementation of this portion is found in batchCheckAndInsert.\nSimilarly, after executing the insertion for all data, return information is set, and the execution results are returned to the client.\nINSERT ON DUPLICATE KEY UPDATE Statement INSERT ON DUPLICATE KEY UPDATE is the most complex among the INSERT statements. Its semantic essence includes both an INSERT and an UPDATE. The complexity arises since during an UPDATE, a row can be updated to any valid version.\nIn the previous section, it was discussed how TiDB uses batching to implement conflict checking for special INSERT statements. The same method is used for INSERT ON DUPLICATE KEY UPDATE, but the implementation process is somewhat more complex due to the semantic complexity.\nInitially, similar to INSERT IGNORE, the keys constructed from the data to be inserted are read out at once using BatchGetValues, resulting in a key-value map. Then, all records corresponding to the read keys are again read using a batch BatchGetValues, prepared for possible future UPDATE operations. The specific implementation is in initDupOldRowValue.\nThen, during conflict checking, if a conflict occurs, an UPDATE is performed first. As discussed in the Basic INSERT section earlier, TiDB executes INSERT in TiKV during commit. Similarly, UPDATE is also executed in TiKV during commit. In this UPDATE process, unique constraint conflicts might still occur. If so, then an error is returned. If the statement is INSERT IGNORE ON DUPLICATE KEY UPDATE, this error is ignored, and the next row proceeds.\nIn the UPDATE from the previous step, another scenario can occur, as in the SQL below:\nCREATE TABLE t (i INT UNIQUE); INSERT INTO t VALUES (1), (1) ON DUPLICATE KEY UPDATE i = i; Here, it is clear that there are no original data in the table; the INSERT in the second line cannot read out possibly conflicting data, but there is a conflict between the two rows of data intended to be inserted themselves. Correct execution here should involve the first 1 being inserted normally, with the second 1 encountering conflict and updating the first 1. Thus, it is necessary to handle it as follows: remove the key-value of the data updated in the previous step from the initial step\u0026rsquo;s key-value map, reconstruct unique constraint keys and values for the data from the UPDATE based on table information, and add this key-value pair back into the initial key-value map for subsequent data conflict checking. The detail implementation is in fillBackKeys. This scenario also arises in other INSERT statements like INSERT IGNORE, REPLACE, and LOAD DATA. It is introduced here because INSERT ON DUPLICATE KEY UPDATE showcases the full functionality of the batchChecker.\nFinally, after all data completes insertion/update, return information is set, and results are returned to the client.\nREPLACE Statement Although the REPLACE statement appears as a separate type of DML, in examining its syntax, it is merely replacing INSERT with REPLACE compared to a standard Basic INSERT. The difference is that REPLACE is a one-to-many statement. Briefly, for a typical INSERT statement which needs to INSERT a row and encounters a unique constraint conflict, various treatments are available:\nAbandon the insert and return an error: Basic INSERT Abandon the insert without error: INSERT IGNORE Abandon the insert, turning it into updating the conflicting row. If the updated value conflicts again, Return an error: INSERT ON DUPLICATE KEY UPDATE No error: INSERT IGNORE ON DUPLICATE KEY UPDATEThey all handle conflicts when a row of data conflicts with a row in the table differently. However, the REPLACE statement is distinct; it will delete all conflicting rows it encounters until there are no more conflicts, and then insert the data. If there are 5 unique indexes in the table, there could be 5 rows conflicting with the row waiting to be inserted. The REPLACE statement will delete these 5 rows all at once and then insert its own data. See the SQL below: CREATE TABLE t ( i int unique, j int unique, k int unique, l int unique, m int unique); INSERT INTO t VALUES (1, 1, 1, 1, 1), (2, 2, 2, 2, 2), (3, 3, 3, 3, 3), (4, 4, 4, 4, 4); REPLACE INTO t VALUES (1, 2, 3, 4, 5); SELECT * FROM t; i j k l m 1 2 3 4 5 After execution, it actually affects 5 rows of data.\nOnce we understand the uniqueness of the REPLACE statement, we can more easily comprehend its specific implementation.\nSimilar to the INSERT statement, the main execution part of the REPLACE statement is also in its Next method. Unlike INSERT, it passes its own exec method through insertRowsFromSelect and insertRows. In exec, it calls replaceRow, which also uses batch conflict detection in batchChecker. The difference from INSERT is that all detected conflicts are deleted here, and finally, the row to be inserted is written in.\nIn Conclusion The INSERT statement is among the most complex, versatile, and powerful of all DML statements. It includes statements like INSERT ON DUPLICATE UPDATE, which can perform both INSERT and UPDATE operations, and REPLACE, where a single row of data can impact many rows. The INSERT statement itself can be connected to a SELECT statement as input for the data to be inserted, thus its implementation is influenced by the planner (for more on the planner, see related source code reading articles: Part 7: Rule-Based Optimization and Part 8: Cost-Based Optimization). Familiarity with the implementation of various INSERT-related statements in TiDB can help readers use these statements more reasonably and efficiently in the future. Additionally, readers interested in contributing code to TiDB can also gain a quicker understanding of this part of the implementation through this article.\n","permalink":"https://blog.minifish.org/posts/insert-opt/","summary":"\u003cp\u003eIn a previous article \u003ca href=\"https://cn.pingcap.com/blog/tidb-source-code-reading-4\"\u003e“TiDB Source Code Reading Series (4) Overview of INSERT Statement”\u003c/a\u003e, we introduced the general process of the INSERT statement. Why write a separate article for INSERT? Because in TiDB, simply inserting a piece of data is the simplest and most common case. It becomes more complex when defining various behaviors within the INSERT statement, such as how to handle situations with Unique Key conflicts: Should we return an error? Ignore the current data insertion? Or overwrite existing data? Therefore, this article will continue to delve into the INSERT statement.\u003c/p\u003e","title":"How TiDB Implements the INSERT Statement"},{"content":"Installing HAProxy yum install haproxy is effective for CentOS 7. After installation, you can start the service using systemctl start haproxy. But don\u0026rsquo;t rush yet.\nConfiguring HAProxy Add the following content to /etc/haproxy/haproxy.cfg.\nglobal # The content of global is generally fixed and quite understandable. log 127.0.0.1 local2 maxconn 4096 user haproxy group haproxy chroot /var/lib/haproxy daemon pidfile /var/run/haproxy.pid stats socket /var/run/haproxy.sock # Create a socket file for haproxy nbproc 40 # Start 40 processes to forward concurrently, higher versions can use nbthread, a threaded approach. defaults # This section is mostly copied, not entirely clear on the options. log global mode http option tcplog option dontlognull retries 3 option redispatch maxconn 1024 timeout connect 5000ms timeout client 50000ms timeout server 50000ms listen cdb_cluster 0.0.0.0:3030 # The actual proxy name and address for receiving services. ## cdb balance leastconn - the cluster listening on port 3030. mode tcp balance leastconn # This method is most suitable for databases; do not change. server cdb1 172.16.30.3:26257 check # Check seems to require a port for feedback status; without it, it might not work, but it doesn\u0026#39;t matter. server cdb2 172.16.30.3:26258 check server cdb3 172.16.30.3:26259 check server cdb4 172.16.30.3:26260 check Start and Connect systemctl start haproxy to start the service.\npsql -Uroot -h127.0.0.1 -p3030 test to connect to the database.\nCockroachDB Official Recommendation CockroachDB officially provided their recommended configuration. In this configuration, they use:\ndefault # TCP keep-alive on client side. Server already enables them. option clitcpka listen psql option httpchk GET /health?ready=1 These two configurations, the first is to keep the client connection alive, which seems very useful. The second is a status check port, which I understand might be an option to ensure the service is available before dispatching requests, and it also seems very useful. It is recommended to add them.\n","permalink":"https://blog.minifish.org/posts/haproxy/","summary":"\u003ch2 id=\"installing-haproxy\"\u003eInstalling HAProxy\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eyum install haproxy\u003c/code\u003e is effective for CentOS 7. After installation, you can start the service using \u003ccode\u003esystemctl start haproxy\u003c/code\u003e. But don\u0026rsquo;t rush yet.\u003c/p\u003e\n\u003ch2 id=\"configuring-haproxy\"\u003eConfiguring HAProxy\u003c/h2\u003e\n\u003cp\u003eAdd the following content to /etc/haproxy/haproxy.cfg.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eglobal \u003cspan style=\"color:#75715e\"\u003e# The content of global is generally fixed and quite understandable.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        log 127.0.0.1   local2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        maxconn \u003cspan style=\"color:#ae81ff\"\u003e4096\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        user haproxy\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        group haproxy\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        chroot /var/lib/haproxy\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        daemon\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        pidfile /var/run/haproxy.pid\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        stats socket /var/run/haproxy.sock         \u003cspan style=\"color:#75715e\"\u003e# Create a socket file for haproxy\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        nbproc \u003cspan style=\"color:#ae81ff\"\u003e40\u003c/span\u003e                                  \u003cspan style=\"color:#75715e\"\u003e# Start 40 processes to forward concurrently, higher versions can use nbthread, a threaded approach.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edefaults \u003cspan style=\"color:#75715e\"\u003e# This section is mostly copied, not entirely clear on the options.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        log     global\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        mode    http\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        option  tcplog\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        option  dontlognull\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        retries \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        option  redispatch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        maxconn \u003cspan style=\"color:#ae81ff\"\u003e1024\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        timeout connect 5000ms\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        timeout client 50000ms\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        timeout server 50000ms\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003elisten cdb_cluster 0.0.0.0:3030  \u003cspan style=\"color:#75715e\"\u003e# The actual proxy name and address for receiving services.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e## cdb balance leastconn - the cluster listening on port 3030.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        mode tcp\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        balance leastconn  \u003cspan style=\"color:#75715e\"\u003e# This method is most suitable for databases; do not change.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        server cdb1 172.16.30.3:26257 check \u003cspan style=\"color:#75715e\"\u003e# Check seems to require a port for feedback status; without it, it might not work, but it doesn\u0026#39;t matter.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        server cdb2 172.16.30.3:26258 check\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        server cdb3 172.16.30.3:26259 check\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        server cdb4 172.16.30.3:26260 check\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"start-and-connect\"\u003eStart and Connect\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003esystemctl start haproxy\u003c/code\u003e to start the service.\u003c/p\u003e","title":"How to Use HAProxy to Test CockroachDB"},{"content":"Why Test TPC-C First of all, TPC-C is the de facto OLTP benchmark standard. It is a set of specifications, and any database can publish its test results under this standard, so there\u0026rsquo;s no issue of quarreling over the testing tools used.\nSecondly, TPC-C is closer to real-world scenarios as it includes a transaction model within it. In the flow of this transaction model, there are both high-frequency simple transaction statements and low-frequency inventory query statements. Therefore, it tests the database more comprehensively and practically.\nTesting TPC-C on CockroachDB This year, CockroachDB released its TPC-C performance results. However, unfortunately, they did not use a tool recognized by the database industry that implements the TPC-C standard for testing. Instead, they used their own implementation of a TPC-C tool. The compliance level of this tool was not recognized. In the white paper officially released by them, it is also mentioned that this TPC-C cannot be compared with the TPC-C standard.\nTherefore, I thought of using a highly recognized tool in the industry for testing. Here, I chose Benchmarksql version 5.0.\nBenchmarksql 5.0 supports the PostgreSQL protocol, Oracle protocol, and MySQL protocol (the MySQL protocol is supported in the code, but the author hasn\u0026rsquo;t fully tested it, so the official documentation doesn\u0026rsquo;t mention MySQL). Among these, the PostgreSQL protocol is supported by CockroachDB.\nTest Preparation After preparing the Benchmarksql code, don\u0026rsquo;t rush into testing. There are three main pitfalls here that need to be addressed first.\nCockroachDB does not support adding a primary key after table creation. Therefore, you need to include the primary key when creating the table. Specifically, in the run folder under the root directory of the Benchmarksql code, create a sql.cdb folder. Copy tableCreates.sql and indexCreates.sql from the sql.common folder at the same level into sql.cdb. Then move the primary keys in indexCreates.sql into the table creation statements in tableCreates.sql. For how to define indexes while creating tables, please refer to the database documentation syntax via Google.\nCockroachDB is a \u0026ldquo;strongly typed\u0026rdquo; database. This is my own way of describing it. It has a rather peculiar behavior: when you add different data types (e.g., int + float), it will report an error saying, \u0026ldquo;InternalError: unsupported binary operator: \u0026lt;int\u0026gt; + \u0026lt;float\u0026gt;\u0026rdquo;. Generally, databases don\u0026rsquo;t behave like this; most would perform some implicit conversions, or in other words, they are very tolerant of SQL writers. But CockroachDB is unique in that if you don\u0026rsquo;t specify the type, it reports an error. This greatly reduces the burden of type inference in its internal implementation.\nThis behavior causes Benchmarksql to fail to run the tests properly. The solution is to add the required type at the position where the error occurs. For example, change update t set i = i + ?; (the ? is generally filled in using prepare/execute) to update t set i = i + ?::DECIMAL;. Yes, CockroachDB specifies types explicitly by adding ::\u0026lt;type_name\u0026gt; at the end. But strangely, not all additions require type specification.\nCockroachDB does not support SELECT FOR UPDATE. This is the easiest to solve: comment out all FOR UPDATE clauses in Benchmarksql. CockroachDB itself supports the serializable isolation level; lacking FOR UPDATE doesn\u0026rsquo;t affect consistency.\nStarting the Test After overcoming the pitfalls mentioned above, you can proceed with the normal testing process: creating the database, creating tables and indexes, importing data, and testing. You can refer to Benchmarksql\u0026rsquo;s HOW-TO-RUN.txt.\nTest Results On my test machine with 40 cores, 128 GB of memory, and SSD, under 100 warehouses, the tpmC is approximately 5,000. This is about one-tenth of PostgreSQL 10 on the same machine. PostgreSQL can reach around 500,000 tpmC.\n","permalink":"https://blog.minifish.org/posts/cockroachdb-tpcc/","summary":"\u003ch2 id=\"why-test-tpc-c\"\u003eWhy Test TPC-C\u003c/h2\u003e\n\u003cp\u003eFirst of all, TPC-C is the de facto OLTP benchmark standard. It is a set of specifications, and any database can publish its test results under this standard, so there\u0026rsquo;s no issue of quarreling over the testing tools used.\u003c/p\u003e\n\u003cp\u003eSecondly, TPC-C is closer to real-world scenarios as it includes a transaction model within it. In the flow of this transaction model, there are both high-frequency simple transaction statements and low-frequency inventory query statements. Therefore, it tests the database more comprehensively and practically.\u003c/p\u003e","title":"How to Test CockroachDB Performance Using Benchmarksql"},{"content":"Compiling Sysbench with pgsql Support CockroachDB uses the PostgreSQL protocol. If you want to use Sysbench for testing, you need to enable pg protocol support in Sysbench. Sysbench already supports the pg protocol, but it is not enabled by default during compilation. You can configure it with the following command:\n./configure --with-pgsql Of course, preliminary work involves downloading the Sysbench source code and installing the necessary PostgreSQL header files required for compilation (you can use yum or sudo to install them).\nTesting The testing method is no different from testing MySQL or PostgreSQL; you can test any of the create, read, update, delete (CRUD) operations you like. The only thing to note is to set auto_inc to off.\nThis is because CockroachDB\u0026rsquo;s auto-increment behavior is different from PostgreSQL\u0026rsquo;s. It generates a unique id, but it does not guarantee that the ids are sequential or incremental. This is fine when inserting data. However, during delete, update, or query operations, since all SQL statements use id as the condition for these operations, you may encounter situations where data cannot be found.\nThat is:\nWhen auto_inc = on (which is the default value in Sysbench)\nTable Structure CREATE TABLE sbtest1 ( id INT NOT NULL DEFAULT unique_rowid(), k INTEGER NOT NULL DEFAULT 0:::INT, c STRING(120) NOT NULL DEFAULT \u0026#39;\u0026#39;:::STRING, pad STRING(60) NOT NULL DEFAULT \u0026#39;\u0026#39;:::STRING, CONSTRAINT \u0026#34;\u0026#34;primary\u0026#34;\u0026#34; PRIMARY KEY (id ASC), INDEX k_1 (k ASC), FAMILY \u0026#34;\u0026#34;primary\u0026#34;\u0026#34; (id, k, c, pad) ) Data root@:26257/sbtest\u0026gt; SELECT id FROM sbtest1 ORDER BY id LIMIT 1; +--------------------+ | id | +--------------------+ | 354033003848892419 | +--------------------+ As you can see, the data does not start from 1, nor is it sequential. Normally, the id in a Sysbench table should be within the range [1, table_size].\nSQL UPDATE sbtest%u SET k = k + 1 WHERE id = ? Taking the UPDATE statement as an example, id is used as the query condition. Sysbench assumes that this id should be between [1, table_size], but in reality, it\u0026rsquo;s not.\nExample of Correct Testing Command Line sysbench --db-driver=pgsql --pgsql-host=127.0.0.1 --pgsql-port=26257 --pgsql-user=root --pgsql-db=sbtest \\ --time=180 --threads=50 --report-interval=10 --tables=32 --table-size=10000000 \\ oltp_update_index \\ --sum_ranges=50 --distinct_ranges=50 --range_size=100 --simple_ranges=100 --order_ranges=100 \\ --index_updates=100 --non_index_updates=10 --auto_inc=off prepare/run/cleanup INSERT Testing Let\u0026rsquo;s discuss the INSERT test separately. The INSERT test refers to Sysbench\u0026rsquo;s oltp_insert. The characteristic of this test is that when auto_inc is on, data is inserted during the prepare phase of the test; otherwise, only the table is created without inserting data. Because when auto_inc is on, after the prepare phase, during the run phase, the inserted data will not cause conflicts due to the guarantee of the auto-increment column. When auto_inc is off, the id of the data inserted during the run phase is randomly assigned, which aligns with some actual testing scenarios.\nFor CockroachDB, when testing INSERT operations with auto_inc set to off, after the prepare phase, during the run phase of data insertion, you can observe the monitoring metrics (by connecting to CockroachDB\u0026rsquo;s HTTP port) under the \u0026ldquo;Distribution\u0026rdquo; section in \u0026ldquo;KV Transactions\u0026rdquo;. You\u0026rsquo;ll notice a large number of \u0026ldquo;Fast-path Committed\u0026rdquo; transactions. This indicates that transactions are committed using one-phase commit (1PC). That is, the data involved in the transaction does not span across CockroachDB nodes, so there\u0026rsquo;s no need to ensure consistency through two-phase commit transactions. This is an optimization in CockroachDB, which is very effective in INSERT tests and can deliver excellent performance.\nIf auto_inc is on, although for other tests that require read-before-write operations, the results in CockroachDB might be inflated, it is still fair for the INSERT test. If time permits, you can supplement the tests to see the differences.\n","permalink":"https://blog.minifish.org/posts/cockroachdb-sysbench/","summary":"\u003ch2 id=\"compiling-sysbench-with-pgsql-support\"\u003eCompiling Sysbench with pgsql Support\u003c/h2\u003e\n\u003cp\u003eCockroachDB uses the PostgreSQL protocol. If you want to use Sysbench for testing, you need to enable pg protocol support in Sysbench. Sysbench already supports the pg protocol, but it is not enabled by default during compilation. You can configure it with the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e./configure --with-pgsql\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eOf course, preliminary work involves downloading the Sysbench source code and installing the necessary PostgreSQL header files required for compilation (you can use \u003ccode\u003eyum\u003c/code\u003e or \u003ccode\u003esudo\u003c/code\u003e to install them).\u003c/p\u003e","title":"How to Test CockroachDB Performance Using Sysbench"},{"content":"Introduction to OLTP-Bench OLTP-Bench is an open-source benchmarking tool platform for OLTP scenarios from CMU\u0026rsquo;s DB Group. It was designed to provide a simple, easy-to-use, and extensible testing platform.\nIt connects to databases via the JDBC interface, supporting the following test suites:\nTPC-C Wikipedia Synthetic Resource Stresser Twitter Epinions.com TATP AuctionMark SEATS YCSB JPAB (Hibernate) CH-benCHmark Voter (Japanese \u0026ldquo;American Idol\u0026rdquo;) SIBench (Snapshot Isolation) SmallBank LinkBench Detailed project information can be found here, and the GitHub page is here.\nThe project introduction page includes three papers published by the authors, with the one from 2013 being the most important, also linked on the GitHub page.\nBased on the GitHub page, the project does not seem to have a high level of attention and has not been very active recently. Most issues and pull requests come from within CMU.\nOLTP-Bench: An Extensible Testbed for Benchmarking Relational Databases The paper \u0026ldquo;OLTP-Bench: An Extensible Testbed for Benchmarking Relational Databases\u0026rdquo; can be regarded as the most detailed introduction to this project.\nIn the first and second chapters, the authors introduce the motivation for creating this framework, which is to integrate multiple test sets and provide features that simple benchmarking tools do not have, while offering excellent extensibility to attract developers to support more databases.\nFrom the activity on GitHub, it is evident that this extensibility is more about adding database support rather than test sets. However, the number of supported test suites is already quite extensive.\nChapter three introduces the architectural design, with a focus on test suite management, load generators, SQL syntax conversion, multi-client scenarios (similar to multiple sysbench instances stressing a single MySQL), and result collection.\nChapter four discusses the supported test suites. I\u0026rsquo;m only familiar with TPCC and YCSB. The authors classify them from three perspectives:\nTransaction-focused, such as TPCC and SmallBank Internet applications, like LinkBench and Wikipedia Specialized tests, such as YCSB and SIBench Further details can be seen in the table: [table]\nChapter five describes the demo deployment environment, with subsequent sections introducing the demo\u0026rsquo;s features.\nChapter six uses the demo from the previous chapter to introduce features, analyzed as follows:\nRate control. It seems odd for a benchmarking tool to perform rate control, as the conventional understanding is to push performance as high as possible to gauge system limits. The paper provides an example using the Wikipedia test suite, increasing by 25 TPS every 10 seconds to observe database latency changes.\nTagging different transactions in the same test suite for separate statistics – using TPCC as an example to statistically categorize transactions from different stages.\nModifying load content, like switching from read-only to write-only loads.\nChanging the method for load randomness.\nMonitoring server status alongside database monitoring by deploying an OLTP-Bench monitor on the server.\nRunning multiple test suites simultaneously, such as running TPCC and YCSB concurrently.\nMulti-client usage, mentioned in chapter three.\nRepeatability. To prove OLTP-Bench results are genuine and reliable, the authors tested PG\u0026rsquo;s SSI performance using SIBench from the tool on similarly configured machines, achieving results consistent with those in PG\u0026rsquo;s SSI paper.\nIn summary, rate control and transaction tagging stand out as novel features, while the rest are not particularly special.\nChapter seven is arguably the most valuable part of the article, discussing cloud environments where users might only have database access and not server control. Users may struggle to assess the cost-effectiveness of different cloud database services or configurations due to charges encompassing CPU, storage, network, and asynchronous sync in some architectures. Thus, using benchmarking tools to derive performance and subsequently calculate cost-effectiveness is particularly worthwhile. This chapter compares varying perspectives: different service providers, configurations, comparing databases on the same configuration, and presents the cost-effectiveness outcomes.\nIn chapter eight, the authors compare OLTP-Bench with other similar tools, providing a favorable self-assessment.\nChapter nine outlines the authors’ future plans, including support for pure NoSQL, additional databases\u0026rsquo; proprietary SQL syntax, generating real-world load distributions from production data, and support for stored procedures.\nIn conclusion, as the authors mentioned, this is an integrative framework where ease of use and extensibility are key.\nUsage Summary OLTP-Bench is relatively simple to install and use, especially the deployment. Its cross-platform nature provides a better user experience compared to traditional tpcc and sysbench. Usage is relatively straightforward due to the plethora of test configuration templates provided, allowing easy initiation of tests with simple configuration file modifications. The test results are stable, although certain features mentioned in papers, like server status monitoring, still require exploration.\nI tested all 15 test suites on MySQL 5.7 and TiDB, obtaining the following results: [table]\nIts usability is quite evident. As for the ease of secondary development, it should be relatively simple, considering the entire OLTP-Bench project is not particularly large, with around 40,000 lines of code.\nOther tpch: While the framework\u0026rsquo;s code appears to support tpch, it proved unusable during practical tests, likely due to incomplete implementation and thus excluded from the README. Referring to future work mentioned in chapter nine of the paper, especially \u0026ldquo;generating load to match production data distribution,\u0026rdquo; this remains unimplemented, as seen in the codebase. ","permalink":"https://blog.minifish.org/posts/oltp-bench/","summary":"\u003ch2 id=\"introduction-to-oltp-bench\"\u003eIntroduction to OLTP-Bench\u003c/h2\u003e\n\u003cp\u003eOLTP-Bench is an open-source benchmarking tool platform for OLTP scenarios from CMU\u0026rsquo;s DB Group. It was designed to provide a simple, easy-to-use, and extensible testing platform.\u003c/p\u003e\n\u003cp\u003eIt connects to databases via the JDBC interface, supporting the following test suites:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTPC-C\u003c/li\u003e\n\u003cli\u003eWikipedia\u003c/li\u003e\n\u003cli\u003eSynthetic Resource Stresser\u003c/li\u003e\n\u003cli\u003eTwitter\u003c/li\u003e\n\u003cli\u003eEpinions.com\u003c/li\u003e\n\u003cli\u003eTATP\u003c/li\u003e\n\u003cli\u003eAuctionMark\u003c/li\u003e\n\u003cli\u003eSEATS\u003c/li\u003e\n\u003cli\u003eYCSB\u003c/li\u003e\n\u003cli\u003eJPAB (Hibernate)\u003c/li\u003e\n\u003cli\u003eCH-benCHmark\u003c/li\u003e\n\u003cli\u003eVoter (Japanese \u0026ldquo;American Idol\u0026rdquo;)\u003c/li\u003e\n\u003cli\u003eSIBench (Snapshot Isolation)\u003c/li\u003e\n\u003cli\u003eSmallBank\u003c/li\u003e\n\u003cli\u003eLinkBench\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDetailed project information can be found \u003ca href=\"http://db.cs.cmu.edu/projects/oltp-bench/\"\u003ehere\u003c/a\u003e, and the GitHub page is \u003ca href=\"https://github.com/oltpbenchmark/oltpbench\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"How to View CMU DB Group's OLTP-Bench"},{"content":"Background The English font set in Microsoft\u0026rsquo;s VS Code is monospaced, while the Chinese font is not.\nMethod Download and install the fonts from here. This font combines Source Han Sans and is said to align punctuation strictly. Use the method described here to set up VS Code. PS: It is indeed aligned, although the English text appears a bit narrow.\n","permalink":"https://blog.minifish.org/posts/font/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eThe English font set in Microsoft\u0026rsquo;s VS Code is monospaced, while the Chinese font is not.\u003c/p\u003e\n\u003ch2 id=\"method\"\u003eMethod\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eDownload and install the fonts from \u003ca href=\"https://github.com/be5invis/Sarasa-Gothic\"\u003ehere\u003c/a\u003e. This font combines Source Han Sans and is said to align punctuation strictly.\u003c/li\u003e\n\u003cli\u003eUse the method described \u003ca href=\"https://github.com/be5invis/Sarasa-Gothic/issues/8\"\u003ehere\u003c/a\u003e to set up VS Code.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ePS: It is indeed aligned, although the English text appears a bit narrow.\u003c/p\u003e","title":"How to Use Monospaced Fonts in VS Code"},{"content":"Background The DDL paper on F1 serves as the foundation for TiDB\u0026rsquo;s DDL implementation. There are two main papers on F1: one provides an overview of F1\u0026rsquo;s DDL, and the other specifically details the schema change method for DDL. I personally believe the second is key and more confusing to me. There is an introduction to the second paper here, which can help in understanding.\nUnderstanding Online DDL Concept The DDL discussed here refers to online DDL. The concept of online DDL originates from databases like MySQL, whereas PostgreSQL and similar databases might not support it. This concept is also quite vague; the distinction is whether you need to use exclusive locks during DDL operations to block transactions. Therefore, all databases can perform online DDL; it just depends on whether they\u0026rsquo;re willing to put in the effort. For traditional businesses where 24/7 availability isn\u0026rsquo;t a priority, DDL operations can be performed during maintenance times or late at night. Even if a few users are online, at most, they might experience minor delays. However, modern internet businesses are strict about maintenance windows, creating a higher demand for non-blocking DDL, which MySQL, as a quintessential internet database, was first to support. The typical implementation involves creating a copy of the schema table, with operations being sent to both the new and old tables during the transition.\nFor MySQL\u0026rsquo;s supported online DDL, see this webpage. Primarily, it\u0026rsquo;s categorized into operations on indexes and columns. This explains my curiosity about why TiDB\u0026rsquo;s examples for implementation often involve adding indexes.\nF1\u0026rsquo;s Method Having worked on something similar to Aurora before, there were many issues with this area. If you\u0026rsquo;re only performing offline DDL, it doesn\u0026rsquo;t have to be this complicated. According to F1\u0026rsquo;s paper, it uses the following series of state changes to accomplish a DDL:\n(reorganization) absent -\u0026gt; delete only -\u0026gt; write only ---------------\u0026gt; public In this sequence, each node passes through these four states, transitioning to the next state upon receiving a command. The agreed-upon rule is that each state doesn\u0026rsquo;t persist for more than twice the lease time across all nodes. How is this ensured? Through the following rule: if a node takes too long to move to the next state after receiving the transition command (meaning a state exceeds twice the lease time), it means the node received the command too late and will stop providing services and shut down.\nWithin these four states, \u0026lsquo;absent\u0026rsquo; indicates a state where the node hasn\u0026rsquo;t received instructions yet, and \u0026lsquo;public\u0026rsquo; signifies the completion of the DDL. What about the two middle states? The background link described them as follows:\nA delete-only table or column can be modified only by delete operations. A delete-only index can be modified only by delete and update operations. Update operations can delete key-value pairs corresponding to updated index keys, but they cannot create any new ones. A write-only column or index can have their key-value pairs modified by insert, delete, and update operations, but none of their pairs can be read by user transactions. A write-only constraint is applied for all new insert, delete, and update operations, but it is not guaranteed to hold over all existing data. Summarized in a table:\ndelete only write only Tables and columns can only be deleted; indexes can be updated (tables not really) Columns and indexes can be deleted, updated, and inserted The first state is rather peculiar, and the second one even more so—\u0026ldquo;can not read\u0026rdquo; might have been a better name for it. It\u0026rsquo;s said that this design allows two concurrent states to behave consistently, specifically pairs like (absent and delete only), (delete only and write only), and (write only and public).\nExamples:\nAdding index idx to table t, the deployment environment consists of two databases, a and b.\na enters delete only and completes adding the index. b has yet to receive any instructions. Insert operations on a and b: a\u0026rsquo;s idx is ignored, b is unaware and also ignores. Read operations: a and b ignore idx. Deletes and updates: a\u0026rsquo;s idx responds, b ignores. a enters write only, b enters delete only, and indexing is completed. Insert operations on a and b: a\u0026rsquo;s idx responds, b ignores (b loses index data?). Read operations: a and b ignore idx. Deletes and updates: both a and b\u0026rsquo;s idx respond. a enters the public state, b enters the write-only state. Insert operations on a and b: both a and b\u0026rsquo;s idx respond. Read operations: a\u0026rsquo;s idx responds, b ignores. Deletes and updates: both a and b\u0026rsquo;s idx respond. Removing index idx from table t, with two databases a and b in the deployment environment.\na enters delete only and completes removing the index. b has yet to receive any instructions. Insert operations on a and b: a\u0026rsquo;s idx is ignored, b has index idx and inserts into the index. Read operations: a ignores idx, b uses it. Deletes and updates: a\u0026rsquo;s idx responds (no-op if the index is removed), b responds. a enters write only, b enters delete only, and fulfills the command. Insert operations on a and b: a\u0026rsquo;s idx responds (no-op), b ignores. Read operations: both a and b ignore idx. Deletes and updates: both a\u0026rsquo;s and b\u0026rsquo;s idx respond (it\u0026rsquo;s a no-op as the index is already deleted). a enters the public state, b enters write only. Insert operations on a and b: both a and b respond (b no-op?). Read operations: a\u0026rsquo;s idx responds, b ignores. Deletes and updates: both a and b\u0026rsquo;s idx respond (b no-op?). ","permalink":"https://blog.minifish.org/posts/f1/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eThe DDL paper on F1 serves as the foundation for TiDB\u0026rsquo;s DDL implementation. There are two main papers on F1: one provides an overview of F1\u0026rsquo;s DDL, and the other specifically details the schema change method for DDL. I personally believe the second is key and more confusing to me. There is an \u003ca href=\"http://www.ifi.uzh.ch/dbtg/teaching/courses/SDBS/Papaioannou.pdf\"\u003eintroduction to the second paper\u003c/a\u003e here, which can help in understanding.\u003c/p\u003e\n\u003ch2 id=\"understanding\"\u003eUnderstanding\u003c/h2\u003e\n\u003ch3 id=\"online-ddl-concept\"\u003eOnline DDL Concept\u003c/h3\u003e\n\u003cp\u003eThe DDL discussed here refers to online DDL. The concept of online DDL originates from databases like MySQL, whereas PostgreSQL and similar databases might not support it. This concept is also quite vague; the distinction is whether you need to use exclusive locks during DDL operations to block transactions. Therefore, all databases can perform online DDL; it just depends on whether they\u0026rsquo;re willing to put in the effort. For traditional businesses where 24/7 availability isn\u0026rsquo;t a priority, DDL operations can be performed during maintenance times or late at night. Even if a few users are online, at most, they might experience minor delays. However, modern internet businesses are strict about maintenance windows, creating a higher demand for non-blocking DDL, which MySQL, as a quintessential internet database, was first to support. The typical implementation involves creating a copy of the schema table, with operations being sent to both the new and old tables during the transition.\u003c/p\u003e","title":"How to Understand F1's Schema Change"},{"content":"Background The CAP theorem has become one of the hottest theorems in recent years; when discussing distributed systems, CAP is inevitably mentioned. However, I feel that I haven\u0026rsquo;t thoroughly understood it, so I wanted to write a blog post to record my understanding. I will update the content as I gain new insights.\nUnderstanding I read the first part of this paper.\nThe CAP theorem [Bre12] says that you can only have two of the three desirable properties of:\nC: Consistency, which we can think of as serializability for this discussion; A: 100% availability, for both reads and updates; P: tolerance to network partitions. This leads to three kinds of systems: CA, CP and AP, based on what letter you leave out.\nLet me share my understanding, using a network composed of three machines (x, y, and z) as an example:\nC (Consistency): The three machines appear as one. Operations of addition, deletion, modification, and query on any one machine should always be consistent. That is, if you read data from x and then read from y, the results are the same. If you write data to x and then read from y, you should also read the newly written data. Wikipedia also specifically mentions that it\u0026rsquo;s acceptable to read the data just written to x from y after a short period of time (eventual consistency).\nA (Availability): The three machines, as a whole, must always be readable and writable; even if some parts fail, it must be readable and writable.\nP (Partition Tolerance): If the network between x, y, and z is broken, any machine cannot or refuses to provide services; it is neither readable nor writable.\nHere, C is the easiest to understand. The concepts of A and P are somewhat vague and easy to confuse.\nNow let\u0026rsquo;s discuss the three combinations:\nIf the network between x, y, and z is disconnected:\nCA: Ensure data consistency (C). When x writes data, y can read it (C). Allow the system to continue providing services—even if only x and y are operational—ensuring it is readable and writable (A). We can only tolerate z not providing service; it cannot read or write, nor return incorrect data (losing P).\nCP: Ensure data consistency (C). Allow all three machines to provide services (even if only for reads) (P). We can only tolerate that x, y, and z cannot write (losing A).\nAP: Allow all three machines to write (A). Allow all three machines to provide services (reads count) (P). We can only tolerate that the data written by x and y doesn\u0026rsquo;t reach z; z will return data inconsistent with x and y (losing C).\nCA is exemplified by Paxos/Raft, which are majority protocols that sacrifice P; minority nodes remain completely silent. CP represents a read-only system; if a system is read-only, whether there\u0026rsquo;s a network partition doesn\u0026rsquo;t really matter—the tolerance to network partitions is infinitely large. AP is suitable for systems that only append and do not update—only inserts, no deletes or updates. Finally, by merging the results together, it can still function.\n","permalink":"https://blog.minifish.org/posts/cap/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eThe CAP theorem has become one of the hottest theorems in recent years; when discussing distributed systems, CAP is inevitably mentioned. However, I feel that I haven\u0026rsquo;t thoroughly understood it, so I wanted to write a blog post to record my understanding. I will update the content as I gain new insights.\u003c/p\u003e\n\u003ch2 id=\"understanding\"\u003eUnderstanding\u003c/h2\u003e\n\u003cp\u003eI read the first part of this \u003ca href=\"https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45855.pdf\"\u003epaper\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe CAP theorem [Bre12] says that you can only have two of the three desirable properties of:\u003c/p\u003e","title":"Understanding the CAP Theorem"},{"content":"Background Travis CI is generally used for automating tests without needing to update the repository with the test outputs. This article explains how to automatically commit the results from Travis CI.\nProcess The basic process references this gist.\nBelow is the .travis.yml from the gist.\nlanguage: ruby rvm: - 2.0.0 env: global: - USER=\u0026#34;username\u0026#34; - EMAIL=\u0026#34;username@mail.com\u0026#34; - REPO=\u0026#34;name of target repo\u0026#34; - FILES=\u0026#34;README.md foo.txt bar.txt\u0026#34; - GH_REPO=\u0026#34;github.com/${USER}/${REPO}.git\u0026#34; - secure: \u0026#34;put travis gem output here =\u0026gt; http://docs.travis-ci.com/user/encryption-keys/\u0026#34; script: - ruby test.rb after_success: - MESSAGE=$(git log --format=%B -n 1 $TRAVIS_COMMIT) - git clone git://${GH_REPO} - mv -f ${FILES} ${REPO} - cd ${REPO} - git remote - git config user.email ${EMAIL} - git config user.name ${USER} - git add ${FILES} - git commit -m \u0026#34;${MESSAGE}\u0026#34; - git push \u0026#34;https://${GH_TOKEN}@${GH_REPO}\u0026#34; master \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 Note here that MESSAGE should be quoted when committing, which the original gist did not include.\nOriginal README:\n# Travis-CI tested push Sometimes we have a private repository to hold both problems and solutions as a reference for the class projects. The students can see only the problems in a public repository where they are able to clone/fork and develop their own solutions. We do not want the solution files in the public repository and each bug found/feature added in the project requires a push for each repository. It would be cool to work only with the reference repo and use tests to see if our modification is good enough for the public release. This is possible with Travis-CI following simple steps: - Create private and public repos - Download Ruby - Install the travis gem ``gem install travis`` - Generate a token in the Github website to allow others to play with your repos (copy the hash) - Log into your git account - Generate a secure token with the Travis gem (copy long hash) - Fill the environment variables in the ```.travis.yml``` file (USER, EMAIL, REPO, FILES) - Replace the value of **secure** with your long hash - Replace **GH_TOKEN** with your Travis token name - Push ```.travis.yml``` to private repo - Go to Travis to unlock your private repo tests - Push your files to the private repo to test Travis now has a [deployment](https://docs.travis-ci.com/user/deployment/) feature, which may be better for certain scenarios. A simple translation:\nCreate a GitHub project. The original seems to have created two projects, one for updating another. Install Ruby. Usually, gem is installed alongside. Install travis via gem install travis. Apply for a token for your GitHub account. You can Google the details. When selecting token permissions, only tick all related to repo; others can be omitted. Copy the generated token. In the root directory of the local machine\u0026rsquo;s repo (not sure if it must be the root) run travis encrypt GH_TOKEN=\u0026quot;copied token\u0026quot;. This creates an encrypted token to use as ${GH_TOKEN}, essentially an environment variable. The command output, a string on the screen, needs to be pasted into the travis config file after secure:. Use travis encrypt GH_TOKEN=\u0026quot;copied token\u0026quot; --add to write directly into the config file. Commit the modified configuration file. This translation is not strictly literal. The above content is more suited to the following personal configuration:\nlanguage: ruby branches: only: - master rvm: - 2.4.1 exclude: - vendor sudo: false env: global: - secure: rxKkyttLE1L4VsVIhhDUYGoLlER33ijKbdAAJPE8vNDSHwyANYnsP1GXK/rcwQqsL/KcJa55wEjVwEBzTMCqZM4UYNVIWqrJepVYo4rL1WhO+jT5sCqVR3qxK9KbgodcSXbmySJnJs0iLGMIQ2bo8yE91OxIC/GKkLCIwr9x4EGwFd5EcE5bOqmVqoSRk1q/1/5yA0aVF+Pohc5ATCZGw9+IyprU2Dx7qbA7F/T/4FQTOQZ4CLZAgyh/Gp1P+uxf1OK4IMCc/P6jVeTmbzQIbUcX0uG09pR7F0GnlV1ZOutMjY7SF8tQ7LNv2Wf8iWdiqehcwKNe/4TFHjs6rm3lEc6F1ELB5s4Z+QXjIM70haENSwM1FI8K5biL7tndAC1TujKESm0XadxORy5yOz7TfQZDTuMXvmmH3j+NFL3vTYPyMwwFca+IQBwD67a4PKD0PWBgEFD9Kn3rAlAzhV5OYdUuxZhx5zuQjKX5szUbL166fgoRnUwDp8dsOjLgOUqQa47IRqR3CTPzbf3zZIxGuX5x6mWySezCNprnXKCpyCegJBLoxQusA+EEYkvl4AOzhnmkhxFbEbHp+DYBjcSEEgpd06l67l3KzjMkpF02vr9CHNj8r7lAtZxwBVxYmczk289D5csOVR1SZKxQLwhx7k+CuEcYds685tLjIMmB0ZU= - USER=\u0026#34;jackysp\u0026#34; - FULLNAME=\u0026#34;Jack Yu\u0026#34; - EMAIL=\u0026#34;jackysp@gmail.com\u0026#34; - REPO=\u0026#34;jackysp.github.io\u0026#34; - GH_REPO=\u0026#34;github.com/${USER}/${REPO}.git\u0026#34; before_script: - git clone https://${GH_TOKEN}@${GH_REPO} script: bundle exec jekyll b -d ${REPO} after_success: - MESSAGE=$(git log --format=%B -n 1 $TRAVIS_COMMIT) - cd ${REPO} - git config user.email ${EMAIL} - git config user.name ${FULLNAME} - git add --all - git commit -m \u0026#34;${MESSAGE}\u0026#34; - git push --force origin master This configuration is used for automatically updating a blog created with Jekyll. There are two projects, one for source files and another for compiled HTML files. The purpose of this setup is to allow updating the blog without having to set up a Jekyll environment, even allowing updates directly from the GitHub website.\n","permalink":"https://blog.minifish.org/posts/travis-git-push/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eTravis CI is generally used for automating tests without needing to update the repository with the test outputs. This article explains how to automatically commit the results from Travis CI.\u003c/p\u003e\n\u003ch2 id=\"process\"\u003eProcess\u003c/h2\u003e\n\u003cp\u003eThe basic process references \u003ca href=\"https://gist.github.com/Maumagnaguagno/84a9807ed71d233e5d3f\"\u003ethis gist\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBelow is the \u003ccode\u003e.travis.yml\u003c/code\u003e from the gist.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yml\" data-lang=\"yml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003elanguage\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eruby\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003ervm\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003e2.0.0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eenv\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eglobal\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003eUSER=\u0026#34;username\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003eEMAIL=\u0026#34;username@mail.com\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003eREPO=\u0026#34;name of target repo\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003eFILES=\u0026#34;README.md foo.txt bar.txt\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003eGH_REPO=\u0026#34;github.com/${USER}/${REPO}.git\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#f92672\"\u003esecure\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;put travis gem output here =\u0026gt; http://docs.travis-ci.com/user/encryption-keys/\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003escript\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003eruby test.rb\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eafter_success\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003eMESSAGE=$(git log --format=%B -n 1 $TRAVIS_COMMIT)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003egit clone git://${GH_REPO}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003emv -f ${FILES} ${REPO}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003ecd ${REPO}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003egit remote\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003egit config user.email ${EMAIL}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003egit config user.name ${USER}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003egit add ${FILES}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003egit commit -m \u0026#34;${MESSAGE}\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003egit push \u0026#34;https://${GH_TOKEN}@${GH_REPO}\u0026#34; master \u0026gt; /dev/null 2\u0026gt;\u0026amp;1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote here that MESSAGE should be quoted when committing, which the original gist did not include.\u003c/p\u003e","title":"How to Execute Git Push in Travis CI"},{"content":"Some Documents on MySQL Client Usage Guide MySQL Shell User Guide Server Configuration Guide Using MySQL as a Document Store Application Development API Guide X DevAPI User Guide Introduction to Server Internal Implementation X Protocol. Implementation Principle Communication between client and server is over TCP and the protocol uses protobuf. After the server receives a message, it decodes and analyzes it. The protocol includes a concept called namespace, which specifically refers to whether the namespace is empty or \u0026ldquo;sql\u0026rdquo;, in which case the message content is executed as a SQL statement; if it is \u0026ldquo;xplugin\u0026rdquo; or \u0026ldquo;mysqlx,\u0026rdquo; the message is handled in another way. The other ways can be divided into: Administrative commands CRUD operations \u0026ldquo;xplugin\u0026rdquo; and \u0026ldquo;mysqlx\u0026rdquo; have the same function, with the latter being the new name for the former, retained temporarily for compatibility. The content of \u0026ldquo;mysqlx\u0026rdquo; messages, apart from explicit command content like kill_client, are mostly transformed into SQL statements which the server processes, essentially turning most into a form where the namespace is \u0026ldquo;sql\u0026rdquo;. Implementation Steps Start a new server for TiDB. The relevant configuration parameters such as IP, port, and socket need to be set. Implement the reading and writing functionality for message communication. Write a process for this new server to establish connections, including authentication, that follows the protocol. Use tcpdump to capture messages between MySQL and the client to derive protocol content, implementing the process by understanding MySQL source code. The server should include contents like the Query Context from the original TiDB server, as it primarily translates into SQL for execution. Implement the decoding and handling of messages. Although only a sentence, the workload included is substantial. In mysqlx_all_msgs.h, all messages are initialized\ninit_message_factory() { server_message\u0026lt;Mysqlx::Connection::Capabilities\u0026gt;(Mysqlx::ServerMessages::CONN_CAPABILITIES, \u0026#34;CONN_CAPABILITIES\u0026#34;, \u0026#34;Mysqlx.Connection.Capabilities\u0026#34;); server_message\u0026lt;Mysqlx::Error\u0026gt;(Mysqlx::ServerMessages::ERROR, \u0026#34;ERROR\u0026#34;, \u0026#34;Mysqlx.Error\u0026#34;); server_message\u0026lt;Mysqlx::Notice::Frame\u0026gt;(Mysqlx::ServerMessages::NOTICE, \u0026#34;NOTICE\u0026#34;, \u0026#34;Mysqlx.Notice.Frame\u0026#34;); server_message\u0026lt;Mysqlx::Ok\u0026gt;(Mysqlx::ServerMessages::OK, \u0026#34;OK\u0026#34;, \u0026#34;Mysqlx.Ok\u0026#34;); server_message\u0026lt;Mysqlx::Resultset::ColumnMetaData\u0026gt;(Mysqlx::ServerMessages::RESULTSET_COLUMN_META_DATA, \u0026#34;RESULTSET_COLUMN_META_DATA\u0026#34;, \u0026#34;Mysqlx.Resultset.ColumnMetaData\u0026#34;); server_message\u0026lt;Mysqlx::Resultset::FetchDone\u0026gt;(Mysqlx::ServerMessages::RESULTSET_FETCH_DONE, \u0026#34;RESULTSET_FETCH_DONE\u0026#34;, \u0026#34;Mysqlx.Resultset.FetchDone\u0026#34;); server_message\u0026lt;Mysqlx::Resultset::FetchDoneMoreResultsets\u0026gt;(Mysqlx::ServerMessages::RESULTSET_FETCH_DONE_MORE_RESULTSETS, \u0026#34;RESULTSET_FETCH_DONE_MORE_RESULTSETS\u0026#34;, \u0026#34;Mysqlx.Resultset.FetchDoneMoreResultsets\u0026#34;); server_message\u0026lt;Mysqlx::Resultset::Row\u0026gt;(Mysqlx::ServerMessages::RESULTSET_ROW, \u0026#34;RESULTSET_ROW\u0026#34;, \u0026#34;Mysqlx.Resultset.Row\u0026#34;); server_message\u0026lt;Mysqlx::Session::AuthenticateOk\u0026gt;(Mysqlx::ServerMessages::SESS_AUTHENTICATE_OK, \u0026#34;SESS_AUTHENTICATE_OK\u0026#34;, \u0026#34;Mysqlx.Session.AuthenticateOk\u0026#34;); server_message\u0026lt;Mysqlx::Sql::StmtExecuteOk\u0026gt;(Mysqlx::ServerMessages::SQL_STMT_EXECUTE_OK, \u0026#34;SQL_STMT_EXECUTE_OK\u0026#34;, \u0026#34;Mysqlx.Sql.StmtExecuteOk\u0026#34;); client_message\u0026lt;Mysqlx::Connection::CapabilitiesGet\u0026gt;(Mysqlx::ClientMessages::CON_CAPABILITIES_GET, \u0026#34;CON_CAPABILITIES_GET\u0026#34;, \u0026#34;Mysqlx.Connection.CapabilitiesGet\u0026#34;); client_message\u0026lt;Mysqlx::Connection::CapabilitiesSet\u0026gt;(Mysqlx::ClientMessages::CON_CAPABILITIES_SET, \u0026#34;CON_CAPABILITIES_SET\u0026#34;, \u0026#34;Mysqlx.Connection.CapabilitiesSet\u0026#34;); client_message\u0026lt;Mysqlx::Connection::Close\u0026gt;(Mysqlx::ClientMessages::CON_CLOSE, \u0026#34;CON_CLOSE\u0026#34;, \u0026#34;Mysqlx.Connection.Close\u0026#34;); client_message\u0026lt;Mysqlx::Crud::Delete\u0026gt;(Mysqlx::ClientMessages::CRUD_DELETE, \u0026#34;CRUD_DELETE\u0026#34;, \u0026#34;Mysqlx.Crud.Delete\u0026#34;); client_message\u0026lt;Mysqlx::Crud::Find\u0026gt;(Mysqlx::ClientMessages::CRUD_FIND, \u0026#34;CRUD_FIND\u0026#34;, \u0026#34;Mysqlx.Crud.Find\u0026#34;); client_message\u0026lt;Mysqlx::Crud::Insert\u0026gt;(Mysqlx::ClientMessages::CRUD_INSERT, \u0026#34;CRUD_INSERT\u0026#34;, \u0026#34;Mysqlx.Crud.Insert\u0026#34;); client_message\u0026lt;Mysqlx::Crud::Update\u0026gt;(Mysqlx::ClientMessages::CRUD_UPDATE, \u0026#34;CRUD_UPDATE\u0026#34;, \u0026#34;Mysqlx.Crud.Update\u0026#34;); client_message\u0026lt;Mysqlx::Crud::CreateView\u0026gt;(Mysqlx::ClientMessages::CRUD_CREATE_VIEW, \u0026#34;CRUD_CREATE_VIEW\u0026#34;, \u0026#34;Mysqlx.Crud.CreateView\u0026#34;); client_message\u0026lt;Mysqlx::Crud::ModifyView\u0026gt;(Mysqlx::ClientMessages::CRUD_MODIFY_VIEW, \u0026#34;CRUD_MODIFY_VIEW\u0026#34;, \u0026#34;Mysqlx.Crud.ModifyView\u0026#34;); client_message\u0026lt;Mysqlx::Crud::DropView\u0026gt;(Mysqlx::ClientMessages::CRUD_DROP_VIEW, \u0026#34;CRUD_DROP_VIEW\u0026#34;, \u0026#34;Mysqlx.Crud.DropView\u0026#34;); client_message\u0026lt;Mysqlx::Expect::Close\u0026gt;(Mysqlx::ClientMessages::EXPECT_CLOSE, \u0026#34;EXPECT_CLOSE\u0026#34;, \u0026#34;Mysqlx.Expect.Close\u0026#34;); client_message\u0026lt;Mysqlx::Expect::Open\u0026gt;(Mysqlx::ClientMessages::EXPECT_OPEN, \u0026#34;EXPECT_OPEN\u0026#34;, \u0026#34;Mysqlx.Expect.Open\u0026#34;); client_message\u0026lt;Mysqlx::Session::AuthenticateContinue\u0026gt;(Mysqlx::ClientMessages::SESS_AUTHENTICATE_CONTINUE, \u0026#34;SESS_AUTHENTICATE_CONTINUE\u0026#34;, \u0026#34;Mysqlx.Session.AuthenticateContinue\u0026#34;); client_message\u0026lt;Mysqlx::Session::AuthenticateStart\u0026gt;(Mysqlx::ClientMessages::SESS_AUTHENTICATE_START, \u0026#34;SESS_AUTHENTICATE_START\u0026#34;, \u0026#34;Mysqlx.Session.AuthenticateStart\u0026#34;); client_message\u0026lt;Mysqlx::Session::Close\u0026gt;(Mysqlx::ClientMessages::SESS_CLOSE, \u0026#34;SESS_CLOSE\u0026#34;, \u0026#34;Mysqlx.Session.Close\u0026#34;); client_message\u0026lt;Mysqlx::Session::Reset\u0026gt;(Mysqlx::ClientMessages::SESS_RESET, \u0026#34;SESS_RESET\u0026#34;, \u0026#34;Mysqlx.Session.Reset\u0026#34;); client_message\u0026lt;Mysqlx::Sql::StmtExecute\u0026gt;(Mysqlx::ClientMessages::SQL_STMT_EXECUTE, \u0026#34;SQL_STMT_EXECUTE\u0026#34;, \u0026#34;Mysqlx.Sql.StmtExecute\u0026#34;); } Server and client messages are that many. Client messages are dispatched in xpl_dispatcher.cc.\nngs::Error_code do_dispatch_command(xpl::Session \u0026amp;session, xpl::Crud_command_handler \u0026amp;crudh, xpl::Expectation_stack \u0026amp;expect, ngs::Request \u0026amp;command) { switch (command.get_type()) { case Mysqlx::ClientMessages::SQL_STMT_EXECUTE: return on_stmt_execute(session, static_cast\u0026lt;const Mysqlx::Sql::StmtExecute\u0026amp;\u0026gt;(*command.message())); case Mysqlx::ClientMessages::CRUD_FIND: return crudh.execute_crud_find(session, static_cast\u0026lt;const Mysqlx::Crud::Find\u0026amp;\u0026gt;(*command.message())); case Mysqlx::ClientMessages::CRUD_INSERT: return crudh.execute_crud_insert(session, static_cast\u0026lt;const Mysqlx::Crud::Insert\u0026amp;\u0026gt;(*command.message())); case Mysqlx::ClientMessages::CRUD_UPDATE: return crudh.execute_crud_update(session, static_cast\u0026lt;const Mysqlx::Crud::Update\u0026amp;\u0026gt;(*command.message())); case Mysqlx::ClientMessages::CRUD_DELETE: return crudh.execute_crud_delete(session, static_cast\u0026lt;const Mysqlx::Crud::Delete\u0026amp;\u0026gt;(*command.message())); case Mysqlx::ClientMessages::CRUD_CREATE_VIEW: return crudh.execute_create_view(session, static_cast\u0026lt;const Mysqlx::Crud::CreateView\u0026amp;\u0026gt;(*command.message())); case Mysqlx::ClientMessages::CRUD_MODIFY_VIEW: return crudh.execute_modify_view(session, static_cast\u0026lt;const Mysqlx::Crud::ModifyView\u0026amp;\u0026gt;(*command.message())); case Mysqlx::ClientMessages::CRUD_DROP_VIEW: return crudh.execute_drop_view(session, static_cast\u0026lt;const Mysqlx::Crud::DropView\u0026amp;\u0026gt;(*command.message())); case Mysqlx::ClientMessages::EXPECT_OPEN: return on_expect_open(session, expect, static_cast\u0026lt;const Mysqlx::Expect::Open\u0026amp;\u0026gt;(*command.message())); case Mysqlx::ClientMessages::EXPECT_CLOSE: return on_expect_close(session, expect, static_cast\u0026lt;const Mysqlx::Expect::Close\u0026amp;\u0026gt;(*command.message())); } session.proto().get_protocol_monitor().on_error_unknown_msg_type(); return ngs::Error(ER_UNKNOWN_COM_ERROR, \u0026#34;Unexpected message received\u0026#34;); } The rest is filling in the gaps.\nClient::run =\u0026gt; Client::handle_message =\u0026gt; Session::handle_message =\u0026gt; Session::handle_auth_message =\u0026gt; some auth handlers =\u0026gt; Session::handle_ready_message =\u0026gt; xpl::dispatcher::dispatch_command =\u0026gt; ngs::Error_code do_dispatch_command =\u0026gt; some crud handlers Mapping between MySQL type and X protocol type\n// ================= ============ ======= ========== ====== ======== // SQL Type .type .length .frac_dig .flags .charset // ================= ============ ======= ========== ====== ======== // TINY SINT x // TINY UNSIGNED UINT x x // SHORT SINT x // SHORT UNSIGNED UINT x x // INT24 SINT x // INT24 UNSIGNED UINT x x // INT SINT x // INT UNSIGNED UINT x x // LONGLONG SINT x // LONGLONG UNSIGNED UINT x x // DOUBLE DOUBLE x x x // FLOAT FLOAT x x x // DECIMAL DECIMAL x x x // VARCHAR,CHAR,... BYTES x x x // GEOMETRY BYTES // TIME TIME x // DATE DATETIME x // DATETIME DATETIME x // YEAR UINT x x // TIMESTAMP DATETIME x // SET SET x // ENUM ENUM x // NULL BYTES // BIT BIT x // ================= ============ ======= ========== ====== ======== The first SQL field information of MySQL:\nField 1: `@@lower_case_table_names` Catalog: `def` Database: `` Table: `` Org_table: `` Type: LONGLONG Collation: binary (63) Length: 21 Max_length: 1 Decimals: 0 Flags: UNSIGNED BINARY NUM Field 2: `connection_id()` Catalog: `def` Database: `` Table: `` Org_table: `` Type: LONGLONG Collation: binary (63) Length: 21 Max_length: 1 Decimals: 0 Flags: NOT_NULL UNSIGNED BINARY NUM Field 3: `variable_value` Catalog: `def` Database: `performance_schema` Table: `session_status` Org_table: `session_status` Type: VAR_STRING Collation: utf8_general_ci (33) Length: 3072 Max_length: 0 Decimals: 0 Flags: For TiDB:\nField 1: `@@lower_case_table_names` Catalog: `def` Database: `` Table: `` Org_table: `` Type: STRING Collation: ? (0) Length: 0 Max_length: 1 Decimals: 31 Flags: Field 2: `connection_id()` Catalog: `def` Database: `` Table: `` Org_table: `` Type: LONGLONG Collation: binary (63) Length: 20 Max_length: 1 Decimals: 0 Flags: UNSIGNED BINARY NUM Field 3: `variable_value` Catalog: `def` Database: `` Table: `` Org_table: `` Type: STRING Collation: utf8_general_ci (33) Length: 1024 Max_length: 0 Decimals: 0 Flags: ","permalink":"https://blog.minifish.org/posts/mysqlx-protocol/","summary":"\u003ch2 id=\"some-documents-on-mysql\"\u003eSome Documents on MySQL\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eClient Usage Guide \u003ca href=\"https://dev.mysql.com/doc/refman/5.7/en/mysql-shell.html\"\u003eMySQL Shell User Guide\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eServer Configuration Guide \u003ca href=\"https://dev.mysql.com/doc/refman/5.7/en/document-store.html\"\u003eUsing MySQL as a Document Store\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eApplication Development API Guide \u003ca href=\"https://dev.mysql.com/doc/x-devapi-userguide/en/\"\u003eX DevAPI User Guide\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eIntroduction to Server Internal Implementation \u003ca href=\"https://dev.mysql.com/doc/internals/en/x-protocol.html\"\u003eX Protocol\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"implementation-principle\"\u003eImplementation Principle\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCommunication between client and server is over TCP and the protocol uses protobuf.\u003c/li\u003e\n\u003cli\u003eAfter the server receives a message, it decodes and analyzes it. The protocol includes a concept called namespace, which specifically refers to whether the namespace is empty or \u0026ldquo;sql\u0026rdquo;, in which case the message content is executed as a SQL statement; if it is \u0026ldquo;xplugin\u0026rdquo; or \u0026ldquo;mysqlx,\u0026rdquo; the message is handled in another way. The other ways can be divided into:\n\u003cul\u003e\n\u003cli\u003eAdministrative commands\u003c/li\u003e\n\u003cli\u003eCRUD operations\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u0026ldquo;xplugin\u0026rdquo; and \u0026ldquo;mysqlx\u0026rdquo; have the same function, with the latter being the new name for the former, retained temporarily for compatibility.\u003c/li\u003e\n\u003cli\u003eThe content of \u0026ldquo;mysqlx\u0026rdquo; messages, apart from explicit command content like kill_client, are mostly transformed into SQL statements which the server processes, essentially turning most into a form where the namespace is \u0026ldquo;sql\u0026rdquo;.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"implementation-steps\"\u003eImplementation Steps\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eStart a new server for TiDB. The relevant configuration parameters such as IP, port, and socket need to be set.\u003c/li\u003e\n\u003cli\u003eImplement the reading and writing functionality for message communication.\u003c/li\u003e\n\u003cli\u003eWrite a process for this new server to establish connections, including authentication, that follows the protocol. Use tcpdump to capture messages between MySQL and the client to derive protocol content, implementing the process by understanding MySQL source code.\u003c/li\u003e\n\u003cli\u003eThe server should include contents like the Query Context from the original TiDB server, as it primarily translates into SQL for execution.\u003c/li\u003e\n\u003cli\u003eImplement the decoding and handling of messages. Although only a sentence, the workload included is substantial.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn \u003ccode\u003emysqlx_all_msgs.h\u003c/code\u003e, all messages are initialized\u003c/p\u003e","title":"How to Implement MySQL X Protocol on TiDB"},{"content":"Server Side Disable SeLinux\nEdit the configuration file:\nvi /etc/selinux/config Modify as follows:\n#SELINUX=enforcing # Comment out #SELINUXTYPE=targeted # Comment out SELINUX=disabled # Add this line Then reboot the system:\nreboot # Restart the system Create a Directory\nUsing the root user, create a directory named /nfs. Note: It\u0026rsquo;s best to check which partition has the most space by running df, as the root (/) partition may not have the most space. In some automatic partitioning setups, the /home partition may have the most space.\nInstall NFS Utilities and RPC Bind\nyum -y install nfs-utils rpcbind Enable Services at Boot\nchkconfig nfs on chkconfig rpcbind on chkconfig nfslock on Configure Exports\nEdit the NFS exports file:\nvi /etc/exports Add the following line:\n/home/nfs 192.168.1.0/24(rw,sync,no_all_squash) Start NFS Services\nservice rpcbind start service nfs start service nfslock start exportfs -a Configure NFS Ports\nEdit the NFS configuration file:\nvi /etc/sysconfig/nfs Uncomment the following lines:\nLOCKD_TCPPORT=32803 LOCKD_UDPPORT=32769 MOUNTD_PORT=892 Restart NFS Services\nservice rpcbind restart service nfs restart service nfslock restart Verify RPC Services\nrpcinfo -p localhost Note down the ports and their types.\nConfigure Firewall Rules\nAdjust the IP range according to your network:\niptables -I INPUT -m state --state NEW -p tcp -m multiport --dport 111,892,2049,32803 -s 192.168.0.0/24 -j ACCEPT iptables -I INPUT -m state --state NEW -p udp -m multiport --dport 111,892,2049,32769 -s 192.168.0.0/24 -j ACCEPT Save Firewall Rules\nTest from the client side. If successful, save the iptables configuration:\nservice iptables save Client Side Create Mount Point\nmkdir /nfs Check RPC Services on Server\nrpcinfo -p [server_ip] Show NFS Exports\nshowmount -e [server_ip] Mount NFS Share\nmount -t nfs -o soft,intr,bg,rw [server_ip]:/home/nfs /nfs Unmount NFS Share\numount /nfs Configure Automatic Mounting\nEdit the fstab file:\nvi /etc/fstab Add the following line:\n[server_ip]:/home/nfs /nfs nfs soft,intr,bg,rw 0 0 ","permalink":"https://blog.minifish.org/posts/centos6-nfs/","summary":"\u003ch2 id=\"server-side\"\u003eServer Side\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDisable SeLinux\u003c/strong\u003e\u003cbr\u003e\nEdit the configuration file:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003evi /etc/selinux/config\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eModify as follows:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-text\" data-lang=\"text\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e#SELINUX=enforcing    # Comment out\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e#SELINUXTYPE=targeted # Comment out\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSELINUX=disabled      # Add this line\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThen reboot the system:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ereboot  \u003cspan style=\"color:#75715e\"\u003e# Restart the system\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCreate a Directory\u003c/strong\u003e\u003cbr\u003e\nUsing the root user, create a directory named \u003ccode\u003e/nfs\u003c/code\u003e. Note: It\u0026rsquo;s best to check which partition has the most space by running \u003ccode\u003edf\u003c/code\u003e, as the root (\u003ccode\u003e/\u003c/code\u003e) partition may not have the most space. In some automatic partitioning setups, the \u003ccode\u003e/home\u003c/code\u003e partition may have the most space.\u003c/p\u003e","title":"How to Configure CentOS 6 NFS Service"},{"content":"What Is Bridging Bridging highly simulates a network card, making the router believe that the virtual machine\u0026rsquo;s network card truly exists. Personally, I feel it\u0026rsquo;s similar to resistors connected in parallel, whereas NAT (another common virtual machine network connection method) is more like parasitizing on the host\u0026rsquo;s network card.\nWhy Use Bridging It allows you to treat the virtual machine as a completely independent machine, enabling mutual access with the external network (which is not possible with NAT).\nHow to Configure Bridging In CentOS 6, refer to the command-line method in this article.\nWe don\u0026rsquo;t use the GUI method because:\nWe\u0026rsquo;re unsure which options to fill in on the last screen. We don\u0026rsquo;t know how to reset if we make a wrong selection. Command-line steps:\nCheck if bridge-utils is installed:\nrpm -q bridge-utils Usually, it\u0026rsquo;s already installed. If not, install it:\nsu - yum install bridge-utils Verify your network interfaces:\nRun ifconfig to ensure you have at least three network interfaces:\neth0 Link encap:Ethernet HWaddr 00:18:E7:16:DA:65 inet addr:192.168.0.117 Bcast:192.168.0.255 Mask:255.255.255.0 inet6 addr: fe80::218:e7ff:fe16:da65/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:556 errors:0 dropped:0 overruns:0 frame:0 TX packets:414 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:222834 (217.6 KiB) TX bytes:48430 (47.2 KiB) Interrupt:16 Base address:0x4f00 lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:8 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:480 (480.0 b) TX bytes:480 (480.0 b) virbr0 Link encap:Ethernet HWaddr 52:54:00:2A:C1:7E inet addr:192.168.122.1 Bcast:192.168.122.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:13 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 b) TX bytes:2793 (2.7 KiB) Navigate to the network scripts directory:\nsu – cd /etc/sysconfig/network-scripts Bring down the eth0 interface:\nifdown eth0 This step is crucial and must be performed locally. When I first configured this, I didn\u0026rsquo;t shut down the network (since I was working remotely). I didn\u0026rsquo;t realize that updating the ifcfg-eth0 configuration without restarting the network would immediately apply changes, resulting in loss of network connectivity.\nEdit ifcfg-eth0:\nIn the ifcfg-eth0 file, include:\nDEVICE=eth0 ONBOOT=yes BRIDGE=br0 Keep only these three lines in the file. There\u0026rsquo;s no need to configure an IP address here. Bridging seems to replace the original network card with the bridge, so you can delegate the configuration to the bridge.\nCreate a new file ifcfg-br0:\nDEVICE=br0 ONBOOT=yes TYPE=Bridge BOOTPROTO=static IPADDR=xxx.xxx.xxx.xxx # Use the IP you originally had in ifcfg-eth0 GATEWAY=xxx.xxx.xxx.xxx # Your gateway address NETMASK=255.255.255.0 # Your netmask DNS1=xxx.xxx.xxx.xxx # Your primary DNS server DNS2=xxx.xxx.xxx.xxx # Your secondary DNS server (if any) STP=on DELAY=0 Note: Replace xxx.xxx.xxx.xxx with your actual network settings.\nBring up the interfaces:\nifup br0 ifup eth0 Verify the bridge interface:\nCheck ifconfig to ensure that br0 is now present.\nUpdate firewall rules:\nEdit /etc/sysconfig/iptables and add:\n-A INPUT -i br0 -j ACCEPT (This is a general example; you may need to adjust it based on your specific firewall configuration.)\nRestart the firewall:\nservice iptables restart Configure bridging in virt-manager:\nWhen creating a new virtual machine using virt-manager, you can now select br0 for the network interface. Without this bridge, the bridging option would not be available.\nNote: When configuring the IP inside the virtual machine, be sure to specify the GATEWAY. Otherwise, the virtual machine will only be able to access the internal network and not the external network. At this point, the virtual machine won\u0026rsquo;t automatically discover the gateway.\n","permalink":"https://blog.minifish.org/posts/centos6-kvm-net-bridge/","summary":"\u003ch2 id=\"what-is-bridging\"\u003eWhat Is Bridging\u003c/h2\u003e\n\u003cp\u003eBridging highly simulates a network card, making the router believe that the virtual machine\u0026rsquo;s network card truly exists. Personally, I feel it\u0026rsquo;s similar to resistors connected in parallel, whereas NAT (another common virtual machine network connection method) is more like parasitizing on the host\u0026rsquo;s network card.\u003c/p\u003e\n\u003ch2 id=\"why-use-bridging\"\u003eWhy Use Bridging\u003c/h2\u003e\n\u003cp\u003eIt allows you to treat the virtual machine as a completely independent machine, enabling mutual access with the external network (which is not possible with NAT).\u003c/p\u003e","title":"How to Configure CentOS KVM Network Bridging Mode"},{"content":"Installation Process Installed Version: CentOS 6.3\nUsing Win32DiskImager to create a USB flash drive image was unsuccessful; installing from an external USB optical drive was successful. During the installation process, make sure to select the \u0026ldquo;Virtual Host\u0026rdquo; installation mode. The rest can be set to default or slightly modified, such as choosing the time zone. After installation, it will include the KVM suite and SSH. Installation Notes No internet connection is needed throughout the process, which is much better than Debian and Ubuntu. You\u0026rsquo;re not forced to set up a non-root user. Before installation, be sure to check whether your CPU supports virtualization and enable the motherboard\u0026rsquo;s virtualization setting. If the motherboard supports virtualization but doesn\u0026rsquo;t have a virtualization option, you can still use virtualization as it\u0026rsquo;s definitely enabled by default. There\u0026rsquo;s a saying that Intel CPUs with a \u0026lsquo;K\u0026rsquo; cannot perform virtualization. \u0026lsquo;K\u0026rsquo; means Intel CPUs that can be overclocked. It seems that faster and newer CPUs are not necessarily better. ","permalink":"https://blog.minifish.org/posts/centos6-install/","summary":"\u003ch2 id=\"installation-process\"\u003eInstallation Process\u003c/h2\u003e\n\u003cp\u003eInstalled Version: CentOS 6.3\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eUsing Win32DiskImager to create a USB flash drive image was unsuccessful; installing from an external USB optical drive was successful.\u003c/li\u003e\n\u003cli\u003eDuring the installation process, make sure to select the \u0026ldquo;Virtual Host\u0026rdquo; installation mode.\u003c/li\u003e\n\u003cli\u003eThe rest can be set to default or slightly modified, such as choosing the time zone.\u003c/li\u003e\n\u003cli\u003eAfter installation, it will include the KVM suite and SSH.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"installation-notes\"\u003eInstallation Notes\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eNo internet connection is needed throughout the process, which is much better than Debian and Ubuntu.\u003c/li\u003e\n\u003cli\u003eYou\u0026rsquo;re not forced to set up a non-root user.\u003c/li\u003e\n\u003cli\u003eBefore installation, be sure to check whether your CPU supports virtualization and enable the motherboard\u0026rsquo;s virtualization setting. If the motherboard supports virtualization but doesn\u0026rsquo;t have a virtualization option, you can still use virtualization as it\u0026rsquo;s definitely enabled by default. There\u0026rsquo;s a saying that Intel CPUs with a \u0026lsquo;K\u0026rsquo; cannot perform virtualization. \u0026lsquo;K\u0026rsquo; means Intel CPUs that can be overclocked. It seems that faster and newer CPUs are not necessarily better.\u003c/li\u003e\n\u003c/ul\u003e","title":"How to Install CentOS as a Virtualization Host"},{"content":"The text provided is a detailed set of instructions and queries for practicing SQL using PostgreSQL 9.4 BETA 2, focusing on creating and querying tables related to students, courses, scores, and teachers. Here\u0026rsquo;s a summary:\nDatabase Structure The database consists of four tables:\nSTUDENT: Contains student number (SNO), name (SNAME), gender (SSEX), birthday (SBIRTHDAY), and class (CLASS). COURSE: Includes course number (CNO), name (CNAME), and teacher number (TNO). SCORE: Records student number (SNO), course number (CNO), and degree (DEGREE). TEACHER: Holds teacher number (TNO), name (TNAME), gender (TSEX), birthday (TBIRTHDAY), professional title (PROF), and department (DEPART). Sample Data Students such as Zeng Hua, Kang Ming, and Wang Fang are stored with specific details, including their class and gender. Courses like \u0026ldquo;Introduction to Computers\u0026rdquo; and \u0026ldquo;Operating Systems\u0026rdquo; are associated with teacher numbers. Scores are recorded for students across various courses. Teachers are described with their professional roles and departments. Query Problems Several SQL queries are suggested for practice, such as:\nExtracting specific columns like SNAME, SSEX, and CLASS from the STUDENT table. Listing distinct departments for teachers. Calculating and sorting grades within the SCORE table. Performing database operations to find student averages, count of students per class, and comparing scores. Advanced Query Exercises Performing set operations and conditional joins to answer complex questions like finding students who scored more than others or comparing teachers\u0026rsquo; scores. Use of SQL functions like DATE_PART, subqueries, and unions to gather specific data. Additional Queries Techniques to refine queries for performance, like avoiding the NOT IN method. Handling conditions like age calculations using AGE(SBIRTHDAY) and filtering by name patterns. Overall, these exercises provide a robust framework for practicing SQL skills on a structured set of sample data, focusing on various database manipulation and retrieval techniques.7. Query:\n- `SELECT A.TNAME, B.CNAME FROM TEACHER A JOIN COURSE B ON A.TNO = B.TNO WHERE A.TSEX='男';` - Explanation: Joins teacher and course tables to select male teachers and their course names. Query:\nSELECT A.* FROM SCORE A WHERE DEGREE=(SELECT MAX(DEGREE) FROM SCORE B); Explanation: Selects all columns from the highest score in the score table. Query:\nSELECT SNAME FROM STUDENT A WHERE SSEX=(SELECT SSEX FROM STUDENT B WHERE B.SNAME='李军'); Explanation: Selects student names who have the same gender as the student named \u0026lsquo;Li Jun.\u0026rsquo; Query:\nSELECT SNAME FROM STUDENT A WHERE SSEX=(SELECT SSEX FROM STUDENT B WHERE B.SNAME='李军') AND CLASS=(SELECT CLASS FROM STUDENT C WHERE C.SNAME='李军'); Explanation: Selects student names who have the same gender and class as the student named \u0026lsquo;Li Jun.\u0026rsquo; Two Answers:\nSELECT A.* FROM SCORE A JOIN STUDENT B ON A.SNO = B.SNO JOIN COURSE C ON A.CNO = C.CNO WHERE B.SSEX='男' AND C.CNAME='计算机导论'; SELECT * FROM SCORE WHERE SNO IN(SELECT SNO FROM STUDENT WHERE SSEX='男') AND CNO=(SELECT CNO FROM COURSE WHERE CNAME='计算机导论'); Explanation: Both queries select scores of male students for the course \u0026lsquo;Introduction to Computer Science.\u0026rsquo; ","permalink":"https://blog.minifish.org/posts/postgresql-practices/","summary":"\u003cp\u003eThe text provided is a detailed set of instructions and queries for practicing SQL using PostgreSQL 9.4 BETA 2, focusing on creating and querying tables related to students, courses, scores, and teachers. Here\u0026rsquo;s a summary:\u003c/p\u003e\n\u003ch2 id=\"database-structure\"\u003eDatabase Structure\u003c/h2\u003e\n\u003cp\u003eThe database consists of four tables:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eSTUDENT\u003c/strong\u003e: Contains student number (SNO), name (SNAME), gender (SSEX), birthday (SBIRTHDAY), and class (CLASS).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCOURSE\u003c/strong\u003e: Includes course number (CNO), name (CNAME), and teacher number (TNO).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSCORE\u003c/strong\u003e: Records student number (SNO), course number (CNO), and degree (DEGREE).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTEACHER\u003c/strong\u003e: Holds teacher number (TNO), name (TNAME), gender (TSEX), birthday (TBIRTHDAY), professional title (PROF), and department (DEPART).\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"sample-data\"\u003eSample Data\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eStudents such as Zeng Hua, Kang Ming, and Wang Fang are stored with specific details, including their class and gender.\u003c/li\u003e\n\u003cli\u003eCourses like \u0026ldquo;Introduction to Computers\u0026rdquo; and \u0026ldquo;Operating Systems\u0026rdquo; are associated with teacher numbers.\u003c/li\u003e\n\u003cli\u003eScores are recorded for students across various courses.\u003c/li\u003e\n\u003cli\u003eTeachers are described with their professional roles and departments.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"query-problems\"\u003eQuery Problems\u003c/h2\u003e\n\u003cp\u003eSeveral SQL queries are suggested for practice, such as:\u003c/p\u003e","title":"How to Practice Using SQL"}]